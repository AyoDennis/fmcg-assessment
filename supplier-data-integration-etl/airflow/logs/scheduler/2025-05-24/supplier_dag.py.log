[2025-05-24T18:23:32.255+0000] {processor.py:186} INFO - Started process (PID=51) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:23:32.256+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:23:32.258+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:23:32.258+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:23:32.265+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:23:32.262+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 2, in <module>
    from airflow.operators import PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.operators' (/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/__init__.py)
[2025-05-24T18:23:32.266+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:23:32.289+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.040 seconds
[2025-05-24T18:24:03.316+0000] {processor.py:186} INFO - Started process (PID=121) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:24:03.318+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:24:03.325+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:24:03.325+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:24:03.337+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:24:03.334+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 2, in <module>
    from airflow.operators import PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.operators' (/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/__init__.py)
[2025-05-24T18:24:03.338+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:24:03.383+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.080 seconds
[2025-05-24T18:24:34.253+0000] {processor.py:186} INFO - Started process (PID=185) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:24:34.256+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:24:34.260+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:24:34.260+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:24:34.268+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:24:34.266+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 2, in <module>
    from airflow.operators import PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.operators' (/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/__init__.py)
[2025-05-24T18:24:34.269+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:24:34.290+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.048 seconds
[2025-05-24T18:25:04.583+0000] {processor.py:186} INFO - Started process (PID=249) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:25:04.586+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:25:04.593+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:25:04.593+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:25:04.603+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:25:04.599+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 2, in <module>
    from airflow.operators import PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.operators' (/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/__init__.py)
[2025-05-24T18:25:04.605+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:25:04.639+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.072 seconds
[2025-05-24T18:25:35.333+0000] {processor.py:186} INFO - Started process (PID=313) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:25:35.337+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:25:35.346+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:25:35.345+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:25:35.354+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:25:35.351+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 2, in <module>
    from airflow.operators import PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.operators' (/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/__init__.py)
[2025-05-24T18:25:35.355+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:25:35.423+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.102 seconds
[2025-05-24T18:26:05.688+0000] {processor.py:186} INFO - Started process (PID=377) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:26:05.690+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:26:05.697+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:26:05.696+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:26:05.723+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:26:05.720+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 2, in <module>
    from airflow.operators import PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.operators' (/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/__init__.py)
[2025-05-24T18:26:05.725+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:26:05.770+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.099 seconds
[2025-05-24T18:26:36.208+0000] {processor.py:186} INFO - Started process (PID=441) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:26:36.211+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:26:36.220+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:26:36.220+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:26:36.232+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:26:36.230+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 2, in <module>
    from airflow.operators import PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.operators' (/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/__init__.py)
[2025-05-24T18:26:36.233+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:26:36.271+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.074 seconds
[2025-05-24T18:27:06.619+0000] {processor.py:186} INFO - Started process (PID=505) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:27:06.621+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:27:06.627+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:27:06.626+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:27:06.639+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:27:06.636+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 2, in <module>
    from airflow.operators import PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.operators' (/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/__init__.py)
[2025-05-24T18:27:06.640+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:27:06.701+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.108 seconds
[2025-05-24T18:27:37.365+0000] {processor.py:186} INFO - Started process (PID=569) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:27:37.367+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:27:37.373+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:27:37.373+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:27:37.394+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:27:37.391+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 2, in <module>
    from airflow.operators import PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.operators' (/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/__init__.py)
[2025-05-24T18:27:37.395+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:27:37.430+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.080 seconds
[2025-05-24T18:28:07.634+0000] {processor.py:186} INFO - Started process (PID=632) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:28:07.636+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:28:07.647+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:28:07.647+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:28:07.661+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:28:07.658+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 2, in <module>
    from airflow.operators import PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.operators' (/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/__init__.py)
[2025-05-24T18:28:07.662+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:28:07.715+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.129 seconds
[2025-05-24T18:28:37.905+0000] {processor.py:186} INFO - Started process (PID=696) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:28:37.907+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:28:37.914+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:28:37.913+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:28:37.927+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:28:37.924+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 2, in <module>
    from airflow.operators import PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.operators' (/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/__init__.py)
[2025-05-24T18:28:37.928+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:28:37.969+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.079 seconds
[2025-05-24T18:29:08.708+0000] {processor.py:186} INFO - Started process (PID=760) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:29:08.713+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:29:08.727+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:29:08.725+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:29:08.763+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:29:08.760+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 2, in <module>
    from airflow.operators import PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.operators' (/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/__init__.py)
[2025-05-24T18:29:08.766+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:29:08.818+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.126 seconds
[2025-05-24T18:29:39.091+0000] {processor.py:186} INFO - Started process (PID=824) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:29:39.093+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:29:39.101+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:29:39.101+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:29:39.120+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:29:39.113+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 2, in <module>
    from airflow.operators import PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.operators' (/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/__init__.py)
[2025-05-24T18:29:39.122+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:29:39.197+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.129 seconds
[2025-05-24T18:30:09.379+0000] {processor.py:186} INFO - Started process (PID=888) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:30:09.382+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:30:09.394+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:30:09.390+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:30:09.405+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:30:09.402+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 2, in <module>
    from airflow.operators import PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.operators' (/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/__init__.py)
[2025-05-24T18:30:09.407+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:30:09.463+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.115 seconds
[2025-05-24T18:30:40.395+0000] {processor.py:186} INFO - Started process (PID=952) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:30:40.399+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:30:40.440+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:30:40.439+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:30:40.452+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:30:40.450+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 2, in <module>
    from airflow.operators import PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.operators' (/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/__init__.py)
[2025-05-24T18:30:40.453+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:30:40.480+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.099 seconds
[2025-05-24T18:31:10.839+0000] {processor.py:186} INFO - Started process (PID=1016) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:31:10.842+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:31:10.858+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:31:10.853+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:31:10.873+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:31:10.870+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 2, in <module>
    from airflow.operators import PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.operators' (/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/__init__.py)
[2025-05-24T18:31:10.875+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:31:10.926+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.105 seconds
[2025-05-24T18:31:41.210+0000] {processor.py:186} INFO - Started process (PID=1080) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:31:41.214+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:31:41.222+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:31:41.222+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:31:41.246+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:31:41.243+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 2, in <module>
    from airflow.operators import PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.operators' (/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/__init__.py)
[2025-05-24T18:31:41.248+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:31:41.347+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.158 seconds
[2025-05-24T18:32:11.737+0000] {processor.py:186} INFO - Started process (PID=1144) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:32:11.743+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:32:11.756+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:32:11.755+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:32:11.781+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:32:11.770+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 2, in <module>
    from airflow.operators import PythonOperator
ImportError: cannot import name 'PythonOperator' from 'airflow.operators' (/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/__init__.py)
[2025-05-24T18:32:11.782+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:32:11.827+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.107 seconds
[2025-05-24T18:32:17.098+0000] {processor.py:186} INFO - Started process (PID=1185) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:32:17.101+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:32:17.104+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:32:17.103+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:32:17.111+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:32:17.109+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from modules.etl import extract_csv
ModuleNotFoundError: No module named 'modules'
[2025-05-24T18:32:17.111+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:32:17.126+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.037 seconds
[2025-05-24T18:32:18.171+0000] {processor.py:186} INFO - Started process (PID=1186) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:32:18.172+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:32:18.177+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:32:18.176+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:32:18.183+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:32:18.182+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from modules.etl import extract_csv
ModuleNotFoundError: No module named 'modules'
[2025-05-24T18:32:18.184+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:32:18.204+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.042 seconds
[2025-05-24T18:32:49.113+0000] {processor.py:186} INFO - Started process (PID=1250) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:32:49.117+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:32:49.119+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:32:49.119+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:32:49.125+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:32:49.124+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from modules.etl import extract_csv
ModuleNotFoundError: No module named 'modules'
[2025-05-24T18:32:49.125+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:32:49.138+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.032 seconds
[2025-05-24T18:33:20.128+0000] {processor.py:186} INFO - Started process (PID=1314) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:33:20.134+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:33:20.140+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:33:20.139+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:33:20.152+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:33:20.150+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from modules.etl import extract_csv
ModuleNotFoundError: No module named 'modules'
[2025-05-24T18:33:20.152+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:33:20.171+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.060 seconds
[2025-05-24T18:33:50.507+0000] {processor.py:186} INFO - Started process (PID=1379) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:33:50.508+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:33:50.510+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:33:50.510+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:33:50.515+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:33:50.514+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from modules.etl import extract_csv
ModuleNotFoundError: No module named 'modules'
[2025-05-24T18:33:50.515+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:33:50.530+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.029 seconds
[2025-05-24T18:34:20.939+0000] {processor.py:186} INFO - Started process (PID=1443) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:34:20.941+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:34:20.944+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:34:20.943+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:34:20.950+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:34:20.948+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from modules.etl import extract_csv
ModuleNotFoundError: No module named 'modules'
[2025-05-24T18:34:20.951+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:34:20.971+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.040 seconds
[2025-05-24T18:34:51.645+0000] {processor.py:186} INFO - Started process (PID=1507) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:34:51.646+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:34:51.649+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:34:51.649+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:34:51.655+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:34:51.653+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from modules.etl import extract_csv
ModuleNotFoundError: No module named 'modules'
[2025-05-24T18:34:51.656+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:34:51.671+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.036 seconds
[2025-05-24T18:35:21.915+0000] {processor.py:186} INFO - Started process (PID=1571) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:35:21.917+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:35:21.920+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:35:21.920+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:35:21.928+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:35:21.925+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from modules.etl import extract_csv
ModuleNotFoundError: No module named 'modules'
[2025-05-24T18:35:21.928+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:35:21.944+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.037 seconds
[2025-05-24T18:35:52.358+0000] {processor.py:186} INFO - Started process (PID=1635) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:35:52.359+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:35:52.360+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:35:52.360+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:35:52.364+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:35:52.363+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from modules.etl import extract_csv
ModuleNotFoundError: No module named 'modules'
[2025-05-24T18:35:52.364+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:35:52.380+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.027 seconds
[2025-05-24T18:36:23.380+0000] {processor.py:186} INFO - Started process (PID=1699) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:36:23.381+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:36:23.384+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:36:23.383+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:36:23.390+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:36:23.389+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from modules.etl import extract_csv
ModuleNotFoundError: No module named 'modules'
[2025-05-24T18:36:23.391+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:36:23.408+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.035 seconds
[2025-05-24T18:36:53.727+0000] {processor.py:186} INFO - Started process (PID=1763) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:36:53.728+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:36:53.729+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:36:53.729+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:36:53.733+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:36:53.731+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from modules.etl import extract_csv
ModuleNotFoundError: No module named 'modules'
[2025-05-24T18:36:53.733+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:36:53.744+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.021 seconds
[2025-05-24T18:37:23.846+0000] {processor.py:186} INFO - Started process (PID=1823) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:37:23.847+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:37:23.848+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:37:23.848+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:37:23.851+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:37:23.850+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from modules.etl import extract_csv
ModuleNotFoundError: No module named 'modules'
[2025-05-24T18:37:23.851+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:37:23.863+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.020 seconds
[2025-05-24T18:37:54.369+0000] {processor.py:186} INFO - Started process (PID=1882) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:37:54.372+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:37:54.375+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:37:54.375+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:37:54.383+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:37:54.381+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from modules.etl import extract_csv
ModuleNotFoundError: No module named 'modules'
[2025-05-24T18:37:54.384+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:37:54.401+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.041 seconds
[2025-05-24T18:38:24.201+0000] {processor.py:186} INFO - Started process (PID=1946) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:38:24.202+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:38:24.205+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:38:24.205+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:38:24.213+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:38:24.211+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from utils.etl import extract_csv
ModuleNotFoundError: No module named 'utils'
[2025-05-24T18:38:24.213+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:38:24.227+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.034 seconds
[2025-05-24T18:38:51.732+0000] {processor.py:186} INFO - Started process (PID=1994) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:38:51.732+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:38:51.734+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:38:51.734+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:38:51.743+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:38:51.741+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from etl import extract_csv
ModuleNotFoundError: No module named 'etl'
[2025-05-24T18:38:51.746+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:38:51.759+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.030 seconds
[2025-05-24T18:39:21.834+0000] {processor.py:186} INFO - Started process (PID=2058) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:39:21.837+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:39:21.839+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:39:21.838+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:39:21.846+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:39:21.845+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from etl import extract_csv
ModuleNotFoundError: No module named 'etl'
[2025-05-24T18:39:21.846+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:39:21.858+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.027 seconds
[2025-05-24T18:39:52.090+0000] {processor.py:186} INFO - Started process (PID=2118) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:39:52.092+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:39:52.095+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:39:52.095+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:39:52.104+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:39:52.100+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from etl import extract_csv
ModuleNotFoundError: No module named 'etl'
[2025-05-24T18:39:52.105+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:39:52.128+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.047 seconds
[2025-05-24T18:40:23.097+0000] {processor.py:186} INFO - Started process (PID=2182) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:40:23.099+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:40:23.101+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:40:23.101+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:40:23.107+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:40:23.105+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from etl import extract_csv
ModuleNotFoundError: No module named 'etl'
[2025-05-24T18:40:23.107+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:40:23.123+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.032 seconds
[2025-05-24T18:40:53.650+0000] {processor.py:186} INFO - Started process (PID=2246) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:40:53.653+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:40:53.655+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:40:53.655+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:40:53.660+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:40:53.658+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from etl import extract_csv
ModuleNotFoundError: No module named 'etl'
[2025-05-24T18:40:53.660+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:40:53.673+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.030 seconds
[2025-05-24T18:41:24.001+0000] {processor.py:186} INFO - Started process (PID=2310) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:41:24.002+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:41:24.005+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:41:24.005+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:41:24.010+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:41:24.009+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from etl import extract_csv
ModuleNotFoundError: No module named 'etl'
[2025-05-24T18:41:24.011+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:41:24.036+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.044 seconds
[2025-05-24T18:41:54.855+0000] {processor.py:186} INFO - Started process (PID=2374) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:41:54.857+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:41:54.860+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:41:54.859+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:41:54.866+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:41:54.865+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from etl import extract_csv
ModuleNotFoundError: No module named 'etl'
[2025-05-24T18:41:54.867+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:41:54.882+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.032 seconds
[2025-05-24T18:42:25.123+0000] {processor.py:186} INFO - Started process (PID=2438) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:42:25.125+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:42:25.128+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:42:25.127+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:42:25.134+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:42:25.133+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from etl import extract_csv
ModuleNotFoundError: No module named 'etl'
[2025-05-24T18:42:25.134+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:42:25.147+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.028 seconds
[2025-05-24T18:42:55.323+0000] {processor.py:186} INFO - Started process (PID=2487) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:42:55.325+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:42:55.326+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:42:55.326+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:42:55.330+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:42:55.329+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from etl import extract_csv
ModuleNotFoundError: No module named 'etl'
[2025-05-24T18:42:55.330+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:42:55.342+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.021 seconds
[2025-05-24T18:43:25.398+0000] {processor.py:186} INFO - Started process (PID=2540) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:43:25.399+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:43:25.402+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:43:25.401+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:43:25.412+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:43:25.409+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from etl import extract_csv
ModuleNotFoundError: No module named 'etl'
[2025-05-24T18:43:25.413+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:43:25.431+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.038 seconds
[2025-05-24T18:43:55.771+0000] {processor.py:186} INFO - Started process (PID=2602) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:43:55.773+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T18:43:55.778+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:43:55.778+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:43:55.787+0000] {logging_mixin.py:190} INFO - [2025-05-24T18:43:55.785+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 383, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 4, in <module>
    from etl import extract_csv
ModuleNotFoundError: No module named 'etl'
[2025-05-24T18:43:55.787+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T18:43:55.817+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.063 seconds
[2025-05-24T20:44:39.009+0000] {processor.py:161} INFO - Started process (PID=49) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:44:39.011+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:44:39.020+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:44:39.020+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:44:41.545+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:44:41.544+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:44:41.546+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:44:41.546+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:44:41.547+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:44:41.547+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:44:41.548+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:44:41.548+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:44:41.556+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:44:41.555+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:44:41.561+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:44:41.557+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:44:41.563+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:44:41.602+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.610 seconds
[2025-05-24T20:45:12.266+0000] {processor.py:161} INFO - Started process (PID=110) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:45:12.268+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:45:12.270+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:45:12.269+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:45:12.895+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:45:12.895+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:45:12.896+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:45:12.896+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:45:12.896+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:45:12.896+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:45:12.896+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:45:12.896+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:45:12.897+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:45:12.897+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:45:12.898+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:45:12.897+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:45:12.899+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:45:12.922+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.662 seconds
[2025-05-24T20:45:43.003+0000] {processor.py:161} INFO - Started process (PID=179) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:45:43.004+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:45:43.006+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:45:43.005+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:45:43.787+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:45:43.787+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:45:43.788+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:45:43.788+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:45:43.788+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:45:43.788+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:45:43.788+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:45:43.788+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:45:43.794+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:45:43.794+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:45:43.796+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:45:43.795+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:45:43.797+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:45:43.816+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.818 seconds
[2025-05-24T20:46:14.565+0000] {processor.py:161} INFO - Started process (PID=241) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:46:14.567+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:46:14.569+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:46:14.568+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:46:15.479+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:46:15.479+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:46:15.480+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:46:15.480+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:46:15.481+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:46:15.481+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:46:15.481+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:46:15.481+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:46:15.482+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:46:15.482+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:46:15.484+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:46:15.482+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:46:15.484+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:46:15.510+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.950 seconds
[2025-05-24T20:46:46.041+0000] {processor.py:161} INFO - Started process (PID=302) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:46:46.042+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:46:46.044+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:46:46.043+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:46:46.633+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:46:46.633+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:46:46.634+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:46:46.634+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:46:46.634+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:46:46.634+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:46:46.634+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:46:46.634+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:46:46.635+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:46:46.635+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:46:46.636+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:46:46.635+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:46:46.637+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:46:46.647+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.613 seconds
[2025-05-24T20:47:16.693+0000] {processor.py:161} INFO - Started process (PID=368) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:47:16.694+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:47:16.696+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:47:16.695+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:47:17.305+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:47:17.305+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:47:17.305+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:47:17.305+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:47:17.306+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:47:17.306+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:47:17.306+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:47:17.306+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:47:17.307+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:47:17.307+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:47:17.308+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:47:17.307+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:47:17.308+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:47:17.321+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.631 seconds
[2025-05-24T20:47:48.145+0000] {processor.py:161} INFO - Started process (PID=428) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:47:48.146+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:47:48.149+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:47:48.148+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:47:48.724+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:47:48.723+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:47:48.724+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:47:48.724+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:47:48.725+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:47:48.724+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:47:48.725+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:47:48.725+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:47:48.728+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:47:48.728+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:47:48.730+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:47:48.728+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:47:48.730+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:47:48.745+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.606 seconds
[2025-05-24T20:48:19.529+0000] {processor.py:161} INFO - Started process (PID=488) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:48:19.532+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:48:19.534+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:48:19.534+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:48:20.102+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:48:20.102+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:48:20.102+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:48:20.102+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:48:20.103+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:48:20.103+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:48:20.103+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:48:20.103+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:48:20.104+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:48:20.104+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:48:20.106+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:48:20.104+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:48:20.106+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:48:20.117+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.601 seconds
[2025-05-24T20:48:50.241+0000] {processor.py:161} INFO - Started process (PID=548) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:48:50.242+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:48:50.245+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:48:50.245+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:48:50.854+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:48:50.854+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:48:50.854+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:48:50.854+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:48:50.854+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:48:50.854+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:48:50.855+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:48:50.855+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:48:50.855+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:48:50.855+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:48:50.857+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:48:50.856+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:48:50.857+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:48:50.866+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.636 seconds
[2025-05-24T20:49:20.898+0000] {processor.py:161} INFO - Started process (PID=615) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:49:20.899+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:49:20.900+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:49:20.900+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:49:21.430+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:49:21.430+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:49:21.430+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:49:21.430+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:49:21.431+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:49:21.430+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:49:21.431+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:49:21.431+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:49:21.432+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:49:21.432+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:49:21.433+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:49:21.432+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:49:21.433+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:49:21.447+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.553 seconds
[2025-05-24T20:49:52.026+0000] {processor.py:161} INFO - Started process (PID=675) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:49:52.027+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:49:52.029+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:49:52.029+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:49:52.607+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:49:52.607+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:49:52.607+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:49:52.607+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:49:52.608+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:49:52.607+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:49:52.608+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:49:52.608+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:49:52.609+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:49:52.609+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:49:52.610+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:49:52.609+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:49:52.610+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:49:52.637+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.617 seconds
[2025-05-24T20:50:23.593+0000] {processor.py:161} INFO - Started process (PID=735) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:50:23.595+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:50:23.598+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:50:23.597+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:50:24.139+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:50:24.139+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:50:24.139+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:50:24.139+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:50:24.139+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:50:24.139+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:50:24.140+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:50:24.140+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:50:24.141+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:50:24.141+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:50:24.142+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:50:24.141+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:50:24.142+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:50:24.154+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.568 seconds
[2025-05-24T20:50:54.378+0000] {processor.py:161} INFO - Started process (PID=795) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:50:54.379+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:50:54.381+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:50:54.380+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:50:54.967+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:50:54.966+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:50:54.967+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:50:54.967+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:50:54.967+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:50:54.967+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:50:54.967+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:50:54.967+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:50:54.968+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:50:54.968+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:50:54.969+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:50:54.968+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:50:54.969+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:50:54.979+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.607 seconds
[2025-05-24T20:51:25.187+0000] {processor.py:161} INFO - Started process (PID=862) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:51:25.189+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:51:25.190+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:51:25.190+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:51:25.781+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:51:25.781+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:51:25.782+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:51:25.782+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:51:25.783+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:51:25.782+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:51:25.783+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:51:25.783+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:51:25.785+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:51:25.785+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:51:25.787+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:51:25.785+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:51:25.787+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:51:25.800+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.616 seconds
[2025-05-24T20:51:55.849+0000] {processor.py:161} INFO - Started process (PID=915) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:51:55.851+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:51:55.854+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:51:55.853+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:51:56.270+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:51:56.270+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:51:56.271+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:51:56.270+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:51:56.271+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:51:56.271+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:51:56.271+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:51:56.271+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:51:56.272+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:51:56.272+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:51:56.273+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:51:56.272+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:51:56.274+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:51:56.282+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.440 seconds
[2025-05-24T20:52:26.635+0000] {processor.py:161} INFO - Started process (PID=975) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:52:26.637+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:52:26.639+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:52:26.639+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:52:27.147+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:52:27.147+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:52:27.148+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:52:27.147+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:52:27.148+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:52:27.148+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:52:27.148+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:52:27.148+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:52:27.149+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:52:27.149+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:52:27.151+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:52:27.149+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:52:27.151+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:52:27.162+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.534 seconds
[2025-05-24T20:52:57.473+0000] {processor.py:161} INFO - Started process (PID=1035) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:52:57.475+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:52:57.479+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:52:57.479+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:52:57.922+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:52:57.922+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:52:57.924+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:52:57.924+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:52:57.924+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:52:57.924+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:52:57.924+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:52:57.924+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:52:57.925+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:52:57.925+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:52:57.926+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:52:57.925+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:52:57.927+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:52:57.939+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.477 seconds
[2025-05-24T20:53:27.995+0000] {processor.py:161} INFO - Started process (PID=1097) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:53:27.995+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:53:27.997+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:53:27.996+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:53:28.324+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:53:28.324+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:53:28.324+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:53:28.324+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:53:28.325+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:53:28.325+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:53:28.325+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:53:28.325+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:53:28.326+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:53:28.326+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:53:28.327+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:53:28.326+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:53:28.328+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:53:28.343+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.353 seconds
[2025-05-24T20:53:58.956+0000] {processor.py:161} INFO - Started process (PID=1155) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:53:58.958+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:53:58.962+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:53:58.961+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:53:59.333+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:53:59.333+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:53:59.334+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:53:59.333+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:53:59.334+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:53:59.334+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:53:59.334+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:53:59.334+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:53:59.335+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:53:59.335+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:53:59.336+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:53:59.335+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:53:59.337+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:53:59.348+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.401 seconds
[2025-05-24T20:54:29.383+0000] {processor.py:161} INFO - Started process (PID=1220) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:54:29.384+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:54:29.385+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:54:29.385+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:54:29.704+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:54:29.704+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:54:29.705+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:54:29.704+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:54:29.705+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:54:29.705+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:54:29.706+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:54:29.706+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:54:29.707+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:54:29.707+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:54:29.709+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:54:29.707+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:54:29.709+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:54:29.723+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.344 seconds
[2025-05-24T20:55:00.633+0000] {processor.py:161} INFO - Started process (PID=1283) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:55:00.636+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:55:00.638+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:55:00.638+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:55:00.992+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:55:00.992+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:55:00.994+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:55:00.994+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:55:00.994+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:55:00.994+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:55:00.994+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:55:00.994+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:55:00.999+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:55:00.999+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:55:01.001+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:55:01.000+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:55:01.004+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:55:01.017+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.389 seconds
[2025-05-24T20:55:31.848+0000] {processor.py:161} INFO - Started process (PID=1344) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:55:31.851+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:55:31.854+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:55:31.853+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:55:32.177+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:55:32.177+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:55:32.178+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:55:32.178+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:55:32.178+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:55:32.178+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:55:32.178+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:55:32.178+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:55:32.179+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:55:32.179+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:55:32.181+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:55:32.180+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:55:32.181+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:55:32.196+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.356 seconds
[2025-05-24T20:56:02.763+0000] {processor.py:161} INFO - Started process (PID=1404) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:56:02.765+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:56:02.768+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:56:02.767+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:56:03.245+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:56:03.245+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:56:03.245+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:56:03.245+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:56:03.245+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:56:03.245+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:56:03.245+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:56:03.245+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:56:03.247+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:56:03.246+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:56:03.248+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:56:03.247+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:56:03.248+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:56:03.261+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.508 seconds
[2025-05-24T20:56:33.298+0000] {processor.py:161} INFO - Started process (PID=1468) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:56:33.299+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:56:33.300+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:56:33.300+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:56:33.642+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:56:33.641+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:56:33.642+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:56:33.642+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:56:33.642+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:56:33.642+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:56:33.642+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:56:33.642+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:56:33.644+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:56:33.644+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:56:33.645+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:56:33.644+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:56:33.645+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:56:33.659+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.364 seconds
[2025-05-24T20:57:03.779+0000] {processor.py:161} INFO - Started process (PID=1528) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:57:03.781+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:57:03.785+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:57:03.785+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:57:04.184+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:57:04.184+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:57:04.184+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:57:04.184+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:57:04.185+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:57:04.184+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:57:04.185+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:57:04.185+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:57:04.186+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:57:04.186+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:57:04.188+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:57:04.186+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:57:04.188+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:57:04.200+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.425 seconds
[2025-05-24T20:57:34.565+0000] {processor.py:161} INFO - Started process (PID=1592) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:57:34.566+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:57:34.567+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:57:34.567+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:57:34.878+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:57:34.877+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:57:34.881+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:57:34.881+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:57:34.881+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:57:34.881+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:57:34.882+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:57:34.882+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:57:34.884+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:57:34.883+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:57:34.888+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:57:34.884+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:57:34.889+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:57:34.901+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.340 seconds
[2025-05-24T20:58:04.939+0000] {processor.py:161} INFO - Started process (PID=1657) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:58:04.940+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:58:04.941+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:58:04.941+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:58:05.200+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:58:05.200+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:58:05.201+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:58:05.200+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:58:05.201+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:58:05.201+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:58:05.201+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:58:05.201+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:58:05.202+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:58:05.202+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:58:05.203+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:58:05.202+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:58:05.204+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:58:05.216+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.280 seconds
[2025-05-24T20:58:35.248+0000] {processor.py:161} INFO - Started process (PID=1717) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:58:35.248+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:58:35.249+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:58:35.249+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:58:35.563+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:58:35.563+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:58:35.564+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:58:35.563+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:58:35.564+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:58:35.564+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:58:35.564+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:58:35.564+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:58:35.565+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:58:35.565+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:58:35.566+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:58:35.565+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:58:35.566+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:58:35.583+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.338 seconds
[2025-05-24T20:59:05.928+0000] {processor.py:161} INFO - Started process (PID=1777) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:59:05.929+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:59:05.930+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:59:05.930+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:59:06.262+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:59:06.261+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:59:06.262+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:59:06.262+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:59:06.262+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:59:06.262+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:59:06.263+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:59:06.262+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:59:06.264+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:59:06.264+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:59:06.266+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:59:06.264+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:59:06.266+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:59:06.283+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.359 seconds
[2025-05-24T20:59:36.442+0000] {processor.py:161} INFO - Started process (PID=1840) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:59:36.443+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T20:59:36.444+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:59:36.444+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:59:36.751+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:59:36.751+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T20:59:36.751+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:59:36.751+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T20:59:36.752+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:59:36.751+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T20:59:36.752+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:59:36.752+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T20:59:36.753+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:59:36.753+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T20:59:36.755+0000] {logging_mixin.py:188} INFO - [2025-05-24T20:59:36.754+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T20:59:36.755+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T20:59:36.771+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.333 seconds
[2025-05-24T21:00:06.890+0000] {processor.py:161} INFO - Started process (PID=1900) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:00:06.891+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:00:06.892+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:00:06.892+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:00:07.201+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:00:07.201+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:00:07.201+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:00:07.201+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:00:07.202+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:00:07.201+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:00:07.202+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:00:07.202+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:00:07.203+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:00:07.203+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:00:07.204+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:00:07.203+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T21:00:07.205+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:00:07.217+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.331 seconds
[2025-05-24T21:00:38.118+0000] {processor.py:161} INFO - Started process (PID=1963) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:00:38.119+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:00:38.121+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:00:38.120+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:00:38.375+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:00:38.375+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:00:38.376+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:00:38.376+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:00:38.376+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:00:38.376+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:00:38.376+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:00:38.376+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:00:38.378+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:00:38.377+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:00:38.379+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:00:38.378+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T21:00:38.379+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:00:38.395+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.281 seconds
[2025-05-24T21:01:08.574+0000] {processor.py:161} INFO - Started process (PID=2023) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:01:08.575+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:01:08.577+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:01:08.577+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:01:08.904+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:01:08.904+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:01:08.904+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:01:08.904+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:01:08.905+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:01:08.904+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:01:08.905+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:01:08.905+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:01:08.908+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:01:08.907+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:01:08.910+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:01:08.908+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T21:01:08.910+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:01:08.926+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.358 seconds
[2025-05-24T21:01:39.754+0000] {processor.py:161} INFO - Started process (PID=2083) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:01:39.755+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:01:39.757+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:01:39.757+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:01:40.073+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:01:40.072+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:01:40.073+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:01:40.073+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:01:40.073+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:01:40.073+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:01:40.073+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:01:40.073+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:01:40.075+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:01:40.075+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:01:40.076+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:01:40.075+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T21:01:40.076+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:01:40.092+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.347 seconds
[2025-05-24T21:02:11.088+0000] {processor.py:161} INFO - Started process (PID=2143) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:02:11.096+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:02:11.109+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:11.108+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:02:11.468+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:11.468+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:02:11.468+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:11.468+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:02:11.469+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:11.468+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:02:11.469+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:11.469+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:02:11.470+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:11.470+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:02:11.471+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:11.470+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T21:02:11.472+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:02:11.497+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.421 seconds
[2025-05-24T21:02:42.523+0000] {processor.py:161} INFO - Started process (PID=2203) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:02:42.525+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:02:42.532+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:42.530+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:02:42.907+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:42.907+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:02:42.908+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:42.908+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:02:42.908+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:42.908+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:02:42.908+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:42.908+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:02:42.909+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:42.909+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:02:42.911+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:42.910+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/supplier_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/supplier_dag.py", line 5, in <module>
    from utils.etl import extract_api
ModuleNotFoundError: No module named 'utils'
[2025-05-24T21:02:42.911+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:02:42.934+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.427 seconds
[2025-05-24T21:02:58.040+0000] {processor.py:161} INFO - Started process (PID=2221) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:02:58.041+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:02:58.042+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:58.042+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:02:58.321+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:58.321+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:02:58.321+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:58.321+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:02:58.322+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:58.322+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:02:58.322+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:58.322+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:02:58.324+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:58.324+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:02:58.335+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:02:58.748+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:58.747+0000] {override.py:1829} INFO - Created Permission View: can edit on DAG:supplier_data_etl_pipeline
[2025-05-24T21:02:58.759+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:58.758+0000] {override.py:1829} INFO - Created Permission View: can read on DAG:supplier_data_etl_pipeline
[2025-05-24T21:02:58.766+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:58.766+0000] {override.py:1829} INFO - Created Permission View: can delete on DAG:supplier_data_etl_pipeline
[2025-05-24T21:02:58.768+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:58.767+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:02:58.784+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:58.783+0000] {dag.py:3118} INFO - Creating ORM DAG for supplier_data_etl_pipeline
[2025-05-24T21:02:58.800+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:02:58.799+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-23 00:00:00+00:00, run_after=2025-05-24 00:00:00+00:00
[2025-05-24T21:02:58.821+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.786 seconds
[2025-05-24T21:03:29.462+0000] {processor.py:161} INFO - Started process (PID=2289) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:03:29.465+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:03:29.473+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:03:29.473+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:03:29.783+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:03:29.782+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:03:29.783+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:03:29.783+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:03:29.783+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:03:29.783+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:03:29.783+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:03:29.783+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:03:29.785+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:03:29.785+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:03:29.792+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:03:29.808+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:03:29.808+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:03:29.822+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:03:29.822+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:03:29.833+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.385 seconds
[2025-05-24T21:04:00.148+0000] {processor.py:161} INFO - Started process (PID=2349) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:04:00.149+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:04:00.152+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:00.151+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:04:00.427+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:00.427+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:04:00.428+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:00.428+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:04:00.428+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:00.428+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:04:00.428+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:00.428+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:04:00.429+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:00.429+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:04:00.434+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:04:00.454+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:00.454+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:04:00.473+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:00.473+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:04:00.492+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.350 seconds
[2025-05-24T21:04:30.555+0000] {processor.py:161} INFO - Started process (PID=2409) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:04:30.556+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:04:30.557+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:30.557+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:04:30.891+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:30.891+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:04:30.891+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:30.891+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:04:30.891+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:30.891+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:04:30.892+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:30.891+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:04:30.893+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:30.893+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:04:30.896+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:04:30.908+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:30.907+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:04:30.920+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:30.920+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:04:30.929+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.378 seconds
[2025-05-24T21:04:49.124+0000] {processor.py:161} INFO - Started process (PID=2460) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:04:49.127+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:04:49.132+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:49.131+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:04:49.607+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:49.606+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:04:49.607+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:49.607+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:04:49.607+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:49.607+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:04:49.607+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:49.607+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:04:49.609+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:49.609+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:04:49.615+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:04:50.057+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:50.057+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:04:50.068+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:04:50.067+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:04:50.078+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.977 seconds
[2025-05-24T21:05:20.751+0000] {processor.py:161} INFO - Started process (PID=2520) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:05:20.754+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:05:20.759+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:05:20.758+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:05:21.260+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:05:21.260+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:05:21.261+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:05:21.260+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:05:21.261+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:05:21.261+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:05:21.261+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:05:21.261+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:05:21.266+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:05:21.266+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:05:21.281+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:05:21.305+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:05:21.305+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:05:21.672+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:05:21.671+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:05:21.684+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.961 seconds
[2025-05-24T21:05:52.378+0000] {processor.py:161} INFO - Started process (PID=2580) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:05:52.380+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:05:52.384+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:05:52.383+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:05:52.767+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:05:52.767+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:05:52.768+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:05:52.768+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:05:52.768+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:05:52.768+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:05:52.768+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:05:52.768+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:05:52.770+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:05:52.770+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:05:52.775+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:05:52.795+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:05:52.794+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:05:53.066+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:05:53.066+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:05:53.076+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.708 seconds
[2025-05-24T21:06:23.222+0000] {processor.py:161} INFO - Started process (PID=2640) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:06:23.223+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:06:23.240+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:06:23.238+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:06:24.019+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:06:24.019+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:06:24.019+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:06:24.019+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:06:24.020+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:06:24.020+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:06:24.020+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:06:24.020+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:06:24.020+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:06:24.020+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:06:24.023+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:06:24.035+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:06:24.035+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:06:24.046+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:06:24.046+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:06:24.055+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.849 seconds
[2025-05-24T21:06:54.856+0000] {processor.py:161} INFO - Started process (PID=2700) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:06:54.858+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:06:54.867+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:06:54.866+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:06:55.660+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:06:55.660+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:06:55.660+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:06:55.660+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:06:55.661+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:06:55.661+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:06:55.661+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:06:55.661+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:06:55.662+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:06:55.662+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:06:55.666+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:06:55.681+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:06:55.681+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:06:55.695+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:06:55.695+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:06:55.709+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.869 seconds
[2025-05-24T21:07:26.557+0000] {processor.py:161} INFO - Started process (PID=2760) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:07:26.559+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:07:26.564+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:07:26.564+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:07:28.475+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:07:28.474+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:07:28.476+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:07:28.476+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:07:28.477+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:07:28.477+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:07:28.477+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:07:28.477+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:07:28.480+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:07:28.480+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:07:28.483+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:07:28.509+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:07:28.508+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:07:28.523+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:07:28.523+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:07:28.535+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.004 seconds
[2025-05-24T21:07:58.988+0000] {processor.py:161} INFO - Started process (PID=2820) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:07:58.992+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:07:58.998+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:07:58.997+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:07:59.921+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:07:59.920+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:07:59.921+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:07:59.921+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:07:59.921+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:07:59.921+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:07:59.922+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:07:59.921+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:07:59.922+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:07:59.922+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:07:59.925+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:07:59.998+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:07:59.998+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:08:00.018+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:08:00.017+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:08:00.031+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.054 seconds
[2025-05-24T21:08:30.813+0000] {processor.py:161} INFO - Started process (PID=2894) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:08:30.815+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:08:30.819+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:08:30.818+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:08:31.692+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:08:31.692+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:08:31.692+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:08:31.692+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:08:31.693+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:08:31.692+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:08:31.693+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:08:31.693+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:08:31.694+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:08:31.694+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:08:31.698+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:08:31.718+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:08:31.718+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:08:31.733+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:08:31.733+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:08:31.744+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.945 seconds
[2025-05-24T21:09:02.098+0000] {processor.py:161} INFO - Started process (PID=2954) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:09:02.100+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:09:02.107+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:09:02.106+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:09:03.269+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:09:03.268+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:09:03.269+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:09:03.269+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:09:03.269+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:09:03.269+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:09:03.269+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:09:03.269+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:09:03.270+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:09:03.270+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:09:03.273+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:09:03.287+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:09:03.286+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:09:03.298+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:09:03.298+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:09:03.306+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.229 seconds
[2025-05-24T21:09:33.721+0000] {processor.py:161} INFO - Started process (PID=3021) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:09:33.722+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:09:33.724+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:09:33.724+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:09:34.456+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:09:34.456+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:09:34.456+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:09:34.456+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:09:34.457+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:09:34.456+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:09:34.457+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:09:34.457+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:09:34.458+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:09:34.458+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:09:34.461+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:09:34.473+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:09:34.473+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:09:34.486+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:09:34.486+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:09:34.496+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.781 seconds
[2025-05-24T21:10:05.312+0000] {processor.py:161} INFO - Started process (PID=3082) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:10:05.313+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:10:05.315+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:10:05.314+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:10:06.359+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:10:06.358+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:10:06.359+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:10:06.359+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:10:06.359+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:10:06.359+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:10:06.360+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:10:06.359+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:10:06.361+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:10:06.360+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:10:06.364+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:10:06.378+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:10:06.378+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:10:06.395+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:10:06.395+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:10:06.407+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.103 seconds
[2025-05-24T21:10:36.598+0000] {processor.py:161} INFO - Started process (PID=3143) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:10:36.599+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:10:36.601+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:10:36.601+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:10:37.384+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:10:37.384+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:10:37.384+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:10:37.384+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:10:37.384+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:10:37.384+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:10:37.385+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:10:37.385+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:10:37.386+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:10:37.386+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:10:37.388+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:10:37.403+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:10:37.403+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:10:37.415+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:10:37.415+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:10:37.428+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.837 seconds
[2025-05-24T21:11:30.031+0000] {processor.py:161} INFO - Started process (PID=3213) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:11:30.034+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:11:30.037+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:11:30.037+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:11:32.567+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:11:32.566+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:11:32.567+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:11:32.567+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:11:32.568+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:11:32.568+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:11:32.569+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:11:32.569+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:11:32.571+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:11:32.571+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:11:32.576+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:11:32.607+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:11:32.606+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:11:32.629+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:11:32.629+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:11:32.649+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.640 seconds
[2025-05-24T21:12:03.086+0000] {processor.py:161} INFO - Started process (PID=3282) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:12:03.089+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:12:03.095+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:12:03.093+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:12:04.358+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:12:04.357+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:12:04.358+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:12:04.358+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:12:04.358+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:12:04.358+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:12:04.358+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:12:04.358+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:12:04.359+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:12:04.359+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:12:04.362+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:12:04.372+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:12:04.372+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:12:04.384+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:12:04.384+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:12:04.393+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.336 seconds
[2025-05-24T21:12:34.559+0000] {processor.py:161} INFO - Started process (PID=3344) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:12:34.561+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:12:34.564+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:12:34.563+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:12:35.327+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:12:35.327+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:12:35.328+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:12:35.328+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:12:35.328+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:12:35.328+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:12:35.328+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:12:35.328+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:12:35.329+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:12:35.329+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:12:35.332+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:12:35.344+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:12:35.344+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:12:35.356+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:12:35.355+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:12:35.367+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.814 seconds
[2025-05-24T21:13:05.463+0000] {processor.py:161} INFO - Started process (PID=3413) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:13:05.464+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:13:05.468+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:13:05.467+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:13:06.317+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:13:06.316+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:13:06.317+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:13:06.317+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:13:06.317+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:13:06.317+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:13:06.317+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:13:06.317+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:13:06.319+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:13:06.319+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:13:06.321+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:13:06.332+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:13:06.332+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:13:06.343+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:13:06.343+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:13:06.352+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.896 seconds
[2025-05-24T21:13:36.945+0000] {processor.py:161} INFO - Started process (PID=3483) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:13:36.946+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:13:36.948+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:13:36.948+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:13:37.956+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:13:37.956+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:13:37.957+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:13:37.957+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:13:37.957+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:13:37.957+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:13:37.957+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:13:37.957+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:13:37.959+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:13:37.959+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:13:37.963+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:13:37.978+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:13:37.978+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:13:37.992+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:13:37.992+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:13:38.002+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.064 seconds
[2025-05-24T21:14:08.071+0000] {processor.py:161} INFO - Started process (PID=3544) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:14:08.073+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:14:08.075+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:14:08.074+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:14:08.907+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:14:08.906+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:14:08.907+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:14:08.907+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:14:08.907+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:14:08.907+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:14:08.908+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:14:08.908+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:14:08.910+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:14:08.909+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:14:08.914+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:14:08.928+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:14:08.927+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:14:08.944+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:14:08.944+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:14:08.954+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.890 seconds
[2025-05-24T21:14:39.918+0000] {processor.py:161} INFO - Started process (PID=3614) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:14:39.919+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:14:39.921+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:14:39.921+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:14:40.704+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:14:40.703+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:14:40.704+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:14:40.704+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:14:40.704+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:14:40.704+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:14:40.704+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:14:40.704+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:14:40.706+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:14:40.706+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:14:40.709+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:14:40.723+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:14:40.722+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:14:40.740+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:14:40.740+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:14:40.753+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.841 seconds
[2025-05-24T21:15:11.135+0000] {processor.py:161} INFO - Started process (PID=3676) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:15:11.136+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:15:11.137+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:15:11.137+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:15:11.909+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:15:11.909+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:15:11.909+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:15:11.909+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:15:11.909+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:15:11.909+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:15:11.910+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:15:11.910+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:15:11.911+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:15:11.911+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:15:11.913+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:15:11.924+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:15:11.924+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:15:11.934+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:15:11.934+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:15:11.943+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.814 seconds
[2025-05-24T21:15:42.374+0000] {processor.py:161} INFO - Started process (PID=3739) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:15:42.375+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:15:42.377+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:15:42.377+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:15:43.251+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:15:43.251+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:15:43.251+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:15:43.251+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:15:43.251+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:15:43.251+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:15:43.251+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:15:43.251+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:15:43.253+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:15:43.253+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:15:43.257+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:15:43.270+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:15:43.270+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:15:43.282+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:15:43.282+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:15:43.290+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.934 seconds
[2025-05-24T21:16:13.477+0000] {processor.py:161} INFO - Started process (PID=3800) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:16:13.478+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:16:13.480+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:16:13.479+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:16:14.440+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:16:14.440+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:16:14.440+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:16:14.440+0000] {etl.py:36} INFO - api data source pickled
[2025-05-24T21:16:14.440+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:16:14.440+0000] {etl.py:50} INFO - db source pickled
[2025-05-24T21:16:14.441+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:16:14.441+0000] {etl.py:153} INFO - Starting ETL pipeline
[2025-05-24T21:16:14.442+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:16:14.442+0000] {etl.py:164} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T21:16:14.445+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:16:14.458+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:16:14.458+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:16:14.471+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:16:14.471+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:16:14.480+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.008 seconds
[2025-05-24T21:16:44.962+0000] {processor.py:161} INFO - Started process (PID=3865) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:16:44.964+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:16:44.968+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:16:44.967+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:16:46.012+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:16:46.012+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:16:46.013+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:16:46.013+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:16:46.013+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:16:46.013+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:16:46.013+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:16:46.013+0000] {etl.py:36} INFO - Starting to read CSV from: /opt/airflow/dags/mock_supplier_data.csv
[2025-05-24T21:16:46.024+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:16:46.024+0000] {etl.py:44} INFO - CSV read successfully. Shape: (1000, 9)
[2025-05-24T21:16:46.027+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:16:46.026+0000] {etl.py:51} INFO - CSV data source pickled successfully to: /opt/airflow/dags/tmp/df_csv.pkl
[2025-05-24T21:16:46.028+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:16:46.028+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './api_suppliers.json'
[2025-05-24T21:16:46.032+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:16:46.047+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:16:46.046+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:16:46.058+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:16:46.058+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:16:46.067+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.118 seconds
[2025-05-24T21:17:16.836+0000] {processor.py:161} INFO - Started process (PID=3930) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:17:16.838+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:17:16.842+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:17:16.841+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:17:17.913+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:17:17.913+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:17:17.914+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:17:17.913+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:17:17.914+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:17:17.914+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:17:17.914+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:17:17.914+0000] {etl.py:36} INFO - Starting to read CSV from: /opt/airflow/dags/mock_supplier_data.csv
[2025-05-24T21:17:17.923+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:17:17.923+0000] {etl.py:44} INFO - CSV read successfully. Shape: (1000, 9)
[2025-05-24T21:17:17.925+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:17:17.925+0000] {etl.py:51} INFO - CSV data source pickled successfully to: /opt/airflow/dags/tmp/df_csv.pkl
[2025-05-24T21:17:17.926+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:17:17.926+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './api_suppliers.json'
[2025-05-24T21:17:17.930+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:17:17.951+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:17:17.950+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:17:17.976+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:17:17.976+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:17:17.987+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.178 seconds
[2025-05-24T21:17:48.382+0000] {processor.py:161} INFO - Started process (PID=3990) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:17:48.385+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:17:48.389+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:17:48.388+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:17:49.550+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:17:49.550+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:17:49.551+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:17:49.550+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:17:49.551+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:17:49.551+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:17:49.551+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:17:49.551+0000] {etl.py:36} INFO - Starting to read CSV from: /opt/airflow/dags/mock_supplier_data.csv
[2025-05-24T21:17:49.558+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:17:49.558+0000] {etl.py:44} INFO - CSV read successfully. Shape: (1000, 9)
[2025-05-24T21:17:49.561+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:17:49.561+0000] {etl.py:51} INFO - CSV data source pickled successfully to: /opt/airflow/dags/tmp/df_csv.pkl
[2025-05-24T21:17:49.562+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:17:49.562+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './api_suppliers.json'
[2025-05-24T21:17:49.564+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:17:49.576+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:17:49.576+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:17:49.590+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:17:49.590+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:17:49.608+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.249 seconds
[2025-05-24T21:18:19.886+0000] {processor.py:161} INFO - Started process (PID=4050) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:18:19.888+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:18:19.893+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:18:19.892+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:18:20.912+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:18:20.911+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:18:20.912+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:18:20.912+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:18:20.912+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:18:20.912+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:18:20.913+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:18:20.912+0000] {etl.py:36} INFO - Starting to read CSV from: /opt/airflow/dags/mock_supplier_data.csv
[2025-05-24T21:18:20.924+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:18:20.924+0000] {etl.py:44} INFO - CSV read successfully. Shape: (1000, 9)
[2025-05-24T21:18:20.927+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:18:20.927+0000] {etl.py:51} INFO - CSV data source pickled successfully to: /opt/airflow/dags/tmp/df_csv.pkl
[2025-05-24T21:18:20.928+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:18:20.928+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './api_suppliers.json'
[2025-05-24T21:18:20.932+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:18:20.947+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:18:20.947+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:18:20.960+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:18:20.960+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:18:20.972+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.099 seconds
[2025-05-24T21:18:51.643+0000] {processor.py:161} INFO - Started process (PID=4119) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:18:51.646+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:18:51.655+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:18:51.652+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:18:52.908+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:18:52.907+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:18:52.909+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:18:52.909+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:18:52.909+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:18:52.909+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:18:52.909+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:18:52.909+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:18:52.918+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:18:52.917+0000] {etl.py:20} INFO - csv read
[2025-05-24T21:18:52.921+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:18:52.921+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './api_suppliers.json'
[2025-05-24T21:18:52.924+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:18:52.941+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:18:52.940+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:18:52.955+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:18:52.955+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:18:52.966+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.343 seconds
[2025-05-24T21:19:23.608+0000] {processor.py:161} INFO - Started process (PID=4184) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:19:23.610+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:19:23.631+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:19:23.631+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:19:53.699+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:19:52.644+0000] {timeout.py:68} ERROR - Process timed out, PID: 4184
[2025-05-24T21:20:24.697+0000] {processor.py:161} INFO - Started process (PID=4245) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:20:24.700+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:20:24.701+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:20:24.701+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:20:25.642+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:20:25.642+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:20:25.643+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:20:25.643+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:20:25.643+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:20:25.643+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:20:25.643+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:20:25.643+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:20:25.651+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:20:25.650+0000] {etl.py:20} INFO - csv read
[2025-05-24T21:20:25.653+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:20:25.653+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './api_suppliers.json'
[2025-05-24T21:20:25.655+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:20:25.734+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:20:25.733+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:20:25.748+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:20:25.747+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:20:25.760+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.081 seconds
[2025-05-24T21:20:55.933+0000] {processor.py:161} INFO - Started process (PID=4307) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:20:55.935+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:20:55.937+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:20:55.936+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:20:56.885+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:20:56.884+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:20:56.885+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:20:56.885+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:20:56.885+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:20:56.885+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:20:56.886+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:20:56.886+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:20:56.899+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:20:56.899+0000] {etl.py:20} INFO - csv read
[2025-05-24T21:20:56.903+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:20:56.902+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './api_suppliers.json'
[2025-05-24T21:20:56.906+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:20:56.940+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:20:56.939+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:20:56.956+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:20:56.955+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:20:56.967+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.058 seconds
[2025-05-24T21:21:27.181+0000] {processor.py:161} INFO - Started process (PID=4379) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:21:27.182+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:21:27.182+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:21:27.182+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:21:27.841+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:21:27.841+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:21:27.841+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:21:27.841+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:21:27.842+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:21:27.842+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:21:27.842+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:21:27.842+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:21:27.850+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:21:27.850+0000] {etl.py:20} INFO - csv read
[2025-05-24T21:21:27.853+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:21:27.853+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './api_suppliers.json'
[2025-05-24T21:21:27.855+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:21:27.873+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:21:27.873+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:21:27.884+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:21:27.884+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:21:27.895+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.718 seconds
[2025-05-24T21:22:05.960+0000] {processor.py:161} INFO - Started process (PID=4437) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:22:06.008+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:22:06.028+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:22:06.010+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:23:28.540+0000] {processor.py:161} INFO - Started process (PID=4503) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:23:28.544+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:23:28.546+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:23:28.545+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:23:29.466+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:23:29.465+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:23:29.467+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:23:29.467+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:23:29.467+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:23:29.467+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:23:29.468+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:23:29.467+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:23:29.485+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:23:29.484+0000] {etl.py:20} INFO - csv read
[2025-05-24T21:23:29.491+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:23:29.491+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './api_suppliers.json'
[2025-05-24T21:23:29.497+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:23:29.606+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:23:29.606+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:23:29.617+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:23:29.616+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:23:29.626+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.102 seconds
[2025-05-24T21:24:17.576+0000] {processor.py:161} INFO - Started process (PID=4543) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:24:17.578+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:24:17.580+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:24:17.578+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:24:31.295+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:24:30.588+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:24:32.445+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:24:31.664+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:24:33.883+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:24:33.615+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:24:35.907+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:24:34.543+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:24:56.530+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:24:56.455+0000] {timeout.py:68} ERROR - Process timed out, PID: 4543
[2025-05-24T21:25:27.495+0000] {processor.py:161} INFO - Started process (PID=4610) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:25:27.496+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:25:27.499+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:25:27.497+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:25:28.489+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:25:28.489+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:25:28.490+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:25:28.489+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:25:28.490+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:25:28.490+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:25:28.490+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:25:28.490+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:25:28.492+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:25:28.491+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:25:28.496+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:25:28.603+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:25:28.603+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:25:28.616+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:25:28.616+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:25:28.628+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.158 seconds
[2025-05-24T21:25:58.800+0000] {processor.py:161} INFO - Started process (PID=4672) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:25:58.801+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:25:58.803+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:25:58.802+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:25:59.796+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:25:59.795+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:25:59.796+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:25:59.796+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:25:59.796+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:25:59.796+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:25:59.796+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:25:59.796+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:25:59.798+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:25:59.798+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:25:59.802+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:25:59.830+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:25:59.830+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:25:59.843+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:25:59.843+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:25:59.856+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.076 seconds
[2025-05-24T21:26:30.471+0000] {processor.py:161} INFO - Started process (PID=4734) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:26:30.474+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:26:30.476+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:26:30.475+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:26:31.324+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:26:31.324+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:26:31.325+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:26:31.325+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:26:31.325+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:26:31.325+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:26:31.325+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:26:31.325+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:26:31.327+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:26:31.326+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:26:31.331+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:26:31.361+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:26:31.360+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:26:31.377+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:26:31.377+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:26:31.393+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.935 seconds
[2025-05-24T21:27:01.780+0000] {processor.py:161} INFO - Started process (PID=4796) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:27:01.782+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:27:01.783+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:27:01.782+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:27:02.748+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:27:02.748+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:27:02.749+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:27:02.749+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:27:02.749+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:27:02.749+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:27:02.749+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:27:02.749+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:27:02.751+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:27:02.750+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:27:02.754+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:27:02.781+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:27:02.780+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:27:02.806+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:27:02.805+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:27:02.825+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.058 seconds
[2025-05-24T21:27:33.920+0000] {processor.py:161} INFO - Started process (PID=4864) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:27:33.923+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:27:33.927+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:27:33.925+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:27:35.266+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:27:35.266+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:27:35.266+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:27:35.266+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:27:35.266+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:27:35.266+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:27:35.267+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:27:35.267+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:27:35.268+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:27:35.268+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:27:35.271+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:27:35.283+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:27:35.282+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:27:35.306+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:27:35.305+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:27:35.321+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.421 seconds
[2025-05-24T21:28:05.611+0000] {processor.py:161} INFO - Started process (PID=4930) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:28:05.612+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:28:05.613+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:28:05.613+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:28:06.274+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:28:06.274+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:28:06.274+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:28:06.274+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:28:06.275+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:28:06.275+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:28:06.275+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:28:06.275+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:28:06.276+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:28:06.276+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:28:06.279+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:28:06.290+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:28:06.289+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:28:06.302+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:28:06.302+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:28:06.313+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.707 seconds
[2025-05-24T21:28:36.994+0000] {processor.py:161} INFO - Started process (PID=4996) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:28:36.995+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:28:36.996+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:28:36.996+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:28:37.749+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:28:37.749+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:28:37.750+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:28:37.750+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:28:37.750+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:28:37.750+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:28:37.750+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:28:37.750+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:28:37.751+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:28:37.751+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:28:37.754+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:28:37.765+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:28:37.765+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:28:37.776+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:28:37.776+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:28:37.784+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.796 seconds
[2025-05-24T21:29:08.572+0000] {processor.py:161} INFO - Started process (PID=5064) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:29:08.573+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:29:08.574+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:29:08.574+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:29:09.321+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:29:09.321+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:29:09.321+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:29:09.321+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:29:09.324+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:29:09.324+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:29:09.325+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:29:09.324+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:29:09.326+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:29:09.326+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:29:09.328+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:29:09.338+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:29:09.338+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:29:09.351+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:29:09.351+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:29:09.361+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.798 seconds
[2025-05-24T21:29:48.786+0000] {processor.py:161} INFO - Started process (PID=5137) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:29:48.788+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:29:48.789+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:29:48.789+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:29:50.315+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:29:50.314+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:29:50.317+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:29:50.316+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:29:50.317+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:29:50.317+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:29:50.318+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:29:50.318+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:29:50.324+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:29:50.323+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:29:50.332+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:29:50.394+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:29:50.393+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:29:50.435+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:29:50.435+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:29:50.493+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.715 seconds
[2025-05-24T21:30:37.897+0000] {processor.py:161} INFO - Started process (PID=5191) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:30:37.900+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:30:37.903+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:30:37.902+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:30:38.722+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:30:38.721+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:30:38.722+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:30:38.722+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:30:38.723+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:30:38.723+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:30:38.723+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:30:38.723+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:30:38.724+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:30:38.724+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:30:38.730+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:30:38.749+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:30:38.748+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:30:38.767+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:30:38.767+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:30:38.781+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.901 seconds
[2025-05-24T21:31:08.903+0000] {processor.py:161} INFO - Started process (PID=5250) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:31:08.905+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:31:08.907+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:31:08.906+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:31:09.785+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:31:09.785+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:31:09.786+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:31:09.786+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:31:09.786+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:31:09.786+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:31:09.786+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:31:09.786+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:31:09.787+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:31:09.787+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:31:09.791+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:31:09.817+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:31:09.816+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:31:09.833+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:31:09.833+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:31:09.843+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.949 seconds
[2025-05-24T21:31:40.026+0000] {processor.py:161} INFO - Started process (PID=5312) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:31:40.028+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:31:40.029+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:31:40.029+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:31:40.903+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:31:40.902+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:31:40.903+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:31:40.903+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:31:40.903+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:31:40.903+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:31:40.903+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:31:40.903+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:31:40.904+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:31:40.904+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:31:40.907+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:31:40.941+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:31:40.941+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:31:40.955+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:31:40.954+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:31:40.966+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.958 seconds
[2025-05-24T21:32:11.299+0000] {processor.py:161} INFO - Started process (PID=5374) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:32:11.301+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:32:11.304+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:32:11.303+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:32:12.483+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:32:12.482+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:32:12.483+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:32:12.483+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:32:12.483+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:32:12.483+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:32:12.484+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:32:12.484+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:32:12.485+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:32:12.485+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:32:12.490+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:32:12.546+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:32:12.546+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:32:12.601+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:32:12.600+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:32:12.617+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.340 seconds
[2025-05-24T21:32:43.627+0000] {processor.py:161} INFO - Started process (PID=5442) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:32:43.631+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:32:43.632+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:32:43.632+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:32:44.100+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:32:44.099+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:32:44.100+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:32:44.100+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:32:44.101+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:32:44.101+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:32:44.101+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:32:44.101+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:32:44.102+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:32:44.102+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:32:44.107+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:32:44.128+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:32:44.127+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:32:44.145+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:32:44.145+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:32:44.155+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.542 seconds
[2025-05-24T21:33:14.365+0000] {processor.py:161} INFO - Started process (PID=5504) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:33:14.366+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:33:14.367+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:33:14.367+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:33:14.863+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:33:14.862+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:33:14.863+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:33:14.863+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:33:14.863+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:33:14.863+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:33:14.863+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:33:14.863+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:33:14.864+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:33:14.864+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:33:14.867+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:33:14.885+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:33:14.885+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:33:14.902+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:33:14.902+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:33:14.913+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.557 seconds
[2025-05-24T21:33:45.239+0000] {processor.py:161} INFO - Started process (PID=5566) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:33:45.240+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:33:45.241+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:33:45.241+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:33:45.990+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:33:45.990+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:33:45.990+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:33:45.990+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:33:45.990+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:33:45.990+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:33:45.991+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:33:45.991+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:33:45.991+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:33:45.991+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:33:45.995+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:33:46.030+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:33:46.030+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:33:46.047+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:33:46.046+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:33:46.056+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.824 seconds
[2025-05-24T21:34:16.171+0000] {processor.py:161} INFO - Started process (PID=5627) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:34:16.173+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:34:16.175+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:34:16.174+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:34:17.144+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:34:17.144+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:34:17.145+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:34:17.145+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:34:17.145+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:34:17.145+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:34:17.145+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:34:17.145+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:34:17.148+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:34:17.148+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:34:17.153+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:34:17.187+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:34:17.186+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:34:17.202+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:34:17.201+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:34:17.212+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.056 seconds
[2025-05-24T21:34:47.722+0000] {processor.py:161} INFO - Started process (PID=5689) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:34:47.724+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:34:47.726+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:34:47.726+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:34:48.676+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:34:48.674+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:34:48.676+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:34:48.676+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:34:48.676+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:34:48.676+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:34:48.676+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:34:48.676+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:34:48.678+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:34:48.678+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:34:48.683+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:34:48.711+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:34:48.710+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:34:48.726+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:34:48.726+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:34:48.737+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.040 seconds
[2025-05-24T21:35:19.123+0000] {processor.py:161} INFO - Started process (PID=5761) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:35:19.125+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:35:19.126+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:35:19.125+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:35:20.096+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:35:20.094+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:35:20.096+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:35:20.096+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:35:20.097+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:35:20.096+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:35:20.097+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:35:20.097+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:35:20.099+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:35:20.099+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:35:20.104+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:35:20.152+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:35:20.151+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:35:20.175+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:35:20.174+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:35:20.190+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.076 seconds
[2025-05-24T21:35:50.664+0000] {processor.py:161} INFO - Started process (PID=5828) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:35:50.672+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:35:50.677+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:35:50.676+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:35:51.839+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:35:51.838+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:35:51.840+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:35:51.840+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:35:51.843+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:35:51.843+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:35:51.844+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:35:51.844+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:35:51.853+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:35:51.852+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:35:51.879+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:35:51.937+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:35:51.936+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:35:51.975+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:35:51.975+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:35:52.006+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.382 seconds
[2025-05-24T21:36:22.306+0000] {processor.py:161} INFO - Started process (PID=5890) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:36:22.308+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:36:22.309+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:36:22.308+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:36:22.797+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:36:22.796+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:36:22.797+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:36:22.797+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:36:22.797+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:36:22.797+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:36:22.798+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:36:22.797+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:36:22.801+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:36:22.801+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:36:22.811+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:36:22.863+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:36:22.863+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:36:22.892+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:36:22.892+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:36:22.910+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.614 seconds
[2025-05-24T21:36:53.267+0000] {processor.py:161} INFO - Started process (PID=5952) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:36:53.269+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:36:53.271+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:36:53.270+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:36:54.064+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:36:54.064+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:36:54.064+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:36:54.064+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:36:54.065+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:36:54.065+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:36:54.065+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:36:54.065+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:36:54.071+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:36:54.071+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:36:54.086+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:36:54.160+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:36:54.159+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:36:54.205+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:36:54.204+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:36:54.232+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.976 seconds
[2025-05-24T21:37:24.481+0000] {processor.py:161} INFO - Started process (PID=6014) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:37:24.484+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:37:24.486+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:37:24.486+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:37:24.834+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:37:24.834+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:37:24.835+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:37:24.835+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:37:24.835+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:37:24.835+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:37:24.835+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:37:24.835+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:37:24.838+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:37:24.838+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:37:24.844+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:37:24.880+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:37:24.880+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:37:24.899+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:37:24.899+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:37:24.911+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.443 seconds
[2025-05-24T21:37:55.036+0000] {processor.py:161} INFO - Started process (PID=6081) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:37:55.037+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:37:55.039+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:37:55.038+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:37:55.503+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:37:55.503+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:37:55.504+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:37:55.503+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:37:55.504+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:37:55.504+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:37:55.504+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:37:55.504+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:37:55.511+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:37:55.511+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:37:55.526+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:37:55.554+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:37:55.553+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:37:55.572+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:37:55.572+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:37:55.583+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.559 seconds
[2025-05-24T21:38:25.795+0000] {processor.py:161} INFO - Started process (PID=6143) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:38:25.798+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:38:25.799+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:38:25.799+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:38:26.241+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:38:26.240+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:38:26.241+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:38:26.241+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:38:26.241+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:38:26.241+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:38:26.241+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:38:26.241+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:38:26.246+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:38:26.246+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:38:26.269+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:38:26.293+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:38:26.293+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:38:26.316+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:38:26.316+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:38:26.327+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.543 seconds
[2025-05-24T21:38:56.488+0000] {processor.py:161} INFO - Started process (PID=6205) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:38:56.491+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:38:56.493+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:38:56.492+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:38:57.042+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:38:57.042+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:38:57.043+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:38:57.043+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:38:57.043+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:38:57.043+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:38:57.043+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:38:57.043+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:38:57.047+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:38:57.047+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:38:57.054+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:38:57.090+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:38:57.090+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:38:57.115+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:38:57.114+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:38:57.126+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.651 seconds
[2025-05-24T21:39:27.380+0000] {processor.py:161} INFO - Started process (PID=6267) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:39:27.383+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:39:27.384+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:39:27.383+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:39:27.785+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:39:27.784+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:39:27.785+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:39:27.785+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:39:27.785+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:39:27.785+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:39:27.786+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:39:27.786+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:39:27.789+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:39:27.789+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:39:27.798+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:39:27.841+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:39:27.841+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:39:27.867+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:39:27.867+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:39:27.880+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.511 seconds
[2025-05-24T21:39:57.945+0000] {processor.py:161} INFO - Started process (PID=6334) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:39:57.949+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:39:57.952+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:39:57.950+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:39:58.409+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:39:58.408+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:39:58.409+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:39:58.409+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:39:58.409+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:39:58.409+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:39:58.409+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:39:58.409+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:39:58.415+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:39:58.414+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:39:58.422+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:39:58.451+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:39:58.451+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:39:58.489+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:39:58.489+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:39:58.510+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.583 seconds
[2025-05-24T21:40:28.854+0000] {processor.py:161} INFO - Started process (PID=6401) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:40:28.856+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:40:28.857+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:40:28.856+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:40:29.201+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:40:29.201+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:40:29.202+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:40:29.202+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:40:29.203+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:40:29.203+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:40:29.204+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:40:29.203+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:40:29.205+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:40:29.205+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:40:29.211+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:40:29.226+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:40:29.226+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:40:29.245+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:40:29.245+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:40:29.262+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.415 seconds
[2025-05-24T21:40:59.485+0000] {processor.py:161} INFO - Started process (PID=6469) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:40:59.486+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:40:59.487+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:40:59.487+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:40:59.891+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:40:59.890+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:40:59.892+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:40:59.892+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:40:59.892+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:40:59.892+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:40:59.893+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:40:59.893+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:40:59.900+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:40:59.900+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:40:59.909+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:40:59.937+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:40:59.937+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:40:59.971+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:40:59.971+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:40:59.984+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.505 seconds
[2025-05-24T21:41:30.819+0000] {processor.py:161} INFO - Started process (PID=6530) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:41:30.824+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:41:30.826+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:41:30.826+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:41:31.186+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:41:31.185+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:41:31.186+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:41:31.186+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:41:31.186+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:41:31.186+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:41:31.187+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:41:31.187+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:41:31.191+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:41:31.191+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:41:31.197+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:41:31.222+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:41:31.222+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:41:31.241+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:41:31.241+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:41:31.253+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.443 seconds
[2025-05-24T21:42:01.362+0000] {processor.py:161} INFO - Started process (PID=6592) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:42:01.367+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:42:01.368+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:42:01.368+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:42:01.795+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:42:01.794+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:42:01.795+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:42:01.795+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:42:01.795+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:42:01.795+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:42:01.796+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:42:01.796+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:42:01.800+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:42:01.800+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:42:01.807+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:42:01.827+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:42:01.827+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:42:01.849+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:42:01.848+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:42:01.861+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.513 seconds
[2025-05-24T21:42:31.978+0000] {processor.py:161} INFO - Started process (PID=6654) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:42:31.979+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:42:31.981+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:42:31.980+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:42:32.411+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:42:32.410+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:42:32.411+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:42:32.411+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:42:32.411+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:42:32.411+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:42:32.412+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:42:32.412+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:42:32.416+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:42:32.415+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:42:32.424+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:42:32.448+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:42:32.447+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:42:32.464+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:42:32.464+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:42:32.478+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.509 seconds
[2025-05-24T21:43:02.606+0000] {processor.py:161} INFO - Started process (PID=6721) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:43:02.608+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:43:02.609+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:43:02.608+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:43:03.181+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:43:03.181+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:43:03.182+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:43:03.181+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:43:03.182+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:43:03.182+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:43:03.182+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:43:03.182+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:43:03.187+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:43:03.186+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:43:03.197+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:43:03.244+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:43:03.243+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:43:03.292+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:43:03.292+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:43:03.312+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.715 seconds
[2025-05-24T21:43:34.123+0000] {processor.py:161} INFO - Started process (PID=6783) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:43:34.124+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:43:34.125+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:43:34.125+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:43:34.633+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:43:34.633+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:43:34.634+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:43:34.634+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:43:34.634+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:43:34.634+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:43:34.634+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:43:34.634+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:43:34.642+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:43:34.642+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:43:34.651+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:43:34.682+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:43:34.681+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:43:34.708+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:43:34.708+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:43:34.720+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.600 seconds
[2025-05-24T21:44:04.832+0000] {processor.py:161} INFO - Started process (PID=6845) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:44:04.833+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:44:04.834+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:44:04.834+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:44:05.447+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:44:05.446+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:44:05.447+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:44:05.447+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:44:05.448+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:44:05.448+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:44:05.449+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:44:05.449+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:44:05.456+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:44:05.455+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:44:05.467+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:44:05.501+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:44:05.500+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:44:05.527+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:44:05.527+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:44:05.541+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.729 seconds
[2025-05-24T21:44:36.082+0000] {processor.py:161} INFO - Started process (PID=6907) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:44:36.085+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:44:36.086+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:44:36.086+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:44:36.492+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:44:36.492+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:44:36.493+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:44:36.493+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:44:36.493+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:44:36.493+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:44:36.493+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:44:36.493+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:44:36.496+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:44:36.496+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:44:36.501+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:44:36.519+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:44:36.518+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:44:36.532+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:44:36.532+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:44:36.548+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.471 seconds
[2025-05-24T21:45:07.016+0000] {processor.py:161} INFO - Started process (PID=6974) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:45:07.018+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:45:07.018+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:45:07.018+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:45:07.678+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:45:07.678+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:45:07.678+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:45:07.678+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:45:07.679+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:45:07.679+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:45:07.679+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:45:07.679+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:45:07.683+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:45:07.683+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:45:07.689+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:45:07.725+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:45:07.725+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:45:07.740+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:45:07.739+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:45:07.751+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.741 seconds
[2025-05-24T21:45:38.144+0000] {processor.py:161} INFO - Started process (PID=7040) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:45:38.145+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:45:38.145+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:45:38.145+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:45:38.655+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:45:38.655+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:45:38.655+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:45:38.655+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:45:38.655+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:45:38.655+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:45:38.656+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:45:38.656+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:45:38.660+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:45:38.659+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:45:38.668+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:45:38.693+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:45:38.693+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:45:38.715+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:45:38.715+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:45:38.727+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.589 seconds
[2025-05-24T21:46:24.102+0000] {processor.py:161} INFO - Started process (PID=7113) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:46:24.107+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:46:24.110+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:46:24.109+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:46:24.667+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:46:24.667+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:46:24.668+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:46:24.668+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:46:24.670+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:46:24.670+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:46:24.671+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:46:24.671+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:46:24.674+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:46:24.674+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:46:24.683+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:46:24.722+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:46:24.721+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:46:24.761+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:46:24.761+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:46:25.292+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.205 seconds
[2025-05-24T21:47:00.736+0000] {processor.py:161} INFO - Started process (PID=7166) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:47:00.739+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:47:00.742+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:47:00.741+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:47:01.381+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:47:01.381+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:47:01.382+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:47:01.382+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:47:01.382+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:47:01.382+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:47:01.382+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:47:01.382+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:47:01.393+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:47:01.393+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:47:01.401+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:47:01.428+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:47:01.427+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:47:01.451+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:47:01.450+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:47:01.855+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.145 seconds
[2025-05-24T21:47:32.311+0000] {processor.py:161} INFO - Started process (PID=7235) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:47:32.312+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:47:32.312+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:47:32.312+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:47:32.560+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:47:32.559+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:47:32.560+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:47:32.560+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:47:32.560+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:47:32.560+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:47:32.560+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:47:32.560+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:47:32.563+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:47:32.563+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:47:32.568+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:47:32.586+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:47:32.585+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:47:32.601+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:47:32.601+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:47:32.950+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.646 seconds
[2025-05-24T21:48:03.031+0000] {processor.py:161} INFO - Started process (PID=7304) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:48:03.036+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:48:03.037+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:48:03.037+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:48:03.501+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:48:03.501+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:48:03.501+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:48:03.501+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:48:03.502+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:48:03.502+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:48:03.502+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:48:03.502+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:48:03.505+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:48:03.505+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:48:03.514+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:48:03.540+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:48:03.539+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:48:03.575+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:48:03.575+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:48:03.593+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.577 seconds
[2025-05-24T21:48:34.248+0000] {processor.py:161} INFO - Started process (PID=7365) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:48:34.249+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:48:34.250+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:48:34.250+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:48:34.831+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:48:34.831+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:48:34.832+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:48:34.832+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:48:34.832+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:48:34.832+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:48:34.832+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:48:34.832+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:48:34.836+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:48:34.836+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:48:34.852+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:48:34.878+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:48:34.878+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:48:34.899+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:48:34.899+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:48:34.913+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.672 seconds
[2025-05-24T21:52:34.438+0000] {processor.py:161} INFO - Started process (PID=7432) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:52:34.442+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:52:34.448+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:52:34.445+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:52:37.340+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:52:37.333+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:52:37.341+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:52:37.341+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:52:37.341+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:52:37.341+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:52:37.342+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:52:37.342+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:52:37.349+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:52:37.349+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:52:37.364+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:52:37.449+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:52:37.447+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:52:37.536+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:52:37.536+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:52:37.614+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.255 seconds
[2025-05-24T21:53:59.624+0000] {processor.py:161} INFO - Started process (PID=7494) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:53:59.631+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:53:59.638+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:53:59.637+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:54:00.588+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:54:00.587+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:54:00.588+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:54:00.588+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:54:00.589+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:54:00.589+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:54:00.589+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:54:00.589+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:54:00.596+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:54:00.596+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:54:00.612+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:54:01.501+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:54:01.501+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:54:01.546+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:54:01.546+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:54:01.567+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.979 seconds
[2025-05-24T21:54:31.828+0000] {processor.py:161} INFO - Started process (PID=7557) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:54:31.830+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:54:31.832+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:54:31.831+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:54:32.478+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:54:32.477+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:54:32.478+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:54:32.478+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:54:32.479+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:54:32.478+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:54:32.479+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:54:32.479+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:54:32.481+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:54:32.481+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:54:32.485+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:54:32.848+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:54:32.848+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:54:32.897+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:54:32.897+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:54:32.924+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.121 seconds
[2025-05-24T21:55:02.980+0000] {processor.py:161} INFO - Started process (PID=7626) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:55:02.981+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:55:02.982+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:55:02.982+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:55:03.629+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:55:03.629+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:55:03.629+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:55:03.629+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:55:03.630+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:55:03.630+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:55:03.630+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:55:03.630+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:55:03.630+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:55:03.630+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:55:03.633+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:55:03.645+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:55:03.644+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:55:03.667+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:55:03.667+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:55:03.678+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.705 seconds
[2025-05-24T21:55:34.121+0000] {processor.py:161} INFO - Started process (PID=7688) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:55:34.122+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:55:34.123+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:55:34.123+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:55:34.351+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:55:34.351+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:55:34.351+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:55:34.351+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:55:34.352+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:55:34.351+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:55:34.352+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:55:34.352+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:55:34.353+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:55:34.353+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:55:34.356+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:55:34.610+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:55:34.609+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:55:34.622+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:55:34.622+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:55:34.630+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.513 seconds
[2025-05-24T21:56:04.755+0000] {processor.py:161} INFO - Started process (PID=7743) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:56:04.757+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:56:04.758+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:56:04.757+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:56:05.451+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:56:05.451+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:56:05.452+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:56:05.451+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:56:05.452+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:56:05.452+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:56:05.453+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:56:05.452+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:56:05.454+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:56:05.454+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:56:05.457+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:56:05.471+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:56:05.471+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:56:05.487+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:56:05.487+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:56:05.500+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.765 seconds
[2025-05-24T21:56:35.805+0000] {processor.py:161} INFO - Started process (PID=7805) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:56:35.807+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:56:35.808+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:56:35.808+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:56:36.600+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:56:36.599+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:56:36.601+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:56:36.601+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:56:36.603+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:56:36.603+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:56:36.604+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:56:36.604+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:56:36.609+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:56:36.609+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:56:36.615+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:56:36.635+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:56:36.635+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:56:36.663+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:56:36.663+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:56:36.689+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.897 seconds
[2025-05-24T21:57:06.764+0000] {processor.py:161} INFO - Started process (PID=7867) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:57:06.765+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:57:06.766+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:57:06.765+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:57:07.281+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:57:07.281+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:57:07.281+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:57:07.281+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:57:07.281+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:57:07.281+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:57:07.282+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:57:07.282+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:57:07.282+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:57:07.282+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:57:07.286+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:57:07.297+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:57:07.296+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:57:07.307+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:57:07.307+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:57:07.316+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.556 seconds
[2025-05-24T21:57:37.637+0000] {processor.py:161} INFO - Started process (PID=7926) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:57:37.639+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:57:37.639+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:57:37.639+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:57:38.450+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:57:38.449+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:57:38.450+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:57:38.450+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:57:38.450+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:57:38.450+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:57:38.450+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:57:38.450+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:57:38.451+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:57:38.451+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:57:38.454+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:57:38.479+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:57:38.478+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:57:38.494+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:57:38.494+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:57:38.505+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.877 seconds
[2025-05-24T21:58:08.817+0000] {processor.py:161} INFO - Started process (PID=7988) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:58:08.819+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:58:08.820+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:58:08.819+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:58:09.617+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:58:09.617+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:58:09.617+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:58:09.617+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:58:09.618+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:58:09.618+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:58:09.618+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:58:09.618+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:58:09.619+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:58:09.619+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:58:09.622+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:58:09.644+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:58:09.644+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:58:09.662+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:58:09.661+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:58:09.675+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.870 seconds
[2025-05-24T21:58:39.950+0000] {processor.py:161} INFO - Started process (PID=8050) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:58:39.952+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:58:39.953+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:58:39.953+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:58:40.817+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:58:40.817+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:58:40.817+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:58:40.817+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:58:40.818+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:58:40.818+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:58:40.818+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:58:40.818+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:58:40.820+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:58:40.820+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:58:40.824+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:58:40.848+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:58:40.847+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:58:40.861+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:58:40.861+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:58:40.872+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.938 seconds
[2025-05-24T21:59:11.599+0000] {processor.py:161} INFO - Started process (PID=8112) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:59:11.601+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:59:11.604+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:59:11.603+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:59:12.560+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:59:12.560+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:59:12.561+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:59:12.560+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:59:12.561+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:59:12.561+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:59:12.561+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:59:12.561+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:59:12.563+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:59:12.562+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:59:12.565+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:59:12.605+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:59:12.605+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:59:12.619+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:59:12.619+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:59:12.632+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.045 seconds
[2025-05-24T21:59:43.239+0000] {processor.py:161} INFO - Started process (PID=8174) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:59:43.242+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T21:59:43.243+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:59:43.242+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:59:44.167+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:59:44.167+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T21:59:44.168+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:59:44.167+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T21:59:44.168+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:59:44.168+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T21:59:44.168+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:59:44.168+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T21:59:44.170+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:59:44.170+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T21:59:44.175+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T21:59:44.191+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:59:44.191+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T21:59:44.210+0000] {logging_mixin.py:188} INFO - [2025-05-24T21:59:44.209+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T21:59:44.230+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.998 seconds
[2025-05-24T22:00:14.659+0000] {processor.py:161} INFO - Started process (PID=8236) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:00:14.662+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:00:14.663+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:00:14.662+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:00:15.609+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:00:15.609+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:00:15.609+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:00:15.609+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:00:15.610+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:00:15.610+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:00:15.610+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:00:15.610+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:00:15.612+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:00:15.612+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:00:15.615+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:00:15.635+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:00:15.634+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:00:15.650+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:00:15.649+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:00:15.662+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.010 seconds
[2025-05-24T22:00:46.130+0000] {processor.py:161} INFO - Started process (PID=8298) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:00:46.137+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:00:46.139+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:00:46.138+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:00:47.146+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:00:47.145+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:00:47.146+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:00:47.146+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:00:47.146+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:00:47.146+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:00:47.146+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:00:47.146+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:00:47.148+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:00:47.148+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:00:47.151+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:00:47.177+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:00:47.176+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:00:47.192+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:00:47.192+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:00:47.202+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.092 seconds
[2025-05-24T22:01:17.985+0000] {processor.py:161} INFO - Started process (PID=8360) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:01:17.987+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:01:17.989+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:01:17.988+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:01:19.027+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:01:19.027+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:01:19.027+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:01:19.027+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:01:19.028+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:01:19.028+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:01:19.028+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:01:19.028+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:01:19.032+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:01:19.032+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:01:19.046+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:01:19.066+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:01:19.066+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:01:19.081+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:01:19.081+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:01:19.092+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.119 seconds
[2025-05-24T22:01:49.903+0000] {processor.py:161} INFO - Started process (PID=8422) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:01:49.905+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:01:49.906+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:01:49.905+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:01:50.842+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:01:50.842+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:01:50.843+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:01:50.843+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:01:50.843+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:01:50.843+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:01:50.843+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:01:50.843+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:01:50.845+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:01:50.845+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:01:50.849+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:01:50.881+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:01:50.880+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:01:50.900+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:01:50.900+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:01:50.915+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.027 seconds
[2025-05-24T22:02:21.771+0000] {processor.py:161} INFO - Started process (PID=8490) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:02:21.774+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:02:21.775+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:02:21.774+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:02:22.720+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:02:22.720+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:02:22.720+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:02:22.720+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:02:22.721+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:02:22.721+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:02:22.721+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:02:22.721+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:02:22.722+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:02:22.722+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:02:22.727+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:02:22.758+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:02:22.758+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:02:22.779+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:02:22.779+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:02:22.791+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.036 seconds
[2025-05-24T22:02:53.086+0000] {processor.py:161} INFO - Started process (PID=8552) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:02:53.093+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:02:53.095+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:02:53.094+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:02:54.644+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:02:54.643+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:02:54.644+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:02:54.644+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:02:54.644+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:02:54.644+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:02:54.644+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:02:54.644+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:02:54.647+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:02:54.647+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:02:54.652+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:02:54.692+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:02:54.691+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:02:54.715+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:02:54.714+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:02:54.735+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.673 seconds
[2025-05-24T22:03:25.345+0000] {processor.py:161} INFO - Started process (PID=8613) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:03:25.346+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:03:25.347+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:03:25.347+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:03:26.158+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:03:26.158+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:03:26.159+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:03:26.159+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:03:26.159+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:03:26.159+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:03:26.159+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:03:26.159+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:03:26.160+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:03:26.160+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:03:26.164+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:03:26.181+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:03:26.181+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:03:26.214+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:03:26.214+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:03:26.224+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.886 seconds
[2025-05-24T22:03:56.491+0000] {processor.py:161} INFO - Started process (PID=8675) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:03:56.493+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:03:56.495+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:03:56.494+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:03:57.182+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:03:57.182+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:03:57.183+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:03:57.183+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:03:57.183+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:03:57.183+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:03:57.183+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:03:57.183+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:03:57.185+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:03:57.185+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:03:57.188+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:03:57.212+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:03:57.212+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:03:57.229+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:03:57.229+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:03:57.240+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.757 seconds
[2025-05-24T22:04:27.289+0000] {processor.py:161} INFO - Started process (PID=8737) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:04:27.291+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:04:27.291+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:04:27.291+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:04:28.154+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:04:28.154+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:04:28.155+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:04:28.155+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:04:28.155+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:04:28.155+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:04:28.155+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:04:28.155+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:04:28.156+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:04:28.156+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:04:28.159+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:04:28.181+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:04:28.180+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:04:28.204+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:04:28.204+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:04:28.214+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.931 seconds
[2025-05-24T22:04:58.569+0000] {processor.py:161} INFO - Started process (PID=8799) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:04:58.572+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:04:58.573+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:04:58.573+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:04:59.391+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:04:59.391+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:04:59.392+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:04:59.392+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:04:59.392+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:04:59.392+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:04:59.392+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:04:59.392+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:04:59.394+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:04:59.394+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:04:59.397+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:04:59.418+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:04:59.417+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:04:59.435+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:04:59.435+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:04:59.446+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.889 seconds
[2025-05-24T22:05:29.559+0000] {processor.py:161} INFO - Started process (PID=8861) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:05:29.561+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:05:29.563+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:05:29.562+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:05:30.476+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:05:30.475+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:05:30.476+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:05:30.476+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:05:30.476+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:05:30.476+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:05:30.476+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:05:30.476+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:05:30.478+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:05:30.477+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:05:30.482+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:05:30.503+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:05:30.502+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:05:30.519+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:05:30.519+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:05:30.530+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.977 seconds
[2025-05-24T22:06:00.900+0000] {processor.py:161} INFO - Started process (PID=8923) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:06:00.901+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:06:00.902+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:06:00.902+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:06:01.622+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:06:01.622+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:06:01.622+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:06:01.622+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:06:01.623+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:06:01.623+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:06:01.623+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:06:01.623+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:06:01.627+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:06:01.627+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:06:01.631+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:06:01.650+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:06:01.649+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:06:01.666+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:06:01.666+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:06:01.676+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.786 seconds
[2025-05-24T22:06:32.079+0000] {processor.py:161} INFO - Started process (PID=8985) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:06:32.080+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:06:32.081+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:06:32.081+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:06:32.957+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:06:32.957+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:06:32.958+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:06:32.958+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:06:32.958+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:06:32.958+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:06:32.958+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:06:32.958+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:06:32.960+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:06:32.960+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:06:32.965+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:06:32.984+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:06:32.984+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:06:33.000+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:06:33.000+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:06:33.010+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.940 seconds
[2025-05-24T22:07:03.283+0000] {processor.py:161} INFO - Started process (PID=9047) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:07:03.286+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:07:03.291+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:07:03.290+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:07:04.885+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:07:04.885+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:07:04.886+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:07:04.886+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:07:04.886+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:07:04.886+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:07:04.886+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:07:04.886+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:07:04.889+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:07:04.889+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:07:04.894+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:07:04.958+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:07:04.957+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:07:04.988+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:07:04.988+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:07:05.011+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.738 seconds
[2025-05-24T22:07:35.615+0000] {processor.py:161} INFO - Started process (PID=9109) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:07:35.618+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:07:35.620+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:07:35.619+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:07:36.588+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:07:36.585+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:07:36.589+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:07:36.589+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:07:36.589+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:07:36.589+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:07:36.589+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:07:36.589+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:07:36.591+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:07:36.591+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:07:36.595+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:07:36.628+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:07:36.627+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:07:36.646+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:07:36.645+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:07:36.657+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.060 seconds
[2025-05-24T22:08:07.712+0000] {processor.py:161} INFO - Started process (PID=9171) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:08:07.715+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:08:07.718+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:08:07.716+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:08:08.757+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:08:08.757+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:08:08.757+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:08:08.757+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:08:08.758+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:08:08.757+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:08:08.758+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:08:08.758+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:08:08.760+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:08:08.759+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:08:08.764+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:08:08.812+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:08:08.811+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:08:08.831+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:08:08.831+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:08:08.843+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.158 seconds
[2025-05-24T22:08:38.943+0000] {processor.py:161} INFO - Started process (PID=9233) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:08:38.947+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:08:38.948+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:08:38.947+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:08:39.874+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:08:39.872+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:08:39.875+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:08:39.874+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:08:39.875+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:08:39.875+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:08:39.875+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:08:39.875+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:08:39.877+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:08:39.877+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:08:39.880+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:08:39.918+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:08:39.918+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:08:39.938+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:08:39.938+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:08:39.956+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.024 seconds
[2025-05-24T22:09:10.069+0000] {processor.py:161} INFO - Started process (PID=9294) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:09:10.072+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:09:10.074+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:09:10.073+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:09:11.039+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:09:11.037+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:09:11.039+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:09:11.039+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:09:11.040+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:09:11.040+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:09:11.040+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:09:11.040+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:09:11.042+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:09:11.042+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:09:11.052+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:09:11.097+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:09:11.097+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:09:11.115+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:09:11.115+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:09:11.127+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.078 seconds
[2025-05-24T22:09:41.621+0000] {processor.py:161} INFO - Started process (PID=9356) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:09:41.623+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:09:41.625+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:09:41.624+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:09:42.598+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:09:42.597+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:09:42.598+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:09:42.598+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:09:42.599+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:09:42.599+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:09:42.599+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:09:42.599+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:09:42.601+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:09:42.601+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:09:42.606+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:09:42.642+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:09:42.641+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:09:42.656+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:09:42.656+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:09:42.668+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.059 seconds
[2025-05-24T22:10:13.034+0000] {processor.py:161} INFO - Started process (PID=9418) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:10:13.036+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:10:13.038+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:10:13.037+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:10:14.012+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:10:14.012+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:10:14.012+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:10:14.012+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:10:14.013+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:10:14.013+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:10:14.013+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:10:14.013+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:10:14.015+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:10:14.015+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:10:14.018+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:10:14.054+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:10:14.053+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:10:14.069+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:10:14.069+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:10:14.082+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.071 seconds
[2025-05-24T22:10:45.085+0000] {processor.py:161} INFO - Started process (PID=9480) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:10:45.088+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:10:45.090+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:10:45.089+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:10:47.344+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:10:47.330+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:10:47.346+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:10:47.345+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:10:47.347+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:10:47.346+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:10:47.348+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:10:47.348+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:10:47.372+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:10:47.372+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:10:47.411+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:10:47.540+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:10:47.539+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:10:47.585+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:10:47.583+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:10:47.608+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.539 seconds
[2025-05-24T22:11:17.870+0000] {processor.py:161} INFO - Started process (PID=9542) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:11:17.873+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:11:17.875+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:11:17.874+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:11:18.866+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:11:18.866+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:11:18.866+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:11:18.866+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:11:18.866+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:11:18.866+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:11:18.867+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:11:18.867+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:11:18.868+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:11:18.868+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:11:18.872+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:11:18.917+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:11:18.916+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:11:18.940+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:11:18.939+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:11:18.955+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.108 seconds
[2025-05-24T22:11:49.732+0000] {processor.py:161} INFO - Started process (PID=9605) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:11:49.737+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:11:49.739+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:11:49.738+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:11:50.551+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:11:50.551+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:11:50.552+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:11:50.552+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:11:50.552+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:11:50.552+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:11:50.552+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:11:50.552+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:11:50.553+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:11:50.553+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:11:50.555+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:11:50.565+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:11:50.565+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:11:50.576+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:11:50.576+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:11:50.588+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.889 seconds
[2025-05-24T22:12:21.391+0000] {processor.py:161} INFO - Started process (PID=9671) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:12:21.392+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:12:21.395+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:12:21.394+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:12:22.194+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:12:22.194+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:12:22.195+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:12:22.195+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:12:22.195+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:12:22.195+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:12:22.195+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:12:22.195+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:12:22.196+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:12:22.196+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './airflow/dags/mock_supplier_data.csv'
[2025-05-24T22:12:22.199+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:12:22.215+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:12:22.215+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:12:22.225+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:12:22.225+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:12:22.236+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.853 seconds
[2025-05-24T22:12:52.277+0000] {processor.py:161} INFO - Started process (PID=9740) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:12:52.278+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:12:52.278+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:12:52.278+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:12:53.070+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:12:53.070+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:12:53.070+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:12:53.070+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:12:53.071+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:12:53.071+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:12:53.071+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:12:53.071+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:12:53.073+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:12:53.072+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T22:12:53.077+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:12:53.091+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:12:53.090+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:12:53.111+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:12:53.111+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:12:53.127+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.854 seconds
[2025-05-24T22:13:23.401+0000] {processor.py:161} INFO - Started process (PID=9809) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:13:23.402+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:13:23.403+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:13:23.403+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:13:24.275+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:13:24.275+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T22:13:24.276+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:13:24.276+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T22:13:24.277+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:13:24.277+0000] {etl.py:85} INFO - db source pickled
[2025-05-24T22:13:24.281+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:13:24.281+0000] {etl.py:188} INFO - Starting ETL pipeline
[2025-05-24T22:13:24.282+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:13:24.282+0000] {etl.py:199} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T22:13:24.286+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:13:24.302+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:13:24.301+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:13:24.316+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:13:24.316+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:13:24.326+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.930 seconds
[2025-05-24T22:13:54.430+0000] {processor.py:161} INFO - Started process (PID=9878) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:13:54.431+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:13:54.431+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:13:54.431+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:13:55.311+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:13:55.311+0000] {etl.py:185} INFO - Starting ETL pipeline
[2025-05-24T22:13:55.313+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:13:55.313+0000] {etl.py:196} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T22:13:55.316+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:13:55.326+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:13:55.326+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:13:55.335+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:13:55.335+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:13:55.345+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.918 seconds
[2025-05-24T22:14:25.496+0000] {processor.py:161} INFO - Started process (PID=9941) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:14:25.497+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:14:25.497+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:14:25.497+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:14:25.879+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:14:25.879+0000] {etl.py:185} INFO - Starting ETL pipeline
[2025-05-24T22:14:25.880+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:14:25.880+0000] {etl.py:196} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T22:14:25.883+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:14:25.892+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:14:25.891+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:14:25.904+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:14:25.904+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:14:25.919+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.427 seconds
[2025-05-24T22:14:56.539+0000] {processor.py:161} INFO - Started process (PID=10008) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:14:56.543+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:14:56.543+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:14:56.543+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:14:56.942+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:14:56.942+0000] {etl.py:185} INFO - Starting ETL pipeline
[2025-05-24T22:14:56.943+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:14:56.943+0000] {etl.py:196} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T22:14:56.945+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:14:56.957+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:14:56.957+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:14:56.969+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:14:56.968+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:14:56.978+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.442 seconds
[2025-05-24T22:15:27.251+0000] {processor.py:161} INFO - Started process (PID=10062) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:15:27.251+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:15:27.252+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:15:27.252+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:15:28.099+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:15:28.099+0000] {etl.py:185} INFO - Starting ETL pipeline
[2025-05-24T22:15:28.100+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:15:28.100+0000] {etl.py:196} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T22:15:28.103+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:15:28.133+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:15:28.133+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:15:28.148+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:15:28.148+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:15:28.161+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.915 seconds
[2025-05-24T22:15:59.173+0000] {processor.py:161} INFO - Started process (PID=10123) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:15:59.174+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:15:59.175+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:15:59.174+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:16:00.328+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:16:00.328+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:16:00.330+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:16:00.330+0000] {etl.py:197} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T22:16:00.335+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:16:00.371+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:16:00.371+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:16:00.422+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:16:00.422+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:16:00.590+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.430 seconds
[2025-05-24T22:16:31.550+0000] {processor.py:161} INFO - Started process (PID=10186) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:16:31.551+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:16:31.552+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:16:31.552+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:16:31.936+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:16:31.936+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:16:31.940+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:16:31.940+0000] {etl.py:197} ERROR - ETL failed: [Errno 2] No such file or directory: './mock_supplier_data.csv'
[2025-05-24T22:16:31.943+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:16:31.958+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:16:31.957+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:16:31.970+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:16:31.969+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:16:31.979+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.438 seconds
[2025-05-24T22:17:02.214+0000] {processor.py:161} INFO - Started process (PID=10250) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:17:02.224+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:17:02.230+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:17:02.228+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:17:04.490+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:17:04.490+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:17:04.498+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:17:04.498+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:17:04.506+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:17:04.505+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:17:04.507+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:17:04.507+0000] {etl.py:197} ERROR - ETL failed: [Errno 2] No such file or directory: './opt/airflow/dags/api_suppliers.json'
[2025-05-24T22:17:04.512+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:17:04.527+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:17:04.527+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:17:04.540+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:17:04.540+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:17:04.553+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.357 seconds
[2025-05-24T22:17:35.134+0000] {processor.py:161} INFO - Started process (PID=10318) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:17:35.137+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:17:35.140+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:17:35.138+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:17:36.430+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:17:36.430+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:17:36.446+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:17:36.446+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:17:36.451+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:17:36.451+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:17:36.453+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:17:36.453+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:17:36.455+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:17:36.455+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:17:36.457+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:17:36.457+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:17:36.458+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:17:36.457+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:17:36.459+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:17:36.459+0000] {etl.py:197} ERROR - ETL failed: Execution failed on sql 'SELECT * FROM supplier_records': no such table: supplier_records
[2025-05-24T22:17:36.461+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:17:36.484+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:17:36.483+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:17:36.495+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:17:36.495+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:17:36.506+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.380 seconds
[2025-05-24T22:18:06.597+0000] {processor.py:161} INFO - Started process (PID=10381) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:18:06.598+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:18:06.599+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:06.598+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:18:07.476+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:07.476+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:18:07.488+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:07.487+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:18:07.490+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:07.490+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:18:07.492+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:07.492+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:18:07.494+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:07.494+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:18:07.495+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:07.495+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:18:07.497+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:07.497+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:18:07.502+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:07.502+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:18:07.502+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:07.502+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:18:07.504+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:07.504+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:18:07.505+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:07.505+0000] {etl.py:197} ERROR - ETL failed: [Errno 2] No such file or directory: './tmp/df_csv.pkl'
[2025-05-24T22:18:07.508+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:18:07.521+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:07.521+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:18:07.532+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:07.532+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:18:07.540+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.952 seconds
[2025-05-24T22:18:38.090+0000] {processor.py:161} INFO - Started process (PID=10443) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:18:38.097+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:18:38.151+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:38.120+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:18:39.633+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:39.632+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:18:39.651+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:39.651+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:18:39.656+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:39.655+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:18:39.658+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:39.658+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:18:39.661+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:39.661+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:18:39.663+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:39.663+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:18:39.665+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:39.665+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:18:39.672+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:39.672+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:18:39.672+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:39.672+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:18:39.675+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:39.675+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:18:39.676+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:39.675+0000] {etl.py:197} ERROR - ETL failed: [Errno 2] No such file or directory: './tmp/df_csv.pkl'
[2025-05-24T22:18:39.684+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:18:39.716+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:39.715+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:18:39.735+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:18:39.735+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:18:39.758+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.738 seconds
[2025-05-24T22:19:09.897+0000] {processor.py:161} INFO - Started process (PID=10510) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:19:09.901+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:19:09.902+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:09.901+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:19:10.663+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.663+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:19:10.668+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.668+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:19:10.671+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.671+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:19:10.674+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.674+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:19:10.676+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.676+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:19:10.680+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.680+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:19:10.682+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.681+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:19:10.685+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.685+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:19:10.685+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.685+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:19:10.687+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.687+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:19:10.689+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.688+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:19:10.690+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.689+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:19:10.690+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.690+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:19:10.691+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.691+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:19:10.692+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.692+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:19:10.705+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.704+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:19:10.712+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.712+0000] {etl.py:137} ERROR - Failed to create AWS session: 'Variable aws_access_key_id does not exist'
[2025-05-24T22:19:10.796+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.796+0000] {etl.py:181} ERROR - Unexpected error during S3 upload: Unable to locate credentials
[2025-05-24T22:19:10.796+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.796+0000] {etl.py:197} ERROR - ETL failed: Unable to locate credentials
[2025-05-24T22:19:10.800+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:19:10.806+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.806+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:19:10.827+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:10.826+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:19:10.837+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.955 seconds
[2025-05-24T22:19:41.012+0000] {processor.py:161} INFO - Started process (PID=10575) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:19:41.013+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:19:41.014+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:41.014+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:19:41.419+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:41.419+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:19:41.425+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:41.425+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:19:41.428+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:41.428+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:19:41.430+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:41.430+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:19:41.432+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:41.432+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:19:41.434+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:41.434+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:19:41.434+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:41.434+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:19:41.438+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:41.438+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:19:41.438+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:41.438+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:19:41.440+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:41.440+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:19:41.441+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:41.441+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:19:41.442+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:41.442+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:19:41.443+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:41.443+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:19:41.444+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:41.444+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:19:41.444+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:41.444+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:19:41.456+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:41.456+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:19:41.467+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:41.467+0000] {etl.py:137} ERROR - Failed to create AWS session: 'Variable aws_access_key_id does not exist'
[2025-05-24T22:19:43.584+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:43.584+0000] {etl.py:181} ERROR - Unexpected error during S3 upload: Unable to locate credentials
[2025-05-24T22:19:43.584+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:43.584+0000] {etl.py:197} ERROR - ETL failed: Unable to locate credentials
[2025-05-24T22:19:43.593+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:19:43.612+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:43.611+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:19:43.625+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:19:43.625+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:19:43.634+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.625 seconds
[2025-05-24T22:20:14.564+0000] {processor.py:161} INFO - Started process (PID=10651) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:20:14.569+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:20:14.571+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:14.570+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:20:15.088+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:15.088+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:20:15.103+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:15.103+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:20:15.108+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:15.108+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:20:15.110+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:15.110+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:20:15.113+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:15.112+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:20:15.114+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:15.114+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:20:15.115+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:15.115+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:20:15.120+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:15.119+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:20:15.120+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:15.120+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:20:15.122+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:15.122+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:20:15.124+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:15.124+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:20:15.125+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:15.125+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:20:15.126+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:15.126+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:20:15.127+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:15.127+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:20:15.128+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:15.127+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:20:15.140+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:15.140+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:20:15.148+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:15.147+0000] {etl.py:137} ERROR - Failed to create AWS session: 'Variable aws_access_key_id does not exist'
[2025-05-24T22:20:17.243+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:17.242+0000] {etl.py:181} ERROR - Unexpected error during S3 upload: Unable to locate credentials
[2025-05-24T22:20:17.243+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:17.243+0000] {etl.py:197} ERROR - ETL failed: Unable to locate credentials
[2025-05-24T22:20:17.253+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:20:17.262+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:17.262+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:20:17.301+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:17.300+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:20:17.313+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.763 seconds
[2025-05-24T22:20:47.589+0000] {processor.py:161} INFO - Started process (PID=10732) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:20:47.590+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:20:47.591+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:47.591+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:20:47.905+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:47.905+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:20:47.914+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:47.914+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:20:47.918+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:47.918+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:20:47.921+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:47.921+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:20:47.924+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:47.924+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:20:47.925+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:47.925+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:20:47.926+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:47.926+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:20:47.932+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:47.931+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:20:47.932+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:47.932+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:20:47.934+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:47.934+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:20:47.937+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:47.937+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:20:47.939+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:47.939+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:20:47.941+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:47.941+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:20:47.942+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:47.942+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:20:47.942+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:47.942+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:20:47.959+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:47.958+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:20:47.984+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:47.984+0000] {etl.py:137} ERROR - Failed to create AWS session: 'Variable aws_access_key_id does not exist'
[2025-05-24T22:20:50.575+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:50.575+0000] {etl.py:181} ERROR - Unexpected error during S3 upload: Unable to locate credentials
[2025-05-24T22:20:50.576+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:50.576+0000] {etl.py:197} ERROR - ETL failed: Unable to locate credentials
[2025-05-24T22:20:50.580+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:20:50.588+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:50.588+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:20:50.607+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:20:50.607+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:20:50.620+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.036 seconds
[2025-05-24T22:21:20.933+0000] {processor.py:161} INFO - Started process (PID=10799) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:21:20.935+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:21:20.942+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:20.939+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:21:21.475+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:21.474+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:21:21.493+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:21.493+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:21:21.501+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:21.501+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:21:21.503+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:21.503+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:21:21.506+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:21.506+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:21:21.514+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:21.514+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:21:21.520+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:21.519+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:21:21.535+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:21.535+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:21:21.536+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:21.536+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:21:21.538+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:21.538+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:21:21.540+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:21.540+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:21:21.542+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:21.542+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:21:21.543+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:21.543+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:21:21.545+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:21.545+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:21:21.545+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:21.545+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:21:21.562+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:21.562+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:21:21.596+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:21.596+0000] {etl.py:137} ERROR - Failed to create AWS session: 'Variable aws_access_key_id does not exist'
[2025-05-24T22:21:24.177+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:24.177+0000] {etl.py:181} ERROR - Unexpected error during S3 upload: Unable to locate credentials
[2025-05-24T22:21:24.178+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:24.178+0000] {etl.py:197} ERROR - ETL failed: Unable to locate credentials
[2025-05-24T22:21:24.182+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:21:24.192+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:24.192+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:21:24.205+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:24.205+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:21:24.215+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.295 seconds
[2025-05-24T22:21:54.757+0000] {processor.py:161} INFO - Started process (PID=10865) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:21:54.759+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:21:54.765+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:54.763+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:21:55.310+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:55.310+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:21:55.322+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:55.322+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:21:55.325+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:55.324+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:21:55.326+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:55.326+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:21:55.328+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:55.328+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:21:55.330+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:55.330+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:21:55.330+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:55.330+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:21:55.334+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:55.334+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:21:55.334+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:55.334+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:21:55.336+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:55.336+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:21:55.337+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:55.337+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:21:55.338+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:55.338+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:21:55.339+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:55.339+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:21:55.340+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:55.340+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:21:55.340+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:55.340+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:21:55.352+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:55.351+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:21:55.374+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:55.374+0000] {etl.py:137} ERROR - Failed to create AWS session: 'Variable aws_access_key_id does not exist'
[2025-05-24T22:21:57.576+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:57.575+0000] {etl.py:181} ERROR - Unexpected error during S3 upload: Unable to locate credentials
[2025-05-24T22:21:57.576+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:57.576+0000] {etl.py:197} ERROR - ETL failed: Unable to locate credentials
[2025-05-24T22:21:57.847+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:21:57.865+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:57.864+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:21:57.884+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:21:57.884+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:21:57.900+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.157 seconds
[2025-05-24T22:22:28.134+0000] {processor.py:161} INFO - Started process (PID=10931) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:22:28.136+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:22:28.138+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:28.137+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:22:28.966+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:28.966+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:22:28.990+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:28.990+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:22:28.994+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:28.994+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:22:28.997+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:28.997+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:22:29.000+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:29.000+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:22:29.002+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:29.002+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:22:29.004+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:29.004+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:22:29.017+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:29.017+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:22:29.018+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:29.018+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:22:29.020+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:29.020+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:22:29.023+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:29.023+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:22:29.025+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:29.025+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:22:29.027+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:29.026+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:22:29.030+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:29.029+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:22:29.031+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:29.031+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:22:29.066+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:29.066+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:22:29.110+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:29.110+0000] {etl.py:137} ERROR - Failed to create AWS session: 'Variable aws_access_key_id does not exist'
[2025-05-24T22:22:31.946+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:31.934+0000] {etl.py:181} ERROR - Unexpected error during S3 upload: Unable to locate credentials
[2025-05-24T22:22:31.949+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:31.949+0000] {etl.py:197} ERROR - ETL failed: Unable to locate credentials
[2025-05-24T22:22:32.022+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:22:33.136+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:33.114+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:22:33.175+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:22:33.175+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:22:33.215+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 5.108 seconds
[2025-05-24T22:23:03.816+0000] {processor.py:161} INFO - Started process (PID=11005) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:23:03.824+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:23:03.826+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:03.826+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:23:04.622+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:04.622+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:23:04.636+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:04.635+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:23:04.639+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:04.639+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:23:04.644+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:04.644+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:23:04.649+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:04.649+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:23:04.652+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:04.652+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:23:04.653+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:04.653+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:23:04.660+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:04.660+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:23:04.660+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:04.660+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:23:04.662+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:04.662+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:23:04.664+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:04.664+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:23:04.666+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:04.666+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:23:04.667+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:04.667+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:23:04.668+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:04.668+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:23:04.669+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:04.669+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:23:04.686+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:04.686+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:23:04.712+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:04.712+0000] {etl.py:137} ERROR - Failed to create AWS session: 'Variable aws_access_key_id does not exist'
[2025-05-24T22:23:05.106+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:05.106+0000] {etl.py:181} ERROR - Unexpected error during S3 upload: Unable to locate credentials
[2025-05-24T22:23:05.106+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:05.106+0000] {etl.py:197} ERROR - ETL failed: Unable to locate credentials
[2025-05-24T22:23:05.500+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:23:05.510+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:05.510+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:23:05.536+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:05.536+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:23:05.562+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.770 seconds
[2025-05-24T22:23:36.240+0000] {processor.py:161} INFO - Started process (PID=11073) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:23:36.241+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:23:36.242+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:36.242+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:23:36.706+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:36.706+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:23:36.716+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:36.715+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:23:36.719+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:36.718+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:23:36.720+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:36.720+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:23:36.723+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:36.723+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:23:36.725+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:36.725+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:23:36.725+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:36.725+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:23:36.730+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:36.729+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:23:36.730+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:36.730+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:23:36.731+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:36.731+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:23:36.734+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:36.734+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:23:36.736+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:36.736+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:23:36.739+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:36.739+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:23:36.741+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:36.741+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:23:36.744+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:36.744+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:23:36.762+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:36.761+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:23:36.773+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:36.773+0000] {etl.py:137} ERROR - Failed to create AWS session: 'Variable aws_access_key_id does not exist'
[2025-05-24T22:23:38.870+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:38.870+0000] {etl.py:181} ERROR - Unexpected error during S3 upload: Unable to locate credentials
[2025-05-24T22:23:38.870+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:38.870+0000] {etl.py:197} ERROR - ETL failed: Unable to locate credentials
[2025-05-24T22:23:39.401+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:23:39.409+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:39.409+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:23:39.424+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:23:39.424+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:23:39.442+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.207 seconds
[2025-05-24T22:24:09.760+0000] {processor.py:161} INFO - Started process (PID=11151) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:24:09.761+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:24:09.762+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:09.762+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:24:10.148+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:10.148+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:24:10.160+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:10.160+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:24:10.164+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:10.164+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:24:10.165+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:10.165+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:24:10.167+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:10.167+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:24:10.169+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:10.169+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:24:10.170+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:10.170+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:24:10.174+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:10.174+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:24:10.174+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:10.174+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:24:10.176+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:10.176+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:24:10.177+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:10.177+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:24:10.179+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:10.178+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:24:10.180+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:10.180+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:24:10.181+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:10.181+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:24:10.181+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:10.181+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:24:10.195+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:10.194+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:24:10.207+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:10.207+0000] {etl.py:137} ERROR - Failed to create AWS session: 'Variable aws_access_key_id does not exist'
[2025-05-24T22:24:12.775+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:12.774+0000] {etl.py:181} ERROR - Unexpected error during S3 upload: Unable to locate credentials
[2025-05-24T22:24:12.775+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:12.775+0000] {etl.py:197} ERROR - ETL failed: Unable to locate credentials
[2025-05-24T22:24:12.779+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:24:12.791+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:12.791+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:24:12.812+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:12.812+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:24:12.828+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.076 seconds
[2025-05-24T22:24:43.727+0000] {processor.py:161} INFO - Started process (PID=11222) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:24:43.728+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:24:43.729+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:43.729+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:24:44.099+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:44.099+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:24:44.106+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:44.106+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:24:44.109+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:44.108+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:24:44.110+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:44.110+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:24:44.112+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:44.112+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:24:44.114+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:44.113+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:24:44.114+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:44.114+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:24:44.117+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:44.117+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:24:44.118+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:44.118+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:24:44.119+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:44.119+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:24:44.121+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:44.121+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:24:44.122+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:44.122+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:24:44.123+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:44.123+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:24:44.124+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:44.123+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:24:44.124+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:44.124+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:24:44.136+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:44.136+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:24:44.147+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:44.147+0000] {etl.py:137} ERROR - Failed to create AWS session: 'Variable aws_access_key_id does not exist'
[2025-05-24T22:24:45.267+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:45.266+0000] {etl.py:181} ERROR - Unexpected error during S3 upload: Unable to locate credentials
[2025-05-24T22:24:45.267+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:45.267+0000] {etl.py:197} ERROR - ETL failed: Unable to locate credentials
[2025-05-24T22:24:45.720+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:24:45.730+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:45.729+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:24:45.747+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:24:45.747+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:24:45.761+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.043 seconds
[2025-05-24T22:25:16.109+0000] {processor.py:161} INFO - Started process (PID=11290) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:25:16.111+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:25:16.112+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:16.111+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:25:16.408+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:16.408+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:25:16.415+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:16.415+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:25:16.418+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:16.418+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:25:16.419+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:16.419+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:25:16.421+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:16.421+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:25:16.422+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:16.422+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:25:16.423+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:16.423+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:25:16.426+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:16.426+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:25:16.427+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:16.427+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:25:16.428+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:16.428+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:25:16.429+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:16.429+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:25:16.431+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:16.431+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:25:16.431+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:16.431+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:25:16.432+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:16.432+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:25:16.433+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:16.433+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:25:16.444+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:16.444+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:25:16.454+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:16.454+0000] {etl.py:137} ERROR - Failed to create AWS session: 'Variable aws_access_key_id does not exist'
[2025-05-24T22:25:18.661+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:18.660+0000] {etl.py:181} ERROR - Unexpected error during S3 upload: Unable to locate credentials
[2025-05-24T22:25:18.661+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:18.661+0000] {etl.py:197} ERROR - ETL failed: Unable to locate credentials
[2025-05-24T22:25:18.681+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:25:19.278+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:19.277+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:25:19.291+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:19.291+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:25:19.307+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.211 seconds
[2025-05-24T22:25:49.516+0000] {processor.py:161} INFO - Started process (PID=11361) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:25:49.519+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:25:49.520+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:49.519+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:25:49.845+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:49.845+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:25:49.851+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:49.851+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:25:49.854+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:49.854+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:25:49.856+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:49.856+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:25:49.858+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:49.858+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:25:49.859+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:49.859+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:25:49.860+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:49.860+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:25:49.863+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:49.863+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:25:49.864+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:49.864+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:25:49.865+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:49.865+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:25:49.867+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:49.867+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:25:49.868+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:49.868+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:25:49.869+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:49.869+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:25:49.870+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:49.869+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:25:49.870+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:49.870+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:25:49.882+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:49.882+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:25:49.890+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:49.890+0000] {etl.py:137} ERROR - Failed to create AWS session: 'Variable aws_access_key_id does not exist'
[2025-05-24T22:25:52.063+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:52.063+0000] {etl.py:181} ERROR - Unexpected error during S3 upload: Unable to locate credentials
[2025-05-24T22:25:52.063+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:52.063+0000] {etl.py:197} ERROR - ETL failed: Unable to locate credentials
[2025-05-24T22:25:52.075+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:25:52.504+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:52.504+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:25:52.519+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:25:52.519+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:25:52.536+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.034 seconds
[2025-05-24T22:26:23.049+0000] {processor.py:161} INFO - Started process (PID=11434) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:26:23.053+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:26:23.055+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:23.054+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:26:24.136+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:24.134+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:26:24.187+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:24.187+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:26:24.195+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:24.195+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:26:24.197+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:24.197+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:26:24.200+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:24.200+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:26:24.202+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:24.202+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:26:24.203+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:24.203+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:26:24.209+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:24.208+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:26:24.209+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:24.209+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:26:24.211+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:24.211+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:26:24.213+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:24.212+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:26:24.214+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:24.214+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:26:24.216+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:24.216+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:26:24.218+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:24.217+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:26:24.218+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:24.218+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:26:24.233+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:24.233+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:26:24.249+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:24.249+0000] {etl.py:137} ERROR - Failed to create AWS session: 'Variable aws_access_key_id does not exist'
[2025-05-24T22:26:26.489+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:26.488+0000] {etl.py:181} ERROR - Unexpected error during S3 upload: Unable to locate credentials
[2025-05-24T22:26:26.490+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:26.489+0000] {etl.py:197} ERROR - ETL failed: Unable to locate credentials
[2025-05-24T22:26:26.518+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:26:27.388+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:26.555+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:26:27.407+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:27.407+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:26:27.418+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 4.402 seconds
[2025-05-24T22:26:57.889+0000] {processor.py:161} INFO - Started process (PID=11516) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:26:57.891+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:26:57.892+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:57.892+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:26:58.340+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.339+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:26:58.353+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.352+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:26:58.361+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.361+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:26:58.365+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.364+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:26:58.369+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.369+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:26:58.374+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.373+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:26:58.380+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.379+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:26:58.395+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.395+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:26:58.396+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.396+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:26:58.398+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.398+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:26:58.401+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.401+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:26:58.404+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.404+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:26:58.406+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.406+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:26:58.408+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.407+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:26:58.408+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.408+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:26:58.426+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.426+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:26:58.462+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.462+0000] {etl.py:137} ERROR - Failed to create AWS session: 'Variable aws_access_key_id does not exist'
[2025-05-24T22:26:58.922+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.922+0000] {etl.py:181} ERROR - Unexpected error during S3 upload: Unable to locate credentials
[2025-05-24T22:26:58.923+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.922+0000] {etl.py:197} ERROR - ETL failed: Unable to locate credentials
[2025-05-24T22:26:58.925+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:26:58.932+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.932+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:26:58.944+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:26:58.944+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:26:58.959+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.084 seconds
[2025-05-24T22:27:29.960+0000] {processor.py:161} INFO - Started process (PID=11580) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:27:29.962+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:27:29.964+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:29.963+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:27:30.435+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:30.434+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:27:30.442+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:30.442+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:27:30.446+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:30.445+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:27:30.448+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:30.447+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:27:30.450+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:30.450+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:27:30.452+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:30.452+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:27:30.453+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:30.453+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:27:30.457+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:30.457+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:27:30.457+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:30.457+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:27:30.459+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:30.459+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:27:30.461+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:30.461+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:27:30.462+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:30.462+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:27:30.464+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:30.464+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:27:30.465+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:30.464+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:27:30.465+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:30.465+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:27:30.497+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:30.497+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:27:30.525+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:30.524+0000] {etl.py:137} ERROR - Failed to create AWS session: 'Variable aws_access_key_id does not exist'
[2025-05-24T22:27:32.642+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:32.641+0000] {etl.py:181} ERROR - Unexpected error during S3 upload: Unable to locate credentials
[2025-05-24T22:27:32.642+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:32.642+0000] {etl.py:197} ERROR - ETL failed: Unable to locate credentials
[2025-05-24T22:27:32.655+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:27:33.066+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:33.066+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:27:33.083+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:27:33.082+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:27:33.095+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.151 seconds
[2025-05-24T22:28:03.662+0000] {processor.py:161} INFO - Started process (PID=11652) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:28:03.665+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:28:03.667+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:03.666+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:28:04.438+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:04.438+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:28:04.455+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:04.455+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:28:04.459+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:04.459+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:28:04.461+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:04.461+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:28:04.463+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:04.463+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:28:04.465+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:04.465+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:28:04.466+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:04.466+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:28:04.474+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:04.474+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:28:04.474+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:04.474+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:28:04.477+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:04.476+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:28:04.481+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:04.480+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:28:04.485+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:04.484+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:28:04.486+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:04.486+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:28:04.487+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:04.487+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:28:04.488+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:04.488+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:28:04.515+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:04.514+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:28:05.747+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:05.745+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:28:05.758+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:28:06.128+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:06.127+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:28:06.140+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:06.140+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:28:06.148+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.517 seconds
[2025-05-24T22:28:36.693+0000] {processor.py:161} INFO - Started process (PID=11718) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:28:36.695+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:28:36.696+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:36.695+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:28:37.072+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:37.072+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:28:37.084+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:37.083+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:28:37.087+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:37.086+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:28:37.088+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:37.088+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:28:37.091+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:37.091+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:28:37.093+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:37.093+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:28:37.094+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:37.094+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:28:37.098+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:37.098+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:28:37.098+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:37.098+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:28:37.100+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:37.100+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:28:37.101+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:37.101+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:28:37.103+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:37.102+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:28:37.105+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:37.105+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:28:37.106+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:37.106+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:28:37.106+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:37.106+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:28:37.121+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:37.121+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:28:38.533+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:38.533+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:28:38.537+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:28:38.545+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:38.545+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:28:38.555+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:28:38.555+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:28:38.563+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.878 seconds
[2025-05-24T22:29:08.771+0000] {processor.py:161} INFO - Started process (PID=11785) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:29:08.773+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:29:08.774+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:08.773+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:29:09.603+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:09.602+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:29:09.618+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:09.617+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:29:09.622+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:09.621+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:29:09.625+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:09.624+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:29:09.629+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:09.629+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:29:09.633+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:09.633+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:29:09.635+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:09.635+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:29:09.649+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:09.649+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:29:09.652+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:09.651+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:29:09.660+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:09.659+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:29:09.667+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:09.667+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:29:09.674+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:09.674+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:29:09.679+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:09.679+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:29:09.681+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:09.681+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:29:09.682+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:09.682+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:29:09.712+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:09.711+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:29:11.246+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:11.244+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:29:11.268+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:29:11.280+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:11.279+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:29:11.299+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:11.298+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:29:11.310+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.548 seconds
[2025-05-24T22:29:41.634+0000] {processor.py:161} INFO - Started process (PID=11846) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:29:41.637+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:29:41.640+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:41.639+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:29:42.618+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:42.617+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:29:42.657+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:42.656+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:29:42.666+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:42.665+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:29:42.669+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:42.669+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:29:42.677+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:42.677+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:29:42.687+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:42.687+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:29:42.690+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:42.689+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:29:42.708+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:42.708+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:29:42.709+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:42.708+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:29:42.712+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:42.711+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:29:42.715+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:42.715+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:29:42.719+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:42.719+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:29:42.721+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:42.721+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:29:42.723+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:42.722+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:29:42.726+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:42.725+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:29:42.758+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:42.758+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:29:45.201+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:45.201+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:29:45.213+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:29:45.226+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:45.225+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:29:45.259+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:29:45.258+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:29:45.277+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.659 seconds
[2025-05-24T22:30:15.318+0000] {processor.py:161} INFO - Started process (PID=11910) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:30:15.319+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:30:15.320+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:15.320+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:30:15.593+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:15.592+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:30:15.602+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:15.601+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:30:15.606+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:15.606+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:30:15.611+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:15.611+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:30:15.613+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:15.612+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:30:15.614+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:15.614+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:30:15.614+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:15.614+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:30:15.617+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:15.617+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:30:15.618+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:15.618+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:30:15.619+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:15.619+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:30:15.620+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:15.620+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:30:15.621+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:15.621+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:30:15.622+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:15.622+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:30:15.623+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:15.623+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:30:15.623+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:15.623+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:30:15.634+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:15.634+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:30:17.291+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:17.291+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:30:17.295+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:30:17.305+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:17.305+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:30:17.321+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:17.320+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:30:17.333+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.021 seconds
[2025-05-24T22:30:48.753+0000] {processor.py:161} INFO - Started process (PID=11998) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:30:48.754+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:30:48.755+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:48.755+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:30:50.626+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:50.625+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:30:50.702+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:50.701+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:30:50.726+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:50.725+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:30:50.731+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:50.731+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:30:50.737+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:50.737+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:30:50.748+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:50.748+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:30:50.753+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:50.753+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:30:50.764+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:50.764+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:30:50.765+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:50.765+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:30:50.768+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:50.768+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:30:50.779+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:50.778+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:30:50.788+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:50.787+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:30:50.792+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:50.792+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:30:50.796+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:50.796+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:30:50.798+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:50.798+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:30:50.910+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:30:50.909+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:31:01.150+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:01.097+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:31:01.248+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:31:01.359+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:01.358+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:31:01.441+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:01.441+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:31:01.598+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 12.852 seconds
[2025-05-24T22:31:38.253+0000] {processor.py:161} INFO - Started process (PID=12090) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:31:38.273+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:31:38.283+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:38.277+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:31:41.102+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:41.100+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:31:41.195+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:41.195+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:31:41.231+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:41.231+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:31:41.242+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:41.242+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:31:41.248+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:41.248+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:31:41.268+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:41.267+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:31:41.283+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:41.282+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:31:41.329+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:41.328+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:31:41.331+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:41.330+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:31:41.361+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:41.360+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:31:41.442+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:41.441+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:31:41.492+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:41.491+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:31:41.518+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:41.509+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:31:41.551+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:41.551+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:31:41.562+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:41.561+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:31:41.721+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:41.720+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:31:46.290+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:46.290+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:31:46.299+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:31:46.318+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:46.317+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:31:46.358+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:31:46.358+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:31:46.512+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 8.371 seconds
[2025-05-24T22:32:16.615+0000] {processor.py:161} INFO - Started process (PID=12152) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:32:16.621+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:32:16.624+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:16.623+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:32:17.238+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:17.237+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:32:17.273+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:17.271+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:32:17.281+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:17.280+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:32:17.286+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:17.285+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:32:17.292+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:17.292+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:32:17.295+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:17.295+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:32:17.296+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:17.296+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:32:17.302+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:17.302+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:32:17.308+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:17.307+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:32:17.315+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:17.313+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:32:17.318+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:17.317+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:32:17.320+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:17.320+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:32:17.324+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:17.324+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:32:17.327+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:17.327+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:32:17.328+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:17.328+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:32:17.362+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:17.362+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:32:19.741+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:19.741+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:32:19.750+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:32:19.792+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:19.791+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:32:19.820+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:19.820+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:32:19.854+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.263 seconds
[2025-05-24T22:32:50.784+0000] {processor.py:161} INFO - Started process (PID=12214) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:32:50.785+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:32:50.786+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:50.786+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:32:51.150+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:51.149+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:32:51.158+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:51.158+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:32:51.161+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:51.161+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:32:51.162+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:51.162+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:32:51.165+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:51.164+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:32:51.166+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:51.166+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:32:51.166+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:51.166+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:32:51.170+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:51.170+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:32:51.170+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:51.170+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:32:51.172+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:51.171+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:32:51.173+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:51.173+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:32:51.175+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:51.175+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:32:51.176+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:51.176+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:32:51.178+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:51.178+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:32:51.178+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:51.178+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:32:51.192+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:51.191+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:32:52.672+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:52.672+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:32:52.680+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:32:52.699+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:52.699+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:32:52.719+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:32:52.719+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:32:52.736+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.965 seconds
[2025-05-24T22:33:23.367+0000] {processor.py:161} INFO - Started process (PID=12283) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:33:23.381+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:33:23.407+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:33:23.388+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:34:47.632+0000] {processor.py:161} INFO - Started process (PID=12361) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:34:47.636+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:34:47.640+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:34:47.637+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:35:18.174+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:18.140+0000] {timeout.py:68} ERROR - Process timed out, PID: 12361
[2025-05-24T22:35:48.713+0000] {processor.py:161} INFO - Started process (PID=12421) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:35:48.717+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:35:48.720+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:48.720+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:35:49.449+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:49.448+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:35:49.471+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:49.470+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:35:49.476+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:49.476+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:35:49.480+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:49.480+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:35:49.482+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:49.482+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:35:49.484+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:49.484+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:35:49.486+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:49.486+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:35:49.492+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:49.491+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:35:49.492+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:49.492+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:35:49.495+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:49.494+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:35:49.496+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:49.496+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:35:49.499+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:49.498+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:35:49.501+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:49.501+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:35:49.502+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:49.502+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:35:49.504+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:49.504+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:35:49.535+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:49.534+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:35:51.623+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:51.622+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:35:51.628+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:35:51.727+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:51.726+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:35:51.740+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:35:51.740+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:35:51.756+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.054 seconds
[2025-05-24T22:36:21.918+0000] {processor.py:161} INFO - Started process (PID=12489) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:36:21.925+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:36:21.927+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:21.926+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:36:22.473+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:22.472+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:36:22.484+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:22.484+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:36:22.487+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:22.487+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:36:22.867+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:22.866+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:36:22.870+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:22.870+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:36:22.875+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:22.875+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:36:22.877+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:22.877+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:36:22.891+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:22.890+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:36:22.893+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:22.893+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:36:22.896+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:22.895+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:36:22.901+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:22.900+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:36:22.904+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:22.904+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:36:22.911+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:22.910+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:36:22.916+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:22.916+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:36:22.920+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:22.920+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:36:22.960+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:22.959+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:36:24.828+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:24.825+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:36:24.841+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:36:24.871+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:24.871+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:36:24.896+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:24.896+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:36:24.914+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.012 seconds
[2025-05-24T22:36:55.273+0000] {processor.py:161} INFO - Started process (PID=12551) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:36:55.274+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:36:55.275+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:55.274+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:36:56.101+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:56.096+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:36:56.137+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:56.136+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:36:56.142+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:56.142+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:36:56.145+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:56.145+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:36:56.149+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:56.149+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:36:56.152+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:56.151+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:36:56.154+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:56.154+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:36:57.475+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:57.454+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:36:57.570+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:57.556+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:36:57.617+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:57.614+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:36:57.635+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:57.634+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:36:57.638+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:57.638+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:36:57.641+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:57.641+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:36:57.708+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:57.663+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:36:57.851+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:57.828+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:36:58.181+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:36:58.170+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:37:18.580+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:37:18.527+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:37:18.916+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:37:21.787+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:37:21.698+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:37:22.117+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:37:22.068+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:37:22.848+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 27.671 seconds
[2025-05-24T22:37:53.203+0000] {processor.py:161} INFO - Started process (PID=12619) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:37:53.205+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:37:53.217+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:37:53.211+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:37:58.863+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:37:58.861+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:37:58.896+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:37:58.895+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:37:58.902+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:37:58.902+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:37:58.907+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:37:58.906+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:37:58.909+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:37:58.909+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:37:58.916+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:37:58.916+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:37:58.917+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:37:58.917+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:37:58.923+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:37:58.923+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:37:58.924+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:37:58.924+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:37:58.926+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:37:58.926+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:37:58.928+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:37:58.928+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:37:58.930+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:37:58.929+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:37:58.934+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:37:58.934+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:37:58.936+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:37:58.936+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:37:58.938+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:37:58.937+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:37:58.973+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:37:58.972+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:38:01.858+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:01.855+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:38:01.882+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:38:01.997+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:01.996+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:38:02.057+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:02.057+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:38:02.097+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 8.909 seconds
[2025-05-24T22:38:32.736+0000] {processor.py:161} INFO - Started process (PID=12696) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:38:32.740+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:38:32.742+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:32.741+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:38:33.199+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:33.198+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:38:33.216+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:33.216+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:38:33.220+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:33.219+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:38:33.225+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:33.225+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:38:33.405+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:33.404+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:38:33.407+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:33.406+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:38:33.408+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:33.407+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:38:33.416+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:33.416+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:38:33.417+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:33.417+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:38:33.419+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:33.419+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:38:33.426+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:33.426+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:38:33.429+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:33.429+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:38:33.432+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:33.432+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:38:33.434+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:33.434+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:38:33.435+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:33.435+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:38:33.453+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:33.453+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:38:34.773+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:34.772+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:38:34.786+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:38:34.809+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:34.808+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:38:34.839+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:38:34.839+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:38:34.860+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.131 seconds
[2025-05-24T22:39:05.033+0000] {processor.py:161} INFO - Started process (PID=12772) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:39:05.034+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:39:05.034+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:05.034+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:39:05.690+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:05.690+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:39:05.702+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:05.701+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:39:05.706+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:05.706+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:39:05.709+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:05.708+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:39:05.711+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:05.711+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:39:05.713+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:05.712+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:39:05.713+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:05.713+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:39:05.718+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:05.718+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:39:05.718+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:05.718+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:39:05.720+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:05.720+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:39:05.722+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:05.722+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:39:05.723+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:05.723+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:39:05.724+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:05.724+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:39:05.725+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:05.725+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:39:05.726+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:05.726+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:39:05.742+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:05.742+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:39:07.227+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:07.226+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:39:07.237+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:39:07.249+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:07.249+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:39:07.275+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:07.275+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:39:07.290+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.260 seconds
[2025-05-24T22:39:37.365+0000] {processor.py:161} INFO - Started process (PID=12834) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:39:37.367+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:39:37.368+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:37.368+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:39:37.960+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:37.960+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:39:37.966+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:37.966+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:39:37.968+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:37.968+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:39:37.970+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:37.969+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:39:37.975+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:37.975+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:39:37.978+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:37.977+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:39:37.979+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:37.979+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:39:37.985+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:37.985+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:39:37.986+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:37.986+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:39:37.987+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:37.987+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:39:37.988+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:37.988+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:39:37.989+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:37.989+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:39:37.990+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:37.990+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:39:37.991+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:37.991+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:39:37.991+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:37.991+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:39:38.010+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:38.009+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:39:39.364+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:39.363+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:39:39.367+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:39:39.376+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:39.376+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:39:39.387+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:39:39.387+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:39:39.400+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.043 seconds
[2025-05-24T22:40:09.823+0000] {processor.py:161} INFO - Started process (PID=12896) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:40:09.824+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:40:09.825+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:09.825+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:40:10.381+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:10.377+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:40:10.392+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:10.392+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:40:10.395+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:10.395+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:40:10.397+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:10.396+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:40:10.400+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:10.399+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:40:10.402+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:10.401+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:40:10.405+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:10.405+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:40:10.415+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:10.415+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:40:10.416+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:10.416+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:40:10.419+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:10.418+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:40:10.421+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:10.421+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:40:10.423+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:10.423+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:40:10.424+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:10.424+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:40:10.426+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:10.426+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:40:10.427+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:10.427+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:40:10.443+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:10.442+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:40:11.720+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:11.720+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:40:11.725+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:40:11.736+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:11.736+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:40:11.748+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:11.748+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:40:11.759+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.943 seconds
[2025-05-24T22:40:42.407+0000] {processor.py:161} INFO - Started process (PID=12957) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:40:42.412+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:40:42.419+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:42.416+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:40:43.492+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:43.487+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:40:43.510+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:43.510+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:40:43.513+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:43.513+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:40:43.515+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:43.514+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:40:43.516+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:43.516+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:40:43.517+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:43.517+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:40:43.520+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:43.520+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:40:43.524+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:43.524+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:40:43.524+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:43.524+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:40:43.526+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:43.526+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:40:43.529+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:43.529+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:40:43.530+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:43.530+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:40:43.532+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:43.532+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:40:43.533+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:43.533+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:40:43.534+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:43.534+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:40:43.548+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:43.548+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:40:45.060+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:45.059+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:40:45.070+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:40:45.082+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:45.082+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:40:45.096+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:40:45.096+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:40:45.110+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.771 seconds
[2025-05-24T22:41:15.526+0000] {processor.py:161} INFO - Started process (PID=13022) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:41:15.527+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:41:15.528+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:15.527+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:41:16.634+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:16.627+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:41:16.679+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:16.678+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:41:16.688+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:16.688+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:41:16.695+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:16.694+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:41:16.701+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:16.701+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:41:16.706+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:16.706+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:41:16.714+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:16.713+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:41:16.729+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:16.728+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:41:16.730+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:16.730+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:41:16.734+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:16.734+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:41:16.741+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:16.741+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:41:16.745+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:16.745+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:41:16.748+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:16.748+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:41:16.752+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:16.752+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:41:16.754+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:16.754+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:41:16.784+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:16.783+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:41:18.111+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:18.109+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:41:18.119+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:41:18.142+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:18.141+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:41:18.179+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:18.179+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:41:18.196+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.674 seconds
[2025-05-24T22:41:49.161+0000] {processor.py:161} INFO - Started process (PID=13084) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:41:49.162+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:41:49.164+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:49.163+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:41:49.791+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:49.786+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:41:49.807+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:49.807+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:41:49.811+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:49.811+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:41:49.813+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:49.812+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:41:49.815+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:49.814+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:41:49.816+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:49.816+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:41:49.818+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:49.818+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:41:49.823+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:49.823+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:41:49.823+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:49.823+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:41:49.825+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:49.825+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:41:49.826+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:49.826+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:41:49.827+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:49.827+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:41:49.828+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:49.828+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:41:49.830+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:49.830+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:41:49.830+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:49.830+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:41:49.846+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:49.846+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:41:51.264+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:51.264+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:41:51.270+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:41:51.285+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:51.284+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:41:51.298+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:41:51.298+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:41:51.310+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.166 seconds
[2025-05-24T22:42:21.455+0000] {processor.py:161} INFO - Started process (PID=13153) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:42:21.456+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:42:21.457+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:21.457+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:42:22.089+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:22.085+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:42:22.108+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:22.106+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:42:22.111+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:22.111+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:42:22.113+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:22.113+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:42:22.116+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:22.116+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:42:22.117+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:22.117+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:42:22.120+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:22.120+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:42:22.127+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:22.127+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:42:22.129+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:22.128+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:42:22.132+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:22.132+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:42:22.134+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:22.133+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:42:22.135+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:22.135+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:42:22.136+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:22.136+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:42:22.137+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:22.137+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:42:22.138+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:22.138+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:42:22.151+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:22.151+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:42:23.543+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:23.542+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:42:23.548+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:42:23.561+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:23.559+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:42:23.582+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:23.582+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:42:23.594+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.146 seconds
[2025-05-24T22:42:53.731+0000] {processor.py:161} INFO - Started process (PID=13218) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:42:53.733+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:42:53.734+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:53.733+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:42:54.433+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:54.431+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:42:54.445+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:54.445+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:42:54.448+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:54.448+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:42:54.450+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:54.450+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:42:54.452+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:54.452+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:42:54.453+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:54.453+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:42:54.455+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:54.455+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:42:54.460+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:54.460+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:42:54.460+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:54.460+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:42:54.462+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:54.462+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:42:54.464+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:54.463+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:42:54.465+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:54.465+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:42:54.466+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:54.466+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:42:54.467+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:54.467+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:42:54.469+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:54.469+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:42:54.485+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:54.485+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:42:56.055+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:56.053+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:42:56.061+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:42:56.079+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:56.079+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:42:56.095+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:42:56.095+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:42:56.107+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.387 seconds
[2025-05-24T22:43:27.067+0000] {processor.py:161} INFO - Started process (PID=13279) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:43:27.077+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:43:27.083+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:43:27.079+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:43:28.716+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:43:28.713+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:43:28.745+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:43:28.745+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:43:28.750+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:43:28.750+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:43:28.752+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:43:28.752+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:43:28.756+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:43:28.756+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:43:28.758+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:43:28.758+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:43:28.760+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:43:28.760+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:43:28.767+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:43:28.767+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:43:28.768+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:43:28.767+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:43:28.772+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:43:28.771+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:43:28.774+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:43:28.774+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:43:28.776+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:43:28.776+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:43:28.779+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:43:28.779+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:43:28.781+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:43:28.781+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:43:28.783+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:43:28.783+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:43:28.817+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:43:28.817+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:43:30.463+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:43:30.461+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:43:30.472+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:43:30.504+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:43:30.503+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:43:30.528+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:43:30.527+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:43:30.540+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.486 seconds
[2025-05-24T22:44:00.870+0000] {processor.py:161} INFO - Started process (PID=13347) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:44:00.872+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:44:00.873+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:00.873+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:44:02.242+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:02.238+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:44:02.262+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:02.262+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:44:02.267+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:02.267+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:44:02.273+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:02.271+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:44:02.276+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:02.276+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:44:02.283+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:02.282+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:44:02.285+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:02.285+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:44:02.298+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:02.297+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:44:02.300+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:02.300+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:44:02.303+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:02.303+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:44:02.308+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:02.308+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:44:02.311+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:02.311+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:44:02.315+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:02.315+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:44:02.318+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:02.318+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:44:02.320+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:02.320+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:44:02.339+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:02.338+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:44:03.962+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:03.933+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:44:04.063+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:44:04.148+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:04.143+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:44:04.263+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:04.263+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:44:04.328+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.472 seconds
[2025-05-24T22:44:35.086+0000] {processor.py:161} INFO - Started process (PID=13409) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:44:35.088+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:44:35.090+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:35.089+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:44:35.974+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:35.972+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:44:35.990+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:35.989+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:44:35.993+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:35.993+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:44:35.996+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:35.995+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:44:35.998+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:35.998+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:44:36.000+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:36.000+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:44:36.001+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:36.001+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:44:36.006+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:36.006+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:44:36.007+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:36.007+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:44:36.010+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:36.010+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:44:36.013+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:36.013+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:44:36.016+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:36.016+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:44:36.019+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:36.018+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:44:36.022+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:36.021+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:44:36.024+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:36.023+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:44:36.057+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:36.056+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:44:37.274+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:37.273+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:44:37.282+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:44:37.297+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:37.297+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:44:37.310+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:44:37.310+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:44:37.324+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.260 seconds
[2025-05-24T22:45:07.873+0000] {processor.py:161} INFO - Started process (PID=13471) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:45:07.880+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:45:07.888+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:07.882+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:45:08.898+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:08.897+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:45:08.913+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:08.913+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:45:08.917+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:08.916+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:45:08.920+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:08.919+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:45:08.922+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:08.922+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:45:08.926+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:08.926+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:45:08.928+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:08.928+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:45:08.935+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:08.934+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:45:08.935+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:08.935+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:45:08.938+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:08.938+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:45:08.941+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:08.941+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:45:08.942+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:08.942+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:45:08.945+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:08.944+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:45:08.946+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:08.946+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:45:08.946+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:08.946+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:45:08.974+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:08.974+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:45:10.559+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:10.557+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:45:10.565+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:45:10.585+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:10.584+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:45:10.598+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:10.598+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:45:10.617+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.756 seconds
[2025-05-24T22:45:40.709+0000] {processor.py:161} INFO - Started process (PID=13532) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:45:40.710+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:45:40.711+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:40.711+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:45:41.626+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:41.624+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:45:41.636+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:41.636+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:45:41.640+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:41.640+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:45:41.642+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:41.642+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:45:41.644+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:41.644+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:45:41.646+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:41.646+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:45:41.647+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:41.647+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:45:41.652+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:41.651+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:45:41.653+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:41.653+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:45:41.656+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:41.655+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:45:41.658+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:41.658+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:45:41.659+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:41.659+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:45:41.660+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:41.660+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:45:41.661+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:41.661+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:45:41.663+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:41.663+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:45:41.678+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:41.678+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:45:43.256+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:43.251+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:45:43.264+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:45:43.293+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:43.292+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:45:43.311+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:45:43.311+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:45:43.324+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.632 seconds
[2025-05-24T22:46:14.010+0000] {processor.py:161} INFO - Started process (PID=13594) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:46:14.013+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:46:14.016+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:14.013+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:46:15.050+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:15.049+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:46:15.061+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:15.061+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:46:15.064+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:15.064+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:46:15.066+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:15.066+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:46:15.068+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:15.068+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:46:15.069+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:15.069+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:46:15.071+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:15.071+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:46:15.075+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:15.075+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:46:15.075+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:15.075+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:46:15.077+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:15.077+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:46:15.079+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:15.079+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:46:15.080+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:15.080+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:46:15.081+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:15.081+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:46:15.083+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:15.083+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:46:15.084+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:15.084+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:46:15.105+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:15.104+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:46:16.893+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:16.889+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:46:16.899+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:46:16.930+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:16.929+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:46:16.953+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:16.953+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:46:16.970+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.977 seconds
[2025-05-24T22:46:47.326+0000] {processor.py:161} INFO - Started process (PID=13656) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:46:47.330+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:46:47.332+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:47.331+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:46:48.366+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:48.364+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:46:48.381+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:48.380+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:46:48.386+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:48.386+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:46:48.388+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:48.388+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:46:48.390+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:48.390+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:46:48.395+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:48.395+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:46:48.399+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:48.399+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:46:48.420+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:48.420+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:46:48.421+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:48.421+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:46:48.424+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:48.424+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:46:48.426+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:48.426+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:46:48.427+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:48.427+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:46:48.428+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:48.428+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:46:48.429+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:48.429+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:46:48.430+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:48.430+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:46:48.445+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:48.445+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:46:49.757+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:49.755+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:46:49.762+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:46:49.775+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:49.774+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:46:49.789+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:46:49.789+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:46:49.800+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.497 seconds
[2025-05-24T22:47:19.929+0000] {processor.py:161} INFO - Started process (PID=13718) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:47:19.932+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:47:19.934+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:19.933+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:47:20.986+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:20.983+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:47:21.003+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:21.003+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:47:21.008+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:21.008+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:47:21.011+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:21.011+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:47:21.015+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:21.014+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:47:21.043+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:21.043+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:47:21.054+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:21.053+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:47:21.072+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:21.071+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:47:21.078+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:21.078+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:47:21.082+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:21.082+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:47:21.088+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:21.087+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:47:21.091+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:21.091+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:47:21.094+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:21.094+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:47:21.097+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:21.097+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:47:21.099+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:21.099+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:47:21.117+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:21.117+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:47:22.433+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:22.431+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:47:22.437+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:47:22.452+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:22.451+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:47:22.468+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:22.468+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:47:22.482+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.566 seconds
[2025-05-24T22:47:53.490+0000] {processor.py:161} INFO - Started process (PID=13780) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:47:53.494+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:47:53.502+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:53.496+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:47:54.562+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:54.560+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:47:54.572+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:54.572+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:47:54.574+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:54.574+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:47:54.577+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:54.576+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:47:54.579+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:54.579+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:47:54.580+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:54.580+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:47:54.582+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:54.581+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:47:54.586+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:54.586+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:47:54.586+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:54.586+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:47:54.587+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:54.587+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:47:54.589+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:54.589+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:47:54.590+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:54.590+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:47:54.591+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:54.591+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:47:54.592+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:54.592+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:47:54.593+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:54.593+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:47:54.608+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:54.608+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:47:56.208+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:56.207+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:47:56.213+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:47:56.230+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:56.230+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:47:56.244+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:47:56.244+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:47:56.257+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.797 seconds
[2025-05-24T22:48:27.049+0000] {processor.py:161} INFO - Started process (PID=13842) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:48:27.052+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:48:27.055+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:48:27.053+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:48:28.025+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:48:28.024+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:48:28.035+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:48:28.035+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:48:28.037+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:48:28.037+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:48:28.040+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:48:28.040+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:48:28.042+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:48:28.042+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:48:28.043+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:48:28.043+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:48:28.044+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:48:28.044+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:48:28.048+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:48:28.048+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:48:28.049+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:48:28.049+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:48:28.050+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:48:28.050+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:48:28.051+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:48:28.051+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:48:28.052+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:48:28.052+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:48:28.053+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:48:28.053+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:48:28.055+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:48:28.055+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:48:28.057+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:48:28.057+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:48:28.078+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:48:28.078+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:48:29.617+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:48:29.614+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:48:29.622+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:48:29.637+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:48:29.636+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:48:29.653+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:48:29.652+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:48:29.670+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.641 seconds
[2025-05-24T22:49:00.000+0000] {processor.py:161} INFO - Started process (PID=13904) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:49:00.004+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:49:00.007+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:00.005+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:49:01.012+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:01.010+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:49:01.024+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:01.024+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:49:01.027+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:01.027+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:49:01.029+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:01.029+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:49:01.031+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:01.031+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:49:01.033+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:01.033+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:49:01.034+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:01.034+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:49:01.040+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:01.040+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:49:01.040+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:01.040+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:49:01.042+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:01.042+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:49:01.044+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:01.044+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:49:01.045+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:01.044+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:49:01.046+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:01.046+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:49:01.047+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:01.047+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:49:01.049+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:01.049+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:49:01.063+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:01.063+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:49:02.882+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:02.877+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:49:02.893+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:49:02.917+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:02.917+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:49:02.946+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:02.946+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:49:02.965+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.980 seconds
[2025-05-24T22:49:33.316+0000] {processor.py:161} INFO - Started process (PID=13972) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:49:33.319+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:49:33.321+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:33.320+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:49:34.407+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:34.405+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:49:34.416+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:34.416+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:49:34.420+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:34.419+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:49:34.423+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:34.423+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:49:34.425+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:34.425+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:49:34.427+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:34.427+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:49:34.429+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:34.429+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:49:34.436+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:34.436+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:49:34.437+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:34.436+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:49:34.439+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:34.439+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:49:34.441+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:34.441+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:49:34.443+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:34.443+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:49:34.444+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:34.444+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:49:34.445+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:34.445+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:49:34.447+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:34.447+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:49:34.469+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:34.469+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:49:35.818+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:35.815+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:49:35.827+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:49:35.849+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:35.849+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:49:35.887+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:49:35.886+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:49:35.925+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.628 seconds
[2025-05-24T22:50:06.041+0000] {processor.py:161} INFO - Started process (PID=14034) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:50:06.045+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:50:06.046+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:06.045+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:50:06.996+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:06.994+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:50:07.010+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:07.009+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:50:07.012+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:07.012+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:50:07.015+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:07.015+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:50:07.017+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:07.017+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:50:07.019+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:07.019+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:50:07.020+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:07.020+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:50:07.029+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:07.029+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:50:07.030+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:07.029+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:50:07.031+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:07.031+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:50:07.033+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:07.033+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:50:07.035+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:07.035+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:50:07.036+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:07.036+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:50:07.038+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:07.038+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:50:07.040+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:07.040+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:50:07.056+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:07.056+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:50:08.865+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:08.863+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:50:08.871+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:50:08.898+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:08.898+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:50:08.926+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:08.925+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:50:08.944+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.919 seconds
[2025-05-24T22:50:39.811+0000] {processor.py:161} INFO - Started process (PID=14097) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:50:39.813+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:50:39.813+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:39.813+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:50:40.406+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:40.404+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:50:40.416+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:40.416+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:50:40.418+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:40.418+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:50:40.420+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:40.420+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:50:40.421+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:40.421+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:50:40.422+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:40.422+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:50:40.423+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:40.423+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:50:40.427+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:40.427+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:50:40.427+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:40.427+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:50:40.429+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:40.429+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:50:40.430+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:40.430+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:50:40.432+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:40.432+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:50:40.433+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:40.433+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:50:40.434+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:40.434+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:50:40.435+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:40.435+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:50:40.450+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:40.450+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:50:41.927+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:41.927+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:50:41.931+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:50:41.938+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:41.938+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:50:41.952+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:50:41.952+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:50:41.962+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.156 seconds
[2025-05-24T22:51:12.383+0000] {processor.py:161} INFO - Started process (PID=14162) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:51:12.385+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:51:12.385+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:12.385+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:51:13.053+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:13.050+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:51:13.068+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:13.068+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:51:13.072+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:13.072+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:51:13.075+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:13.074+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:51:13.078+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:13.078+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:51:13.081+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:13.081+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:51:13.088+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:13.088+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:51:13.099+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:13.098+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:51:13.099+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:13.099+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:51:13.103+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:13.102+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:51:13.109+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:13.109+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:51:13.114+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:13.113+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:51:13.117+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:13.116+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:51:13.121+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:13.120+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:51:13.122+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:13.122+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:51:13.180+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:13.177+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:51:14.711+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:14.710+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:51:14.720+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:51:14.737+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:14.736+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:51:14.750+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:14.750+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:51:14.769+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.394 seconds
[2025-05-24T22:51:45.199+0000] {processor.py:161} INFO - Started process (PID=14229) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:51:45.200+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:51:45.201+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:45.200+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:51:45.789+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:45.788+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:51:45.802+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:45.801+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:51:45.804+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:45.804+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:51:45.806+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:45.806+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:51:45.809+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:45.808+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:51:45.810+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:45.810+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:51:45.811+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:45.811+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:51:45.817+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:45.816+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:51:45.818+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:45.817+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:51:45.821+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:45.821+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:51:45.823+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:45.823+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:51:45.825+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:45.825+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:51:45.827+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:45.827+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:51:45.829+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:45.829+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:51:45.831+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:45.831+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:51:45.847+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:45.847+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:51:47.165+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:47.164+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:51:47.169+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:51:47.181+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:47.180+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:51:47.193+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:51:47.192+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:51:47.201+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.007 seconds
[2025-05-24T22:52:17.541+0000] {processor.py:161} INFO - Started process (PID=14290) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:52:17.543+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:52:17.544+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:17.543+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:52:18.486+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:18.483+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:52:18.499+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:18.498+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:52:18.502+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:18.502+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:52:18.505+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:18.504+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:52:18.507+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:18.507+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:52:18.509+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:18.509+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:52:18.510+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:18.510+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:52:18.515+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:18.515+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:52:18.515+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:18.515+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:52:18.517+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:18.517+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:52:18.520+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:18.520+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:52:18.524+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:18.524+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:52:18.537+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:18.536+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:52:18.549+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:18.549+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:52:18.552+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:18.552+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:52:18.597+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:18.596+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:52:20.431+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:20.428+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:52:20.438+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:52:20.458+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:20.458+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:52:20.480+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:20.480+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:52:20.499+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.976 seconds
[2025-05-24T22:52:51.180+0000] {processor.py:161} INFO - Started process (PID=14351) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:52:51.183+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:52:51.185+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:51.185+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:52:52.293+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:52.290+0000] {etl.py:186} INFO - Starting ETL pipeline
[2025-05-24T22:52:52.307+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:52.307+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:52:52.312+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:52.312+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:52:52.315+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:52.314+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:52:52.317+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:52.317+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:52:52.319+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:52.319+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:52:52.323+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:52.323+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:52:52.329+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:52.329+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:52:52.329+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:52.329+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:52:52.332+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:52.332+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:52:52.334+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:52.334+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:52:52.336+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:52.335+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:52:52.337+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:52.337+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:52:52.338+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:52.338+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:52:52.340+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:52.340+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:52:52.357+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:52.357+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:52:53.997+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:53.995+0000] {etl.py:195} INFO - ETL completed successfully
[2025-05-24T22:52:54.004+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:52:54.022+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:54.022+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:52:54.048+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:52:54.047+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:52:54.066+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.901 seconds
[2025-05-24T22:53:24.390+0000] {processor.py:161} INFO - Started process (PID=14414) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:53:24.392+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:53:24.394+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:24.393+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:53:25.412+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:25.412+0000] {etl.py:172} INFO - Starting ETL pipeline
[2025-05-24T22:53:25.424+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:25.424+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:53:25.429+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:25.429+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:53:25.436+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:25.435+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:53:25.439+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:25.438+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:53:25.443+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:25.443+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:53:25.445+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:25.445+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:53:25.452+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:25.452+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:53:25.453+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:25.453+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:53:25.455+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:25.455+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:53:25.457+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:25.457+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:53:25.458+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:25.458+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:53:25.459+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:25.459+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:53:25.461+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:25.461+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:53:25.462+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:25.461+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:53:25.476+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:25.476+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:53:25.491+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:25.491+0000] {etl.py:167} ERROR - Unexpected error during S3 upload: 'Variable access_key does not exist'
[2025-05-24T22:53:25.491+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:25.491+0000] {etl.py:183} ERROR - ETL failed: 'Variable access_key does not exist'
[2025-05-24T22:53:25.495+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:53:25.503+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:25.502+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:53:25.514+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:25.514+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:53:25.524+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.153 seconds
[2025-05-24T22:53:56.279+0000] {processor.py:161} INFO - Started process (PID=14483) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:53:56.281+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:53:56.282+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:56.282+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:53:57.177+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:57.176+0000] {etl.py:172} INFO - Starting ETL pipeline
[2025-05-24T22:53:57.187+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:57.187+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:53:57.190+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:57.190+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:53:57.192+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:57.192+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:53:57.194+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:57.194+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:53:57.196+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:57.196+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:53:57.197+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:57.197+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:53:57.200+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:57.200+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:53:57.201+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:57.201+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:53:57.202+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:57.202+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:53:57.203+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:57.203+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:53:57.204+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:57.204+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:53:57.205+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:57.205+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:53:57.206+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:57.206+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:53:57.207+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:57.207+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:53:57.218+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:57.218+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:53:57.226+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:57.226+0000] {etl.py:167} ERROR - Unexpected error during S3 upload: 'Variable access_key does not exist'
[2025-05-24T22:53:57.227+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:57.227+0000] {etl.py:183} ERROR - ETL failed: 'Variable access_key does not exist'
[2025-05-24T22:53:57.231+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:53:57.237+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:57.237+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:53:57.249+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:53:57.249+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:53:57.258+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 0.986 seconds
[2025-05-24T22:54:30.656+0000] {processor.py:161} INFO - Started process (PID=14543) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:54:30.661+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:54:30.663+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:30.663+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:54:36.121+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:36.113+0000] {etl.py:172} INFO - Starting ETL pipeline
[2025-05-24T22:54:36.199+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:36.197+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:54:36.218+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:36.218+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:54:36.235+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:36.235+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:54:36.240+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:36.240+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:54:36.246+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:36.246+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:54:36.252+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:36.251+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:54:36.277+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:36.276+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:54:36.277+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:36.277+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:54:36.281+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:36.281+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:54:36.288+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:36.287+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:54:36.301+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:36.300+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:54:36.307+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:36.307+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:54:36.325+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:36.322+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:54:36.331+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:36.331+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:54:36.404+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:36.404+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:54:36.470+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:36.470+0000] {etl.py:167} ERROR - Unexpected error during S3 upload: 'Variable access_key does not exist'
[2025-05-24T22:54:36.471+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:36.471+0000] {etl.py:183} ERROR - ETL failed: 'Variable access_key does not exist'
[2025-05-24T22:54:36.489+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:54:36.519+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:36.519+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:54:36.550+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:54:36.549+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:54:36.590+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 6.180 seconds
[2025-05-24T22:55:06.656+0000] {processor.py:161} INFO - Started process (PID=14620) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:55:06.658+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:55:06.659+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:06.659+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:55:08.043+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:08.034+0000] {etl.py:172} INFO - Starting ETL pipeline
[2025-05-24T22:55:08.075+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:08.073+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:55:08.081+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:08.081+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:55:08.085+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:08.085+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:55:08.092+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:08.091+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:55:08.095+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:08.094+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:55:08.099+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:08.099+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:55:08.117+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:08.115+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:55:08.119+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:08.119+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:55:08.126+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:08.126+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:55:08.151+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:08.144+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:55:08.160+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:08.159+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:55:08.161+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:08.161+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:55:08.167+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:08.167+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:55:08.172+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:08.172+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:55:08.200+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:08.197+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:55:08.248+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:08.246+0000] {etl.py:167} ERROR - Unexpected error during S3 upload: 'Variable access_key does not exist'
[2025-05-24T22:55:08.248+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:08.248+0000] {etl.py:183} ERROR - ETL failed: 'Variable access_key does not exist'
[2025-05-24T22:55:08.253+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:55:08.285+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:08.284+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:55:08.339+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:55:08.335+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:55:08.392+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.742 seconds
[2025-05-24T22:56:03.261+0000] {processor.py:161} INFO - Started process (PID=14678) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:56:03.271+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:56:03.286+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:03.279+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:56:06.331+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:06.330+0000] {etl.py:172} INFO - Starting ETL pipeline
[2025-05-24T22:56:06.351+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:06.350+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:56:06.359+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:06.359+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:56:06.365+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:06.364+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:56:06.373+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:06.373+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:56:06.378+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:06.378+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:56:06.380+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:06.380+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:56:06.399+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:06.399+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:56:06.401+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:06.401+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:56:06.403+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:06.403+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:56:06.405+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:06.405+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:56:06.407+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:06.406+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:56:06.408+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:06.408+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:56:06.411+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:06.410+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:56:06.411+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:06.411+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:56:06.448+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:06.448+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:56:06.492+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:06.492+0000] {etl.py:167} ERROR - Unexpected error during S3 upload: 'Variable access_key does not exist'
[2025-05-24T22:56:06.494+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:06.494+0000] {etl.py:183} ERROR - ETL failed: 'Variable access_key does not exist'
[2025-05-24T22:56:06.501+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:56:06.526+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:06.524+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:56:06.549+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:06.548+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:56:06.569+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.374 seconds
[2025-05-24T22:56:38.579+0000] {processor.py:161} INFO - Started process (PID=14745) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:56:38.634+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:56:39.470+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:38.774+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:56:49.214+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:49.211+0000] {etl.py:172} INFO - Starting ETL pipeline
[2025-05-24T22:56:49.267+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:49.267+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:56:49.281+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:49.280+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:56:49.299+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:49.298+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:56:49.319+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:49.314+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:56:49.344+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:49.340+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:56:49.350+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:49.350+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:56:49.371+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:49.370+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:56:49.375+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:49.375+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:56:49.378+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:49.378+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:56:49.385+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:49.384+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:56:49.389+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:49.388+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:56:49.392+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:49.391+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:56:49.401+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:49.401+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:56:49.404+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:49.404+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:56:49.516+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:49.511+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:56:49.773+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:49.762+0000] {etl.py:167} ERROR - Unexpected error during S3 upload: 'Variable access_key does not exist'
[2025-05-24T22:56:49.776+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:49.776+0000] {etl.py:183} ERROR - ETL failed: 'Variable access_key does not exist'
[2025-05-24T22:56:49.812+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:56:49.876+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:49.872+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:56:49.951+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:56:49.951+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:56:49.986+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 12.479 seconds
[2025-05-24T22:57:20.644+0000] {processor.py:161} INFO - Started process (PID=14807) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:57:20.659+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:57:20.679+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:20.670+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:57:23.946+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:23.940+0000] {etl.py:172} INFO - Starting ETL pipeline
[2025-05-24T22:57:23.986+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:23.985+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:57:23.994+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:23.994+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:57:23.997+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:23.996+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:57:23.999+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:23.999+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:57:24.002+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:24.002+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:57:24.005+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:24.005+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:57:24.017+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:24.017+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:57:24.018+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:24.018+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:57:24.020+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:24.020+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:57:24.022+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:24.022+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:57:24.026+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:24.025+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:57:24.030+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:24.029+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:57:24.033+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:24.033+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:57:24.035+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:24.035+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:57:24.062+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:24.061+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:57:24.134+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:24.132+0000] {etl.py:167} ERROR - Unexpected error during S3 upload: 'Variable access_key does not exist'
[2025-05-24T22:57:24.134+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:24.134+0000] {etl.py:183} ERROR - ETL failed: 'Variable access_key does not exist'
[2025-05-24T22:57:24.145+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:57:24.170+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:24.166+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:57:24.191+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:24.191+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:57:24.218+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.721 seconds
[2025-05-24T22:57:57.330+0000] {processor.py:161} INFO - Started process (PID=14869) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:57:57.332+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:57:57.333+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:57.333+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:57:57.990+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:57.990+0000] {etl.py:172} INFO - Starting ETL pipeline
[2025-05-24T22:57:58.001+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:58.000+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:57:58.003+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:58.003+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:57:58.005+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:58.005+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:57:58.007+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:58.007+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:57:58.009+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:58.009+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:57:58.010+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:58.010+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:57:58.013+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:58.013+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:57:58.013+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:58.013+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:57:58.014+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:58.014+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:57:58.016+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:58.015+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:57:58.016+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:58.016+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:57:58.017+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:58.017+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:57:58.018+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:58.018+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:57:58.018+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:58.018+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:57:58.035+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:58.035+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:57:59.485+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:59.484+0000] {etl.py:159} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T22:57:59.489+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:59.488+0000] {etl.py:181} INFO - ETL completed successfully
[2025-05-24T22:57:59.505+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:57:59.520+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:59.519+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:57:59.535+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:57:59.535+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:57:59.547+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.252 seconds
[2025-05-24T22:58:29.783+0000] {processor.py:161} INFO - Started process (PID=14934) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:58:29.785+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:58:29.786+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:29.786+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:58:30.385+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:30.384+0000] {etl.py:172} INFO - Starting ETL pipeline
[2025-05-24T22:58:30.394+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:30.393+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:58:30.397+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:30.397+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:58:30.399+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:30.399+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:58:30.402+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:30.402+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:58:30.403+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:30.403+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:58:30.404+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:30.404+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:58:30.409+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:30.408+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:58:30.409+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:30.409+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:58:30.411+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:30.411+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:58:30.412+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:30.412+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:58:30.413+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:30.413+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:58:30.414+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:30.414+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:58:30.415+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:30.415+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:58:30.416+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:30.416+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:58:30.429+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:30.429+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:58:32.436+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:32.436+0000] {etl.py:159} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T22:58:32.437+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:32.437+0000] {etl.py:181} INFO - ETL completed successfully
[2025-05-24T22:58:32.441+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:58:32.449+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:32.449+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:58:32.461+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:58:32.461+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:58:32.472+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.694 seconds
[2025-05-24T22:59:04.816+0000] {processor.py:161} INFO - Started process (PID=15003) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:59:04.817+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:59:04.819+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:04.818+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:59:25.556+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:25.554+0000] {etl.py:172} INFO - Starting ETL pipeline
[2025-05-24T22:59:25.569+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:25.569+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:59:25.572+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:25.572+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:59:25.575+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:25.574+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:59:25.577+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:25.577+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:59:25.578+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:25.578+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:59:25.580+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:25.580+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:59:25.587+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:25.587+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:59:25.587+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:25.587+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:59:25.589+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:25.589+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:59:25.590+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:25.590+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:59:25.592+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:25.592+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:59:25.593+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:25.593+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:59:25.602+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:25.601+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:59:25.611+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:25.611+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:59:25.644+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:25.644+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T22:59:27.307+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:27.303+0000] {etl.py:159} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T22:59:27.311+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:27.311+0000] {etl.py:181} INFO - ETL completed successfully
[2025-05-24T22:59:27.322+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:59:27.340+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:27.339+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T22:59:27.375+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:27.375+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T22:59:27.391+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 22.582 seconds
[2025-05-24T22:59:57.865+0000] {processor.py:161} INFO - Started process (PID=15073) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:59:57.868+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T22:59:57.869+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:57.868+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T22:59:58.798+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:58.798+0000] {etl.py:172} INFO - Starting ETL pipeline
[2025-05-24T22:59:58.831+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:58.831+0000] {etl.py:20} INFO - csv read
[2025-05-24T22:59:58.841+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:58.841+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T22:59:58.844+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:58.843+0000] {etl.py:66} INFO - api data read
[2025-05-24T22:59:58.847+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:58.846+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T22:59:58.849+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:58.849+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T22:59:58.851+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:58.851+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T22:59:58.871+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:58.871+0000] {etl.py:79} INFO - queried db source
[2025-05-24T22:59:58.872+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:58.872+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T22:59:58.874+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:58.874+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T22:59:58.876+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:58.876+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T22:59:58.878+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:58.877+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T22:59:58.879+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:58.878+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T22:59:58.880+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:58.880+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T22:59:58.881+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:58.881+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T22:59:58.900+0000] {logging_mixin.py:188} INFO - [2025-05-24T22:59:58.900+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:00:00.716+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:00.711+0000] {etl.py:159} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:00:00.737+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:00.735+0000] {etl.py:181} INFO - ETL completed successfully
[2025-05-24T23:00:00.764+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:00:00.831+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:00.831+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:00:00.880+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:00.880+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:00:00.931+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.104 seconds
[2025-05-24T23:00:31.434+0000] {processor.py:161} INFO - Started process (PID=15157) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:00:31.436+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:00:31.438+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:31.437+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:00:32.212+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:32.212+0000] {etl.py:172} INFO - Starting ETL pipeline
[2025-05-24T23:00:32.219+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:32.219+0000] {etl.py:20} INFO - csv read
[2025-05-24T23:00:32.221+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:32.221+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T23:00:32.223+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:32.223+0000] {etl.py:66} INFO - api data read
[2025-05-24T23:00:32.225+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:32.225+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T23:00:32.226+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:32.226+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T23:00:32.227+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:32.227+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T23:00:32.237+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:32.237+0000] {etl.py:79} INFO - queried db source
[2025-05-24T23:00:32.238+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:32.238+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T23:00:32.240+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:32.240+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T23:00:32.242+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:32.241+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:00:32.243+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:32.243+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:00:32.244+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:32.244+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:00:32.246+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:32.246+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T23:00:32.247+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:32.247+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T23:00:32.267+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:32.266+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:00:33.885+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:33.884+0000] {etl.py:159} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:00:33.891+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:33.891+0000] {etl.py:181} INFO - ETL completed successfully
[2025-05-24T23:00:33.903+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:00:33.924+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:33.923+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:00:33.956+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:00:33.955+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:00:33.983+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.560 seconds
[2025-05-24T23:01:04.189+0000] {processor.py:161} INFO - Started process (PID=15229) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:01:04.190+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:01:04.191+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:04.191+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:01:05.142+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:05.142+0000] {etl.py:172} INFO - Starting ETL pipeline
[2025-05-24T23:01:05.160+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:05.159+0000] {etl.py:20} INFO - csv read
[2025-05-24T23:01:05.166+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:05.166+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T23:01:05.167+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:05.167+0000] {etl.py:66} INFO - api data read
[2025-05-24T23:01:05.169+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:05.169+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T23:01:05.170+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:05.170+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T23:01:05.172+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:05.172+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T23:01:05.176+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:05.176+0000] {etl.py:79} INFO - queried db source
[2025-05-24T23:01:05.177+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:05.176+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T23:01:05.180+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:05.180+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T23:01:05.190+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:05.190+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:01:05.195+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:05.195+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:01:05.197+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:05.197+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:01:05.199+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:05.199+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T23:01:05.200+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:05.200+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T23:01:05.270+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:05.269+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:01:08.391+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:08.383+0000] {etl.py:159} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:01:08.511+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:08.479+0000] {etl.py:181} INFO - ETL completed successfully
[2025-05-24T23:01:08.619+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:01:09.193+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:09.187+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:01:09.954+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:09.899+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:01:18.270+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 14.027 seconds
[2025-05-24T23:01:50.460+0000] {processor.py:161} INFO - Started process (PID=15305) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:01:50.464+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:01:50.466+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:50.465+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:01:52.513+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:52.512+0000] {etl.py:172} INFO - Starting ETL pipeline
[2025-05-24T23:01:52.536+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:52.536+0000] {etl.py:20} INFO - csv read
[2025-05-24T23:01:52.544+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:52.544+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T23:01:52.548+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:52.548+0000] {etl.py:66} INFO - api data read
[2025-05-24T23:01:52.553+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:52.552+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T23:01:52.557+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:52.557+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T23:01:52.565+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:52.563+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T23:01:52.599+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:52.599+0000] {etl.py:79} INFO - queried db source
[2025-05-24T23:01:52.601+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:52.600+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T23:01:52.607+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:52.607+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T23:01:52.617+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:52.616+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:01:52.625+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:52.624+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:01:52.631+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:52.631+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:01:52.639+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:52.638+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T23:01:52.649+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:52.648+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T23:01:52.692+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:52.692+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:01:55.384+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:55.383+0000] {etl.py:159} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:01:55.396+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:55.394+0000] {etl.py:181} INFO - ETL completed successfully
[2025-05-24T23:01:55.413+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:01:55.436+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:55.436+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:01:55.466+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:01:55.466+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:01:55.481+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 5.033 seconds
[2025-05-24T23:02:26.802+0000] {processor.py:161} INFO - Started process (PID=15374) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:02:26.806+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:02:26.811+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:26.810+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:02:28.066+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:28.066+0000] {etl.py:172} INFO - Starting ETL pipeline
[2025-05-24T23:02:28.091+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:28.090+0000] {etl.py:20} INFO - csv read
[2025-05-24T23:02:28.094+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:28.094+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T23:02:28.096+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:28.096+0000] {etl.py:66} INFO - api data read
[2025-05-24T23:02:28.098+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:28.098+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T23:02:28.101+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:28.100+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T23:02:28.102+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:28.101+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T23:02:28.108+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:28.107+0000] {etl.py:79} INFO - queried db source
[2025-05-24T23:02:28.109+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:28.109+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T23:02:28.112+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:28.112+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T23:02:28.114+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:28.114+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:02:28.116+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:28.116+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:02:28.118+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:28.118+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:02:28.119+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:28.119+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T23:02:28.120+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:28.120+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T23:02:28.138+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:28.137+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:02:31.611+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:31.608+0000] {etl.py:159} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:02:31.620+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:31.617+0000] {etl.py:181} INFO - ETL completed successfully
[2025-05-24T23:02:31.693+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:02:31.757+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:31.757+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:02:31.787+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:02:31.787+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:02:31.823+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 5.123 seconds
[2025-05-24T23:03:02.512+0000] {processor.py:161} INFO - Started process (PID=15440) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:03:02.517+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:03:02.519+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:02.518+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:03:03.892+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:03.892+0000] {etl.py:172} INFO - Starting ETL pipeline
[2025-05-24T23:03:03.905+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:03.904+0000] {etl.py:20} INFO - csv read
[2025-05-24T23:03:03.911+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:03.911+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T23:03:03.913+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:03.913+0000] {etl.py:66} INFO - api data read
[2025-05-24T23:03:03.916+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:03.916+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T23:03:03.918+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:03.918+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T23:03:03.919+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:03.919+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T23:03:03.924+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:03.924+0000] {etl.py:79} INFO - queried db source
[2025-05-24T23:03:03.924+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:03.924+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T23:03:03.926+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:03.926+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T23:03:03.927+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:03.927+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:03:03.928+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:03.928+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:03:03.929+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:03.929+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:03:03.931+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:03.931+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T23:03:03.932+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:03.932+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T23:03:03.943+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:03.943+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:03:05.675+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:05.675+0000] {etl.py:159} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:03:05.676+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:05.676+0000] {etl.py:181} INFO - ETL completed successfully
[2025-05-24T23:03:05.685+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:03:05.711+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:05.711+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:03:05.726+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:05.726+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:03:05.739+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.250 seconds
[2025-05-24T23:03:35.951+0000] {processor.py:161} INFO - Started process (PID=15506) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:03:35.953+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:03:35.955+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:35.954+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:03:36.721+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:36.720+0000] {etl.py:172} INFO - Starting ETL pipeline
[2025-05-24T23:03:36.729+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:36.729+0000] {etl.py:20} INFO - csv read
[2025-05-24T23:03:36.731+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:36.731+0000] {etl.py:22} INFO - csv data source pickled
[2025-05-24T23:03:36.732+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:36.732+0000] {etl.py:66} INFO - api data read
[2025-05-24T23:03:36.734+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:36.734+0000] {etl.py:68} INFO - api json data normalized
[2025-05-24T23:03:36.735+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:36.735+0000] {etl.py:70} INFO - api data source pickled
[2025-05-24T23:03:36.736+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:36.736+0000] {etl.py:77} INFO - connected to db source
[2025-05-24T23:03:36.739+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:36.739+0000] {etl.py:79} INFO - queried db source
[2025-05-24T23:03:36.740+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:36.739+0000] {etl.py:81} INFO - closed db connection
[2025-05-24T23:03:36.740+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:36.740+0000] {etl.py:83} INFO - db source pickled
[2025-05-24T23:03:36.741+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:36.741+0000] {etl.py:91} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:03:36.742+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:36.742+0000] {etl.py:93} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:03:36.743+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:36.743+0000] {etl.py:95} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:03:36.744+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:36.744+0000] {etl.py:103} INFO - DataFrames merged side-by-side
[2025-05-24T23:03:36.744+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:36.744+0000] {etl.py:112} INFO - Column names standardized post-merge
[2025-05-24T23:03:36.757+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:36.757+0000] {etl.py:118} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:03:38.600+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:38.599+0000] {etl.py:159} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:03:38.601+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:38.601+0000] {etl.py:181} INFO - ETL completed successfully
[2025-05-24T23:03:38.606+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:03:38.619+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:38.619+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:03:38.634+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:38.634+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:03:38.644+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.705 seconds
[2025-05-24T23:03:40.687+0000] {processor.py:161} INFO - Started process (PID=15528) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:03:40.689+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:03:40.690+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:40.690+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:03:41.657+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:41.656+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:03:41.666+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:41.665+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:03:41.668+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:41.668+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:03:41.673+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:41.673+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:03:41.676+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:41.676+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:03:41.678+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:41.678+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:03:41.681+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:41.681+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:03:41.685+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:41.685+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:03:41.685+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:41.685+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:03:41.687+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:41.687+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:03:41.689+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:41.689+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:03:41.693+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:41.692+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:03:41.696+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:41.696+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:03:41.697+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:41.697+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:03:41.697+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:41.697+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:03:41.721+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:41.719+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:03:43.396+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:43.395+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:03:43.397+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:43.397+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:03:43.414+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:03:43.427+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:43.427+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:03:43.447+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:03:43.447+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:03:43.461+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.797 seconds
[2025-05-24T23:04:14.289+0000] {processor.py:161} INFO - Started process (PID=15594) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:04:14.293+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:04:14.295+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:14.294+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:04:15.505+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:15.504+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:04:15.526+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:15.525+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:04:15.529+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:15.529+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:04:15.531+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:15.531+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:04:15.534+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:15.534+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:04:15.537+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:15.537+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:04:15.539+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:15.539+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:04:15.548+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:15.547+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:04:15.549+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:15.548+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:04:15.551+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:15.551+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:04:15.553+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:15.553+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:04:15.554+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:15.554+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:04:15.556+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:15.556+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:04:15.557+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:15.557+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:04:15.558+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:15.558+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:04:15.582+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:15.582+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:04:17.526+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:17.524+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:04:17.531+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:17.531+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:04:17.548+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:04:17.590+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:17.589+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:04:17.620+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:17.620+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:04:17.644+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.369 seconds
[2025-05-24T23:04:47.909+0000] {processor.py:161} INFO - Started process (PID=15660) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:04:47.912+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:04:47.913+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:47.912+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:04:49.024+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:49.018+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:04:49.064+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:49.059+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:04:49.072+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:49.072+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:04:49.078+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:49.078+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:04:49.100+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:49.097+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:04:49.109+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:49.108+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:04:49.115+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:49.114+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:04:49.150+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:49.150+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:04:49.151+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:49.150+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:04:49.155+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:49.155+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:04:49.161+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:49.161+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:04:49.163+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:49.162+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:04:49.165+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:49.164+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:04:49.178+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:49.178+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:04:49.182+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:49.182+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:04:49.243+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:49.235+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:04:55.036+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:55.028+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:04:55.044+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:55.044+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:04:55.059+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:04:55.080+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:55.080+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:04:55.099+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:04:55.098+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:04:55.123+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 7.226 seconds
[2025-05-24T23:05:26.949+0000] {processor.py:161} INFO - Started process (PID=15727) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:05:27.064+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:05:27.262+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:05:27.201+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:05:58.809+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:05:58.722+0000] {timeout.py:68} ERROR - Process timed out, PID: 15727
[2025-05-24T23:06:29.665+0000] {processor.py:161} INFO - Started process (PID=15788) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:06:29.672+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:06:29.682+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:29.675+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:06:30.596+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:30.595+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:06:30.605+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:30.604+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:06:30.607+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:30.607+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:06:30.609+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:30.609+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:06:30.611+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:30.611+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:06:30.613+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:30.613+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:06:30.614+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:30.614+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:06:30.619+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:30.619+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:06:30.619+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:30.619+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:06:30.621+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:30.621+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:06:30.623+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:30.623+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:06:30.624+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:30.624+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:06:30.625+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:30.625+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:06:30.626+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:30.626+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:06:30.627+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:30.627+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:06:30.639+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:30.639+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:06:32.418+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:32.417+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:06:32.420+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:32.420+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:06:32.426+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:06:32.436+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:32.436+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:06:32.449+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:06:32.449+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:06:32.461+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.814 seconds
[2025-05-24T23:07:03.074+0000] {processor.py:161} INFO - Started process (PID=15861) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:07:03.079+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:07:03.081+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:03.080+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:07:04.282+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:04.282+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:07:04.293+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:04.293+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:07:04.299+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:04.298+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:07:04.302+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:04.301+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:07:04.304+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:04.304+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:07:04.306+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:04.306+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:07:04.308+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:04.308+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:07:04.316+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:04.316+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:07:04.317+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:04.317+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:07:04.321+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:04.321+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:07:04.326+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:04.325+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:07:04.327+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:04.327+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:07:04.329+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:04.329+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:07:04.332+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:04.331+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:07:04.333+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:04.332+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:07:04.348+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:04.348+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:07:05.910+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:05.909+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:07:05.912+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:05.912+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:07:05.917+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:07:05.928+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:05.927+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:07:05.942+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:05.941+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:07:05.951+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.917 seconds
[2025-05-24T23:07:36.946+0000] {processor.py:161} INFO - Started process (PID=15932) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:07:36.948+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:07:36.949+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:36.949+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:07:37.329+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:37.329+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:07:37.337+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:37.337+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:07:37.339+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:37.339+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:07:37.341+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:37.341+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:07:37.343+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:37.343+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:07:37.344+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:37.344+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:07:37.345+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:37.345+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:07:37.348+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:37.348+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:07:37.349+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:37.349+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:07:37.350+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:37.349+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:07:37.351+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:37.351+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:07:37.351+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:37.351+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:07:37.352+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:37.352+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:07:37.353+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:37.353+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:07:37.353+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:37.353+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:07:37.365+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:37.365+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:07:39.987+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:39.985+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:07:39.991+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:39.991+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:07:40.021+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:07:40.035+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:40.034+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:07:40.050+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:07:40.049+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:07:40.062+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.126 seconds
[2025-05-24T23:08:10.130+0000] {processor.py:161} INFO - Started process (PID=16008) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:08:10.132+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:08:10.132+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:10.132+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:08:10.503+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:10.502+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:08:10.512+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:10.512+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:08:10.514+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:10.514+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:08:10.517+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:10.516+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:08:10.519+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:10.519+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:08:10.520+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:10.520+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:08:10.522+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:10.521+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:08:10.525+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:10.525+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:08:10.525+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:10.525+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:08:10.527+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:10.527+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:08:10.528+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:10.528+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:08:10.529+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:10.529+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:08:10.530+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:10.530+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:08:10.530+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:10.530+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:08:10.531+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:10.531+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:08:10.542+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:10.542+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:08:12.357+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:12.356+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:08:12.361+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:12.360+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:08:12.368+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:08:12.385+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:12.384+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:08:12.401+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:12.401+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:08:12.413+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.288 seconds
[2025-05-24T23:08:42.613+0000] {processor.py:161} INFO - Started process (PID=16075) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:08:42.616+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:08:42.617+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:42.616+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:08:43.003+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:43.002+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:08:43.014+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:43.014+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:08:43.017+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:43.017+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:08:43.019+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:43.019+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:08:43.021+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:43.021+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:08:43.023+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:43.023+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:08:43.024+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:43.024+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:08:43.027+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:43.027+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:08:43.027+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:43.027+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:08:43.029+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:43.029+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:08:43.030+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:43.030+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:08:43.031+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:43.031+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:08:43.032+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:43.032+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:08:43.033+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:43.032+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:08:43.033+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:43.033+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:08:43.044+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:43.044+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:08:45.024+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:45.023+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:08:45.026+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:45.026+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:08:45.030+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:08:45.043+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:45.043+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:08:45.058+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:08:45.058+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:08:45.071+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.467 seconds
[2025-05-24T23:09:15.669+0000] {processor.py:161} INFO - Started process (PID=16144) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:09:15.671+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:09:15.671+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:15.671+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:09:16.013+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:16.013+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:09:16.025+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:16.025+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:09:16.027+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:16.027+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:09:16.030+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:16.030+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:09:16.031+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:16.031+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:09:16.032+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:16.032+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:09:16.033+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:16.033+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:09:16.037+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:16.037+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:09:16.037+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:16.037+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:09:16.038+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:16.038+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:09:16.039+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:16.039+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:09:16.040+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:16.040+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:09:16.040+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:16.040+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:09:16.041+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:16.041+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:09:16.041+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:16.041+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:09:16.051+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:16.051+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:09:18.395+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:18.394+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:09:18.399+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:18.398+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:09:18.405+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:09:18.419+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:18.419+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:09:18.433+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:18.433+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:09:18.443+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.779 seconds
[2025-05-24T23:09:49.217+0000] {processor.py:161} INFO - Started process (PID=16213) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:09:49.218+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:09:49.219+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:49.218+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:09:49.613+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:49.613+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:09:49.623+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:49.623+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:09:49.626+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:49.625+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:09:49.628+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:49.628+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:09:49.629+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:49.629+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:09:49.632+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:49.632+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:09:49.633+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:49.633+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:09:49.640+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:49.640+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:09:49.641+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:49.641+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:09:49.644+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:49.643+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:09:49.647+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:49.647+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:09:49.648+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:49.648+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:09:49.649+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:49.648+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:09:49.650+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:49.650+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:09:49.651+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:49.651+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:09:49.666+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:49.665+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:09:51.445+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:51.445+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:09:51.446+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:51.446+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:09:51.450+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:09:51.460+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:51.460+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:09:51.473+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:09:51.473+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:09:51.483+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.273 seconds
[2025-05-24T23:10:22.119+0000] {processor.py:161} INFO - Started process (PID=16290) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:10:22.120+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:10:22.121+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:22.121+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:10:22.685+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:22.685+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:10:22.699+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:22.699+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:10:22.701+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:22.701+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:10:22.703+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:22.703+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:10:22.705+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:22.705+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:10:22.707+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:22.707+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:10:22.709+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:22.709+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:10:22.716+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:22.716+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:10:22.719+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:22.717+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:10:22.724+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:22.724+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:10:22.730+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:22.730+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:10:22.733+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:22.733+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:10:22.736+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:22.736+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:10:22.737+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:22.737+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:10:22.739+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:22.739+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:10:22.750+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:22.750+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:10:24.872+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:24.870+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:10:24.875+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:24.874+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:10:24.883+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:10:24.898+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:24.897+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:10:24.917+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:24.917+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:10:24.930+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.839 seconds
[2025-05-24T23:10:55.965+0000] {processor.py:161} INFO - Started process (PID=16372) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:10:55.966+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:10:55.967+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:55.966+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:10:56.331+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:56.331+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:10:56.340+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:56.340+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:10:56.342+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:56.342+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:10:56.345+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:56.344+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:10:56.346+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:56.346+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:10:56.347+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:56.347+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:10:56.348+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:56.348+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:10:56.352+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:56.352+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:10:56.352+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:56.352+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:10:56.353+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:56.353+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:10:56.355+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:56.355+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:10:56.356+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:56.356+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:10:56.357+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:56.357+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:10:56.358+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:56.358+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:10:56.358+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:56.358+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:10:56.370+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:56.370+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:10:58.393+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:58.392+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:10:58.396+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:58.396+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:10:58.403+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:10:58.419+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:58.419+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:10:58.439+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:10:58.439+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:10:58.450+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.490 seconds
[2025-05-24T23:11:28.777+0000] {processor.py:161} INFO - Started process (PID=16438) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:11:28.780+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:11:28.782+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:28.781+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:11:29.381+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:29.379+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:11:29.401+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:29.401+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:11:29.405+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:29.405+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:11:29.408+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:29.407+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:11:29.410+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:29.410+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:11:29.411+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:29.411+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:11:29.414+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:29.414+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:11:29.420+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:29.419+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:11:29.420+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:29.420+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:11:29.421+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:29.421+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:11:29.422+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:29.422+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:11:29.423+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:29.423+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:11:29.424+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:29.424+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:11:29.426+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:29.426+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:11:29.427+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:29.426+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:11:29.445+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:29.444+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:11:31.677+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:31.673+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:11:31.683+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:31.682+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:11:31.691+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:11:31.718+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:31.717+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:11:31.733+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:11:31.733+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:11:31.746+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.984 seconds
[2025-05-24T23:12:01.905+0000] {processor.py:161} INFO - Started process (PID=16506) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:12:01.906+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:12:01.908+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:01.907+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:12:02.540+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:02.537+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:12:02.551+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:02.551+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:12:02.554+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:02.554+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:12:02.556+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:02.556+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:12:02.558+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:02.558+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:12:02.559+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:02.559+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:12:02.562+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:02.562+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:12:02.566+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:02.566+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:12:02.566+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:02.566+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:12:02.568+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:02.568+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:12:02.571+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:02.571+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:12:02.572+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:02.572+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:12:02.573+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:02.573+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:12:02.575+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:02.574+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:12:02.576+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:02.576+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:12:02.587+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:02.587+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:12:06.423+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:06.407+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:12:06.436+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:06.435+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:12:06.468+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:12:06.541+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:06.539+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:12:06.588+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:06.587+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:12:06.625+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 4.733 seconds
[2025-05-24T23:12:37.337+0000] {processor.py:161} INFO - Started process (PID=16579) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:12:37.338+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:12:37.339+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:37.338+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:12:37.675+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:37.674+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:12:37.684+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:37.684+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:12:37.687+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:37.687+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:12:37.689+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:37.688+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:12:37.690+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:37.690+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:12:37.691+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:37.691+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:12:37.693+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:37.693+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:12:37.696+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:37.696+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:12:37.696+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:37.696+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:12:37.698+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:37.698+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:12:37.700+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:37.700+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:12:37.702+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:37.702+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:12:37.704+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:37.703+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:12:37.706+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:37.706+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:12:37.708+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:37.708+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:12:37.721+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:37.721+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:12:39.721+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:39.717+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:12:39.736+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:39.733+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:12:39.779+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:12:39.817+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:39.815+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:12:39.842+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:12:39.842+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:12:39.856+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.525 seconds
[2025-05-24T23:13:10.119+0000] {processor.py:161} INFO - Started process (PID=16645) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:13:10.120+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:13:10.121+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:10.121+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:13:10.508+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:10.508+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:13:10.519+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:10.519+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:13:10.521+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:10.521+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:13:10.523+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:10.523+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:13:10.524+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:10.524+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:13:10.526+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:10.525+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:13:10.526+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:10.526+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:13:10.530+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:10.530+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:13:10.531+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:10.531+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:13:10.532+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:10.532+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:13:10.533+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:10.533+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:13:10.534+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:10.534+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:13:10.535+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:10.535+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:13:10.536+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:10.536+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:13:10.536+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:10.536+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:13:10.547+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:10.547+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:13:14.025+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:14.024+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:13:14.028+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:14.028+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:13:14.041+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:13:14.065+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:14.064+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:13:14.096+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:14.096+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:13:14.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.998 seconds
[2025-05-24T23:13:44.762+0000] {processor.py:161} INFO - Started process (PID=16718) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:13:44.765+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:13:44.766+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:44.765+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:13:45.162+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:45.162+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:13:45.181+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:45.181+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:13:45.183+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:45.183+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:13:45.185+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:45.185+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:13:45.187+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:45.187+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:13:45.189+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:45.188+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:13:45.190+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:45.189+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:13:45.194+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:45.193+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:13:45.194+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:45.194+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:13:45.195+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:45.195+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:13:45.197+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:45.197+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:13:45.199+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:45.198+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:13:45.200+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:45.200+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:13:45.201+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:45.201+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:13:45.202+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:45.201+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:13:45.216+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:45.215+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:13:47.104+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:47.104+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:13:47.108+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:47.108+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:13:47.111+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:13:47.123+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:47.123+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:13:47.136+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:13:47.136+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:13:47.145+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.393 seconds
[2025-05-24T23:14:17.782+0000] {processor.py:161} INFO - Started process (PID=16788) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:14:17.782+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:14:17.783+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:17.783+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:14:18.220+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:18.220+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:14:18.228+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:18.228+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:14:18.230+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:18.230+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:14:18.232+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:18.232+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:14:18.236+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:18.236+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:14:18.237+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:18.237+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:14:18.240+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:18.240+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:14:18.249+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:18.249+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:14:18.249+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:18.249+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:14:18.251+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:18.251+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:14:18.253+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:18.253+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:14:18.254+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:18.254+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:14:18.255+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:18.255+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:14:18.257+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:18.256+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:14:18.257+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:18.257+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:14:18.270+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:18.270+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:14:20.473+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:20.468+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:14:20.478+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:20.477+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:14:20.486+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:14:20.499+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:20.499+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:14:20.522+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:20.522+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:14:20.534+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.757 seconds
[2025-05-24T23:14:51.139+0000] {processor.py:161} INFO - Started process (PID=16858) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:14:51.141+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:14:51.142+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:51.141+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:14:51.466+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:51.466+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:14:51.475+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:51.475+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:14:51.478+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:51.478+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:14:51.479+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:51.479+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:14:51.481+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:51.481+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:14:51.482+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:51.482+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:14:51.482+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:51.482+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:14:51.485+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:51.485+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:14:51.485+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:51.485+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:14:51.486+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:51.486+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:14:51.487+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:51.487+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:14:51.488+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:51.488+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:14:51.489+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:51.489+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:14:51.489+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:51.489+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:14:51.490+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:51.490+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:14:51.500+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:51.500+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:14:53.379+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:53.379+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:14:53.382+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:53.382+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:14:53.388+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:14:53.396+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:53.395+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:14:53.410+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:14:53.410+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:14:53.420+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.288 seconds
[2025-05-24T23:15:23.962+0000] {processor.py:161} INFO - Started process (PID=16925) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:15:23.965+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:15:23.966+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:23.966+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:15:24.418+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:24.418+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:15:24.429+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:24.428+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:15:24.431+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:24.430+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:15:24.432+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:24.432+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:15:24.434+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:24.434+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:15:24.435+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:24.435+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:15:24.436+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:24.436+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:15:24.439+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:24.439+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:15:24.439+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:24.439+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:15:24.440+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:24.440+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:15:24.441+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:24.441+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:15:24.442+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:24.442+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:15:24.443+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:24.443+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:15:24.444+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:24.444+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:15:24.444+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:24.444+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:15:24.454+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:24.454+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:15:26.716+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:26.715+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:15:26.718+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:26.718+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:15:26.730+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:15:26.840+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:26.838+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:15:26.908+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:26.908+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:15:26.988+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.046 seconds
[2025-05-24T23:15:57.483+0000] {processor.py:161} INFO - Started process (PID=16996) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:15:57.484+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:15:57.485+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:57.485+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:15:57.876+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:57.876+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:15:57.884+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:57.883+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:15:57.886+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:57.885+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:15:57.887+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:57.887+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:15:57.889+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:57.889+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:15:57.890+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:57.890+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:15:57.891+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:57.891+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:15:57.894+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:57.894+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:15:57.895+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:57.895+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:15:57.896+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:57.895+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:15:57.897+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:57.897+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:15:57.898+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:57.898+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:15:57.899+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:57.899+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:15:57.900+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:57.900+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:15:57.901+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:57.901+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:15:57.911+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:15:57.910+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:16:01.644+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:01.644+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:16:01.646+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:01.646+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:16:01.650+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:16:01.669+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:01.668+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:16:01.687+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:01.686+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:16:01.697+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 4.220 seconds
[2025-05-24T23:16:32.245+0000] {processor.py:161} INFO - Started process (PID=17063) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:16:32.248+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:16:32.249+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:32.249+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:16:32.808+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:32.807+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:16:32.816+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:32.816+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:16:32.818+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:32.818+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:16:32.821+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:32.821+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:16:32.823+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:32.823+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:16:32.825+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:32.825+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:16:32.827+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:32.827+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:16:32.834+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:32.834+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:16:32.834+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:32.834+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:16:32.836+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:32.836+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:16:32.837+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:32.837+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:16:32.838+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:32.838+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:16:32.839+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:32.839+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:16:32.840+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:32.840+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:16:32.841+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:32.841+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:16:32.860+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:32.859+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:16:34.922+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:34.921+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:16:34.925+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:34.925+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:16:34.932+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:16:34.944+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:34.943+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:16:34.958+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:16:34.958+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:16:34.967+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.745 seconds
[2025-05-24T23:17:05.458+0000] {processor.py:161} INFO - Started process (PID=17161) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:17:05.459+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:17:05.460+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:05.460+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:17:05.986+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:05.986+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:17:06.004+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:06.004+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:17:06.011+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:06.010+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:17:06.013+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:06.013+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:17:06.017+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:06.017+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:17:06.020+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:06.020+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:17:06.023+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:06.023+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:17:06.029+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:06.029+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:17:06.029+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:06.029+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:17:06.031+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:06.031+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:17:06.033+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:06.033+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:17:06.040+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:06.040+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:17:06.042+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:06.042+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:17:06.045+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:06.045+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:17:06.047+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:06.047+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:17:06.068+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:06.067+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:17:08.213+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:08.211+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:17:08.217+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:08.217+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:17:08.226+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:17:08.238+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:08.238+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:17:08.255+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:08.255+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:17:08.267+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.816 seconds
[2025-05-24T23:17:38.437+0000] {processor.py:161} INFO - Started process (PID=17227) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:17:38.439+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:17:38.441+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:38.440+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:17:38.860+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:38.860+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:17:38.873+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:38.872+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:17:38.876+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:38.875+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:17:38.878+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:38.878+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:17:38.880+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:38.880+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:17:38.882+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:38.881+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:17:38.883+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:38.883+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:17:38.890+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:38.890+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:17:38.891+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:38.891+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:17:38.892+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:38.892+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:17:38.894+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:38.894+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:17:38.895+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:38.895+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:17:38.896+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:38.896+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:17:38.897+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:38.897+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:17:38.898+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:38.898+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:17:38.911+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:38.911+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:17:40.688+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:40.687+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:17:40.691+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:40.690+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:17:40.697+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:17:40.707+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:40.707+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:17:40.726+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:17:40.725+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:17:40.739+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.312 seconds
[2025-05-24T23:18:11.163+0000] {processor.py:161} INFO - Started process (PID=17293) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:18:11.166+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:18:11.167+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:11.166+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:18:11.857+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:11.857+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:18:11.874+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:11.874+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:18:11.877+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:11.877+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:18:11.879+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:11.879+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:18:11.881+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:11.881+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:18:11.883+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:11.883+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:18:11.884+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:11.884+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:18:11.890+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:11.890+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:18:11.890+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:11.890+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:18:11.893+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:11.892+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:18:11.894+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:11.894+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:18:11.895+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:11.895+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:18:11.897+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:11.897+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:18:11.898+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:11.898+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:18:11.899+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:11.899+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:18:11.912+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:11.912+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:18:14.046+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:14.043+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:18:14.053+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:14.053+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:18:14.078+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:18:14.097+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:14.097+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:18:14.131+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:14.131+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:18:14.149+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.999 seconds
[2025-05-24T23:18:57.333+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:18:57.336+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:18:57.340+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:57.339+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:18:59.016+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:59.016+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:18:59.059+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:59.059+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:18:59.076+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:59.075+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:18:59.102+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:59.101+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:18:59.107+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:59.106+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:18:59.120+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:59.120+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:18:59.124+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:59.124+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:18:59.146+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:59.145+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:18:59.148+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:59.148+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:18:59.160+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:59.160+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:18:59.169+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:59.168+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:18:59.173+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:59.173+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:18:59.179+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:59.179+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:18:59.183+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:59.182+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:18:59.186+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:59.185+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:18:59.338+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:18:59.337+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:19:01.629+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:01.625+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:19:01.642+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:01.640+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:19:01.699+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:19:01.809+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:01.804+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:19:01.859+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:01.859+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:19:01.901+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 4.577 seconds
[2025-05-24T23:19:32.852+0000] {processor.py:161} INFO - Started process (PID=112) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:19:32.853+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:19:32.855+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:32.855+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:19:33.368+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:33.368+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:19:33.376+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:33.376+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:19:33.382+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:33.382+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:19:33.386+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:33.386+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:19:33.389+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:33.389+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:19:33.391+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:33.391+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:19:33.392+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:33.392+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:19:33.397+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:33.397+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:19:33.398+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:33.398+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:19:33.399+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:33.399+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:19:33.402+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:33.401+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:19:33.403+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:33.403+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:19:33.405+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:33.405+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:19:33.406+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:33.405+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:19:33.407+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:33.407+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:19:33.431+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:33.431+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:19:35.178+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:35.177+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:19:35.190+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:35.189+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:19:35.221+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:19:35.291+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:35.290+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:19:35.310+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:19:35.309+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:19:35.335+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.491 seconds
[2025-05-24T23:20:05.810+0000] {processor.py:161} INFO - Started process (PID=182) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:20:05.811+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:20:05.815+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:05.815+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:20:06.307+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:06.307+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:20:06.314+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:06.313+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:20:06.317+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:06.317+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:20:06.318+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:06.318+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:20:06.320+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:06.320+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:20:06.322+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:06.322+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:20:06.323+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:06.322+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:20:06.326+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:06.326+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:20:06.326+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:06.326+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:20:06.328+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:06.328+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:20:06.330+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:06.329+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:20:06.331+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:06.331+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:20:06.332+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:06.332+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:20:06.333+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:06.333+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:20:06.334+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:06.334+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:20:06.352+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:06.352+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:20:07.979+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:07.979+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:20:07.980+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:07.980+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:20:07.983+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:20:07.989+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:07.988+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:20:07.999+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:07.999+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:20:08.008+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.203 seconds
[2025-05-24T23:20:38.193+0000] {processor.py:161} INFO - Started process (PID=262) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:20:38.193+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:20:38.194+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:38.194+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:20:38.565+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:38.565+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:20:38.582+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:38.582+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:20:38.584+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:38.584+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:20:38.586+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:38.586+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:20:38.588+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:38.588+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:20:38.589+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:38.589+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:20:38.589+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:38.589+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:20:38.592+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:38.592+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:20:38.592+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:38.592+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:20:38.593+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:38.593+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:20:38.594+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:38.594+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:20:38.595+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:38.595+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:20:38.596+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:38.596+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:20:38.597+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:38.597+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:20:38.597+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:38.597+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:20:38.608+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:38.607+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:20:40.080+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:40.080+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:20:40.083+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:40.082+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:20:40.089+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:20:40.102+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:40.102+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:20:40.131+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:20:40.130+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:20:40.140+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.951 seconds
[2025-05-24T23:21:10.375+0000] {processor.py:161} INFO - Started process (PID=323) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:21:10.378+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:21:10.379+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:10.379+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:21:10.809+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:10.809+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:21:10.815+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:10.815+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:21:10.818+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:10.818+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:21:10.820+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:10.820+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:21:10.821+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:10.821+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:21:10.823+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:10.823+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:21:10.824+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:10.824+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:21:10.827+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:10.827+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:21:10.827+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:10.827+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:21:10.829+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:10.829+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:21:10.830+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:10.830+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:21:10.831+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:10.831+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:21:10.832+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:10.832+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:21:10.833+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:10.833+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:21:10.833+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:10.833+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:21:10.848+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:10.847+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:21:14.516+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:14.511+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:21:14.523+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:14.522+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:21:14.534+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:21:14.547+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:14.547+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:21:14.573+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:14.573+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:21:14.588+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 4.217 seconds
[2025-05-24T23:21:45.212+0000] {processor.py:161} INFO - Started process (PID=421) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:21:45.216+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:21:45.217+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:45.217+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:21:45.738+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:45.738+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:21:45.746+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:45.746+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:21:45.751+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:45.751+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:21:45.753+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:45.753+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:21:45.755+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:45.755+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:21:45.757+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:45.757+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:21:45.758+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:45.758+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:21:45.763+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:45.763+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:21:45.763+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:45.763+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:21:45.766+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:45.765+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:21:45.768+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:45.767+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:21:45.769+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:45.769+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:21:45.770+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:45.770+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:21:45.771+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:45.771+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:21:45.771+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:45.771+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:21:45.786+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:45.785+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:21:47.551+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:47.551+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:21:47.555+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:47.555+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:21:47.566+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:21:47.584+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:47.584+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:21:47.601+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:21:47.601+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:21:47.615+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.407 seconds
[2025-05-24T23:22:17.769+0000] {processor.py:161} INFO - Started process (PID=489) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:22:17.770+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:22:17.771+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:17.771+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:22:18.050+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:18.050+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:22:18.056+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:18.056+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:22:18.057+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:18.057+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:22:18.059+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:18.059+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:22:18.060+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:18.060+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:22:18.061+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:18.061+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:22:18.062+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:18.062+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:22:18.065+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:18.065+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:22:18.065+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:18.065+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:22:18.067+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:18.067+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:22:18.068+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:18.068+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:22:18.069+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:18.069+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:22:18.070+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:18.070+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:22:18.070+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:18.070+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:22:18.071+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:18.071+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:22:18.081+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:18.081+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:22:19.801+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:19.800+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:22:19.804+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:19.804+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:22:19.812+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:22:19.831+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:19.831+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:22:19.851+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:19.851+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:22:19.861+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.095 seconds
[2025-05-24T23:22:50.139+0000] {processor.py:161} INFO - Started process (PID=555) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:22:50.141+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:22:50.144+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:50.143+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:22:50.456+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:50.455+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:22:50.467+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:50.467+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:22:50.469+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:50.469+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:22:50.471+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:50.471+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:22:50.473+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:50.473+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:22:50.474+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:50.474+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:22:50.475+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:50.475+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:22:50.478+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:50.477+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:22:50.478+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:50.478+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:22:50.479+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:50.479+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:22:50.480+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:50.480+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:22:50.481+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:50.481+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:22:50.482+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:50.482+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:22:50.483+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:50.483+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:22:50.483+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:50.483+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:22:50.496+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:50.496+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:22:55.305+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:55.303+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:22:55.309+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:55.309+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:22:55.324+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:22:55.349+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:55.348+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:22:55.366+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:22:55.365+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:22:55.375+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 5.245 seconds
[2025-05-24T23:23:25.772+0000] {processor.py:161} INFO - Started process (PID=625) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:23:25.775+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:23:25.778+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:25.778+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:23:26.065+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:26.065+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:23:26.069+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:26.069+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:23:26.071+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:26.071+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:23:26.072+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:26.072+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:23:26.073+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:26.073+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:23:26.074+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:26.074+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:23:26.075+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:26.074+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:23:26.077+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:26.077+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:23:26.077+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:26.077+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:23:26.078+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:26.078+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:23:26.079+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:26.078+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:23:26.079+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:26.079+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:23:26.080+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:26.080+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:23:26.080+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:26.080+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:23:26.081+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:26.081+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:23:26.090+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:26.089+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:23:27.711+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:27.710+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:23:27.714+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:27.713+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:23:27.721+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:23:27.734+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:27.734+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:23:27.755+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:27.754+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:23:27.771+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.008 seconds
[2025-05-24T23:23:57.986+0000] {processor.py:161} INFO - Started process (PID=691) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:23:57.987+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:23:57.989+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:57.988+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:23:58.279+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:58.279+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:23:58.284+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:58.284+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:23:58.287+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:58.286+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:23:58.288+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:58.288+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:23:58.289+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:58.289+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:23:58.290+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:58.290+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:23:58.291+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:58.291+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:23:58.293+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:58.293+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:23:58.293+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:58.293+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:23:58.294+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:58.294+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:23:58.295+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:58.295+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:23:58.296+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:58.295+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:23:58.296+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:58.296+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:23:58.298+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:58.297+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:23:58.298+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:58.298+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:23:58.310+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:58.310+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:23:59.822+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:59.821+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:23:59.824+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:59.824+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:23:59.828+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:23:59.837+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:59.836+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:23:59.848+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:23:59.848+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:23:59.857+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.875 seconds
[2025-05-24T23:24:30.698+0000] {processor.py:161} INFO - Started process (PID=763) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:24:30.700+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:24:30.702+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:30.702+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:24:31.144+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:31.144+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:24:31.148+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:31.148+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:24:31.150+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:31.150+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:24:31.151+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:31.151+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:24:31.152+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:31.152+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:24:31.153+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:31.153+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:24:31.154+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:31.154+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:24:31.157+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:31.157+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:24:31.157+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:31.157+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:24:31.159+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:31.159+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:24:31.160+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:31.160+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:24:31.161+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:31.161+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:24:31.163+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:31.162+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:24:31.163+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:31.163+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:24:31.164+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:31.164+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:24:31.174+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:31.174+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:24:32.820+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:32.819+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:24:32.823+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:32.823+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:24:32.829+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:24:32.842+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:32.841+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:24:32.858+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:24:32.858+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:24:32.869+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.177 seconds
[2025-05-24T23:25:03.346+0000] {processor.py:161} INFO - Started process (PID=829) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:25:03.348+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:25:03.350+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:03.349+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:25:03.769+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:03.769+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:25:03.774+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:03.774+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:25:03.776+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:03.776+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:25:03.777+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:03.777+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:25:03.779+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:03.779+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:25:03.780+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:03.780+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:25:03.780+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:03.780+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:25:03.783+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:03.783+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:25:03.783+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:03.783+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:25:03.784+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:03.784+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:25:03.785+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:03.785+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:25:03.785+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:03.785+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:25:03.787+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:03.786+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:25:03.787+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:03.787+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:25:03.788+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:03.788+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:25:03.797+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:03.797+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:25:05.222+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:05.221+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:25:05.224+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:05.224+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:25:05.230+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:25:05.247+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:05.246+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:25:05.260+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:05.260+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:25:05.270+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.934 seconds
[2025-05-24T23:25:35.521+0000] {processor.py:161} INFO - Started process (PID=895) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:25:35.522+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:25:35.524+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:35.524+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:25:35.903+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:35.903+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:25:35.908+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:35.908+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:25:35.910+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:35.910+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:25:35.912+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:35.912+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:25:35.914+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:35.914+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:25:35.915+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:35.915+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:25:35.916+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:35.916+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:25:35.918+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:35.918+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:25:35.918+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:35.918+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:25:35.919+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:35.919+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:25:35.920+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:35.920+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:25:35.921+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:35.921+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:25:35.922+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:35.922+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:25:35.922+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:35.922+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:25:35.923+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:35.923+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:25:35.932+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:35.932+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:25:37.700+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:37.699+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:25:37.701+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:37.701+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:25:37.704+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:25:37.713+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:37.712+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:25:37.729+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:25:37.729+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:25:37.742+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.228 seconds
[2025-05-24T23:26:08.390+0000] {processor.py:161} INFO - Started process (PID=961) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:26:08.392+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:26:08.395+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:08.394+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:26:08.730+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:08.730+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:26:08.735+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:08.735+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:26:08.736+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:08.736+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:26:08.738+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:08.738+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:26:08.739+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:08.739+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:26:08.740+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:08.740+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:26:08.741+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:08.741+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:26:08.743+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:08.743+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:26:08.743+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:08.743+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:26:08.744+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:08.744+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:26:08.745+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:08.745+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:26:08.745+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:08.745+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:26:08.746+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:08.746+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:26:08.747+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:08.747+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:26:08.747+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:08.747+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:26:08.756+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:08.756+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:26:12.113+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:12.113+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:26:12.116+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:12.116+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:26:12.119+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:26:12.130+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:12.129+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:26:12.145+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:12.144+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:26:12.160+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.775 seconds
[2025-05-24T23:26:42.298+0000] {processor.py:161} INFO - Started process (PID=1031) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:26:42.300+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:26:42.301+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:42.301+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:26:42.589+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:42.589+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:26:42.594+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:42.593+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:26:42.596+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:42.596+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:26:42.597+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:42.597+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:26:42.599+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:42.599+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:26:42.601+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:42.600+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:26:42.601+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:42.601+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:26:42.603+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:42.603+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:26:42.603+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:42.603+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:26:42.604+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:42.604+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:26:42.605+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:42.605+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:26:42.606+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:42.606+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:26:42.608+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:42.608+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:26:42.608+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:42.608+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:26:42.609+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:42.609+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:26:42.619+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:42.618+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:26:44.136+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:44.136+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:26:44.138+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:44.138+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:26:44.140+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:26:44.149+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:44.149+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:26:44.160+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:26:44.160+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:26:44.169+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.877 seconds
[2025-05-24T23:27:15.085+0000] {processor.py:161} INFO - Started process (PID=1101) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:27:15.088+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:27:15.090+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:15.090+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:27:15.294+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:15.294+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:27:15.300+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:15.300+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:27:15.305+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:15.305+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:27:15.307+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:15.306+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:27:15.310+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:15.310+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:27:15.311+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:15.311+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:27:15.311+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:15.311+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:27:15.314+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:15.314+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:27:15.314+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:15.314+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:27:15.316+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:15.316+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:27:15.320+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:15.320+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:27:15.324+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:15.324+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:27:15.329+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:15.328+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:27:15.329+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:15.329+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:27:15.330+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:15.330+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:27:15.346+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:15.346+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:27:17.022+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:17.022+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:27:17.025+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:17.025+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:27:17.033+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:27:17.045+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:17.045+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:27:17.061+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:17.061+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:27:17.071+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.009 seconds
[2025-05-24T23:27:47.190+0000] {processor.py:161} INFO - Started process (PID=1177) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:27:47.194+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:27:47.198+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:47.197+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:27:47.395+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:47.395+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:27:47.400+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:47.399+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:27:47.401+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:47.401+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:27:47.403+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:47.403+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:27:47.404+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:47.404+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:27:47.405+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:47.405+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:27:47.406+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:47.406+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:27:47.408+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:47.408+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:27:47.408+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:47.408+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:27:47.409+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:47.409+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:27:47.410+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:47.410+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:27:47.411+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:47.411+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:27:47.411+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:47.411+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:27:47.412+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:47.412+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:27:47.412+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:47.412+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:27:47.421+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:47.421+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:27:49.187+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:49.186+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:27:49.190+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:49.190+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:27:49.197+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:27:49.209+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:49.208+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:27:49.224+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:27:49.224+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:27:49.233+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.054 seconds
[2025-05-24T23:28:20.232+0000] {processor.py:161} INFO - Started process (PID=1258) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:28:20.233+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:28:20.235+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:20.235+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:28:20.433+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:20.433+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:28:20.437+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:20.437+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:28:20.439+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:20.439+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:28:20.440+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:20.440+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:28:20.442+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:20.441+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:28:20.443+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:20.443+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:28:20.443+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:20.443+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:28:20.445+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:20.445+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:28:20.445+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:20.445+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:28:20.446+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:20.446+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:28:20.447+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:20.447+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:28:20.448+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:20.448+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:28:20.448+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:20.448+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:28:20.449+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:20.449+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:28:20.450+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:20.449+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:28:20.458+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:20.458+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:28:22.115+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:22.115+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:28:22.118+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:22.118+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:28:22.124+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:28:22.138+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:22.138+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:28:22.157+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:22.157+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:28:22.170+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.946 seconds
[2025-05-24T23:28:52.857+0000] {processor.py:161} INFO - Started process (PID=1341) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:28:52.859+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:28:52.861+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:52.860+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:28:53.147+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:53.147+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:28:53.152+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:53.152+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:28:53.155+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:53.154+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:28:53.156+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:53.156+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:28:53.158+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:53.158+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:28:53.159+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:53.159+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:28:53.159+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:53.159+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:28:53.163+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:53.162+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:28:53.163+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:53.163+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:28:53.165+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:53.165+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:28:53.167+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:53.167+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:28:53.168+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:53.168+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:28:53.169+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:53.169+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:28:53.171+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:53.170+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:28:53.171+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:53.171+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:28:53.185+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:53.185+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:28:55.902+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:55.901+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:28:55.903+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:55.903+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:28:55.907+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:28:55.917+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:55.916+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:28:55.933+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:28:55.933+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:28:55.945+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.094 seconds
[2025-05-24T23:29:26.103+0000] {processor.py:161} INFO - Started process (PID=1406) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:29:26.105+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:29:26.108+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:26.108+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:29:26.351+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:26.350+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:29:26.357+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:26.357+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:29:26.359+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:26.359+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:29:26.360+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:26.360+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:29:26.361+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:26.361+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:29:26.362+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:26.362+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:29:26.363+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:26.363+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:29:26.366+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:26.365+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:29:26.366+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:26.366+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:29:26.367+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:26.367+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:29:26.368+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:26.368+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:29:26.369+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:26.369+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:29:26.371+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:26.371+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:29:26.373+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:26.373+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:29:26.374+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:26.373+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:29:26.385+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:26.384+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:29:27.983+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:27.983+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:29:27.985+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:27.984+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:29:27.988+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:29:27.995+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:27.995+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:29:28.007+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:28.007+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:29:28.015+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.921 seconds
[2025-05-24T23:29:58.335+0000] {processor.py:161} INFO - Started process (PID=1472) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:29:58.338+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:29:58.340+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:58.340+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:29:58.553+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:58.553+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:29:58.558+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:58.558+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:29:58.560+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:58.560+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:29:58.561+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:58.561+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:29:58.565+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:58.565+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:29:58.568+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:58.568+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:29:58.575+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:58.575+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:29:58.583+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:58.583+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:29:58.583+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:58.583+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:29:58.585+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:58.584+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:29:58.586+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:58.586+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:29:58.587+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:58.586+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:29:58.587+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:58.587+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:29:58.588+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:58.588+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:29:58.588+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:58.588+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:29:58.598+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:29:58.598+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:30:02.005+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:02.005+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:30:02.008+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:02.008+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:30:02.013+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:30:02.026+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:02.025+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:30:02.043+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:02.043+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:30:02.053+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.727 seconds
[2025-05-24T23:30:32.727+0000] {processor.py:161} INFO - Started process (PID=1546) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:30:32.728+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:30:32.731+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:32.730+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:30:33.023+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:33.023+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:30:33.029+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:33.029+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:30:33.031+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:33.031+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:30:33.032+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:33.032+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:30:33.034+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:33.034+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:30:33.035+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:33.035+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:30:33.036+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:33.036+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:30:33.039+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:33.039+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:30:33.039+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:33.039+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:30:33.041+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:33.040+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:30:33.042+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:33.042+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:30:33.043+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:33.043+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:30:33.044+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:33.044+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:30:33.045+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:33.045+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:30:33.046+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:33.046+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:30:33.061+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:33.060+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:30:35.175+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:35.174+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:30:35.178+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:35.178+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:30:35.185+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:30:35.198+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:35.198+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:30:35.227+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:30:35.226+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:30:35.239+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.519 seconds
[2025-05-24T23:31:06.043+0000] {processor.py:161} INFO - Started process (PID=1612) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:31:06.046+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:31:06.049+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:06.049+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:31:06.237+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:06.237+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:31:06.242+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:06.241+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:31:06.243+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:06.243+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:31:06.244+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:06.244+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:31:06.246+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:06.246+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:31:06.247+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:06.247+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:31:06.247+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:06.247+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:31:06.249+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:06.249+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:31:06.250+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:06.250+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:31:06.251+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:06.251+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:31:06.252+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:06.251+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:31:06.252+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:06.252+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:31:06.253+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:06.253+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:31:06.253+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:06.253+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:31:06.253+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:06.253+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:31:06.263+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:06.263+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:31:07.866+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:07.865+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:31:07.868+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:07.868+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:31:07.874+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:31:07.885+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:07.885+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:31:07.904+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:07.903+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:31:07.916+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.883 seconds
[2025-05-24T23:31:38.212+0000] {processor.py:161} INFO - Started process (PID=1678) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:31:38.213+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:31:38.216+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:38.215+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:31:38.458+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:38.458+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:31:38.462+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:38.462+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:31:38.466+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:38.466+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:31:38.467+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:38.467+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:31:38.469+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:38.469+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:31:38.470+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:38.470+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:31:38.471+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:38.471+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:31:38.473+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:38.473+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:31:38.474+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:38.474+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:31:38.476+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:38.475+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:31:38.477+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:38.477+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:31:38.478+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:38.478+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:31:38.479+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:38.479+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:31:38.479+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:38.479+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:31:38.480+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:38.480+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:31:38.491+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:38.491+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:31:40.309+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:40.309+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:31:40.312+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:40.312+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:31:40.318+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:31:40.331+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:40.330+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:31:40.352+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:31:40.351+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:31:40.370+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.163 seconds
[2025-05-24T23:32:10.823+0000] {processor.py:161} INFO - Started process (PID=1744) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:32:10.825+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:32:10.829+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:10.829+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:32:11.046+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:11.046+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:32:11.051+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:11.051+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:32:11.053+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:11.053+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:32:11.055+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:11.055+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:32:11.056+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:11.056+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:32:11.058+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:11.058+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:32:11.058+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:11.058+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:32:11.060+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:11.060+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:32:11.061+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:11.061+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:32:11.062+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:11.062+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:32:11.064+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:11.064+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:32:11.064+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:11.064+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:32:11.065+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:11.065+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:32:11.066+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:11.066+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:32:11.066+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:11.066+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:32:11.076+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:11.076+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:32:12.621+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:12.620+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:32:12.624+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:12.624+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:32:12.630+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:32:12.644+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:12.643+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:32:12.663+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:12.663+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:32:12.679+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.865 seconds
[2025-05-24T23:32:42.929+0000] {processor.py:161} INFO - Started process (PID=1810) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:32:42.932+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:32:42.936+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:42.935+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:32:43.144+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:43.144+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:32:43.149+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:43.148+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:32:43.151+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:43.151+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:32:43.152+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:43.152+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:32:43.154+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:43.154+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:32:43.155+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:43.155+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:32:43.155+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:43.155+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:32:43.158+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:43.158+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:32:43.158+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:43.158+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:32:43.159+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:43.159+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:32:43.160+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:43.160+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:32:43.160+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:43.160+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:32:43.161+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:43.161+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:32:43.161+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:43.161+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:32:43.162+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:43.162+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:32:43.171+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:43.171+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:32:45.034+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:45.034+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:32:45.036+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:45.035+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:32:45.039+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:32:45.047+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:45.047+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:32:45.064+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:32:45.063+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:32:45.077+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.156 seconds
[2025-05-24T23:33:15.315+0000] {processor.py:161} INFO - Started process (PID=1876) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:33:15.317+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:33:15.320+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:15.319+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:33:15.572+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:15.572+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:33:15.576+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:15.576+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:33:15.578+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:15.578+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:33:15.580+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:15.580+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:33:15.582+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:15.582+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:33:15.583+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:15.583+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:33:15.583+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:15.583+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:33:15.585+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:15.585+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:33:15.585+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:15.585+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:33:15.586+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:15.586+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:33:15.587+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:15.587+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:33:15.588+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:15.588+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:33:15.588+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:15.588+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:33:15.589+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:15.589+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:33:15.589+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:15.589+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:33:15.598+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:15.598+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:33:18.890+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:18.890+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:33:18.891+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:18.891+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:33:18.894+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:33:18.899+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:18.899+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:33:18.907+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:18.907+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:33:18.915+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.607 seconds
[2025-05-24T23:33:49.066+0000] {processor.py:161} INFO - Started process (PID=1950) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:33:49.067+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:33:49.068+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:49.068+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:33:49.249+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:49.248+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:33:49.253+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:49.253+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:33:49.256+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:49.256+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:33:49.257+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:49.257+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:33:49.259+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:49.258+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:33:49.260+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:49.260+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:33:49.260+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:49.260+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:33:49.262+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:49.262+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:33:49.263+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:49.263+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:33:49.263+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:49.263+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:33:49.264+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:49.264+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:33:49.265+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:49.265+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:33:49.266+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:49.265+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:33:49.266+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:49.266+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:33:49.267+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:49.266+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:33:49.275+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:49.275+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:33:51.296+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:51.296+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:33:51.298+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:51.298+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:33:51.301+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:33:51.308+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:51.308+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:33:51.319+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:33:51.319+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:33:51.328+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.265 seconds
[2025-05-24T23:34:21.872+0000] {processor.py:161} INFO - Started process (PID=2026) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:34:21.873+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:34:21.874+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:21.874+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:34:22.076+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:22.076+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:34:22.080+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:22.080+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:34:22.082+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:22.082+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:34:22.084+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:22.084+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:34:22.085+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:22.085+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:34:22.086+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:22.086+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:34:22.087+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:22.087+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:34:22.090+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:22.090+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:34:22.090+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:22.090+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:34:22.091+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:22.091+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:34:22.092+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:22.092+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:34:22.094+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:22.094+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:34:22.095+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:22.095+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:34:22.095+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:22.095+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:34:22.096+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:22.096+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:34:22.107+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:22.106+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:34:25.536+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:25.535+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:34:25.540+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:25.539+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:34:25.549+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:34:25.565+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:25.565+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:34:25.582+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:25.582+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:34:25.592+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.725 seconds
[2025-05-24T23:34:55.945+0000] {processor.py:161} INFO - Started process (PID=2107) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:34:55.946+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:34:55.948+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:55.947+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:34:56.166+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:56.166+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:34:56.172+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:56.172+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:34:56.174+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:56.174+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:34:56.175+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:56.175+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:34:56.177+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:56.177+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:34:56.178+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:56.178+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:34:56.179+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:56.179+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:34:56.181+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:56.181+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:34:56.181+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:56.181+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:34:56.182+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:56.182+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:34:56.183+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:56.183+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:34:56.184+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:56.184+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:34:56.185+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:56.185+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:34:56.186+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:56.186+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:34:56.186+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:56.186+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:34:56.196+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:56.196+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:34:59.755+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:59.750+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:34:59.765+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:59.765+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:34:59.774+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:34:59.791+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:59.791+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:34:59.812+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:34:59.812+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:34:59.822+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.883 seconds
[2025-05-24T23:35:30.869+0000] {processor.py:161} INFO - Started process (PID=2195) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:35:30.870+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:35:30.873+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:30.872+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:35:31.136+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:31.136+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:35:31.143+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:31.143+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:35:31.145+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:31.145+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:35:31.147+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:31.147+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:35:31.149+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:31.149+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:35:31.150+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:31.150+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:35:31.150+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:31.150+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:35:31.153+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:31.153+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:35:31.153+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:31.153+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:35:31.154+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:31.154+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:35:31.155+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:31.155+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:35:31.156+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:31.156+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:35:31.157+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:31.157+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:35:31.158+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:31.158+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:35:31.159+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:31.159+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:35:31.169+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:31.169+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:35:32.912+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:32.911+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:35:32.914+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:32.913+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:35:32.920+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:35:32.935+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:32.934+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:35:32.959+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:35:32.958+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:35:32.971+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.109 seconds
[2025-05-24T23:36:03.554+0000] {processor.py:161} INFO - Started process (PID=2261) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:36:03.556+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:36:03.558+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:03.557+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:36:03.834+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:03.834+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:36:03.840+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:03.840+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:36:03.842+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:03.842+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:36:03.844+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:03.844+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:36:03.845+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:03.845+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:36:03.846+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:03.846+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:36:03.847+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:03.847+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:36:03.849+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:03.849+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:36:03.849+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:03.849+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:36:03.851+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:03.851+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:36:03.852+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:03.852+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:36:03.853+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:03.853+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:36:03.853+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:03.853+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:36:03.854+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:03.854+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:36:03.854+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:03.854+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:36:03.865+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:03.864+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:36:07.963+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:07.962+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:36:07.966+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:07.966+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:36:07.974+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:36:07.990+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:07.989+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:36:08.008+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:08.007+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:36:08.018+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 4.471 seconds
[2025-05-24T23:36:38.495+0000] {processor.py:161} INFO - Started process (PID=2327) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:36:38.498+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:36:38.511+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:38.511+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:36:38.794+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:38.794+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:36:38.801+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:38.801+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:36:38.804+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:38.804+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:36:38.805+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:38.805+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:36:38.807+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:38.807+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:36:38.808+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:38.808+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:36:38.809+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:38.809+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:36:38.811+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:38.811+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:36:38.811+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:38.811+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:36:38.812+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:38.812+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:36:38.814+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:38.814+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:36:38.815+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:38.815+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:36:38.816+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:38.816+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:36:38.816+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:38.816+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:36:38.817+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:38.817+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:36:38.827+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:38.827+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:36:40.516+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:40.515+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:36:40.518+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:40.518+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:36:40.523+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:36:40.530+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:40.530+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:36:40.544+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:36:40.544+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:36:40.556+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.079 seconds
[2025-05-24T23:37:11.308+0000] {processor.py:161} INFO - Started process (PID=2395) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:37:11.309+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:37:11.310+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:11.310+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:37:11.509+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:11.509+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:37:11.514+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:11.513+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:37:11.515+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:11.515+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:37:11.517+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:11.516+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:37:11.518+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:11.518+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:37:11.519+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:11.519+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:37:11.520+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:11.520+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:37:11.522+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:11.522+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:37:11.522+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:11.522+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:37:11.523+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:11.523+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:37:11.524+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:11.524+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:37:11.524+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:11.524+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:37:11.525+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:11.525+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:37:11.526+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:11.526+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:37:11.526+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:11.526+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:37:11.535+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:11.535+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:37:13.263+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:13.263+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:37:13.266+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:13.265+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:37:13.272+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:37:13.284+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:13.284+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:37:13.303+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:13.303+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:37:13.314+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.012 seconds
[2025-05-24T23:37:43.355+0000] {processor.py:161} INFO - Started process (PID=2461) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:37:43.358+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:37:43.360+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:43.360+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:37:43.603+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:43.603+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:37:43.608+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:43.608+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:37:43.610+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:43.610+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:37:43.611+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:43.611+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:37:43.612+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:43.612+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:37:43.614+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:43.614+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:37:43.614+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:43.614+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:37:43.617+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:43.617+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:37:43.617+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:43.617+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:37:43.618+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:43.618+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:37:43.619+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:43.619+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:37:43.620+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:43.620+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:37:43.620+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:43.620+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:37:43.621+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:43.621+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:37:43.622+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:43.621+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:37:43.631+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:43.630+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:37:45.316+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:45.315+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:37:45.317+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:45.317+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:37:45.320+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:37:45.329+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:45.328+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:37:45.350+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:37:45.350+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:37:45.363+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.020 seconds
[2025-05-24T23:38:15.622+0000] {processor.py:161} INFO - Started process (PID=2527) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:38:15.623+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:38:15.625+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:15.624+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:38:15.823+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:15.823+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:38:15.828+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:15.828+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:38:15.830+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:15.830+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:38:15.832+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:15.832+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:38:15.833+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:15.833+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:38:15.835+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:15.834+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:38:15.835+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:15.835+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:38:15.838+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:15.838+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:38:15.839+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:15.838+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:38:15.840+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:15.840+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:38:15.842+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:15.842+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:38:15.843+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:15.843+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:38:15.844+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:15.844+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:38:15.844+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:15.844+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:38:15.845+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:15.845+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:38:15.855+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:15.855+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:38:17.769+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:17.767+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:38:17.773+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:17.772+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:38:17.781+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:38:17.792+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:17.792+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:38:17.804+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:17.804+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:38:17.813+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.208 seconds
[2025-05-24T23:38:48.249+0000] {processor.py:161} INFO - Started process (PID=2593) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:38:48.251+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:38:48.252+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:48.252+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:38:48.486+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:48.485+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:38:48.491+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:48.491+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:38:48.493+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:48.492+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:38:48.494+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:48.494+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:38:48.620+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:48.620+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:38:48.622+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:48.622+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:38:48.623+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:48.623+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:38:48.626+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:48.626+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:38:48.626+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:48.626+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:38:48.628+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:48.628+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:38:48.629+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:48.629+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:38:48.630+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:48.630+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:38:48.630+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:48.630+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:38:48.631+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:48.631+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:38:48.631+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:48.631+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:38:48.642+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:48.642+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:38:50.107+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:50.107+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:38:50.109+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:50.109+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:38:50.114+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:38:50.128+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:50.128+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:38:50.149+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:38:50.149+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:38:50.163+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.922 seconds
[2025-05-24T23:39:20.238+0000] {processor.py:161} INFO - Started process (PID=2659) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:39:20.240+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:39:20.242+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:20.242+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:39:20.481+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:20.481+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:39:20.486+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:20.486+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:39:20.488+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:20.488+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:39:20.490+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:20.490+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:39:20.676+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:20.676+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:39:20.678+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:20.678+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:39:20.679+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:20.678+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:39:20.681+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:20.681+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:39:20.681+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:20.681+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:39:20.683+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:20.682+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:39:20.684+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:20.683+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:39:20.684+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:20.684+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:39:20.685+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:20.685+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:39:20.685+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:20.685+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:39:20.686+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:20.686+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:39:20.696+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:20.696+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:39:22.225+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:22.225+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:39:22.226+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:22.226+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:39:22.230+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:39:22.238+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:22.238+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:39:22.253+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:22.252+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:39:22.265+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.034 seconds
[2025-05-24T23:39:52.458+0000] {processor.py:161} INFO - Started process (PID=2721) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:39:52.459+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:39:52.463+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:52.462+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:39:52.725+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:52.724+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:39:52.730+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:52.730+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:39:52.735+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:52.734+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:39:52.737+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:52.737+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:39:52.739+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:52.739+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:39:52.741+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:52.741+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:39:52.742+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:52.742+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:39:52.747+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:52.747+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:39:52.749+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:52.748+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:39:52.754+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:52.753+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:39:52.760+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:52.760+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:39:52.765+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:52.765+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:39:52.766+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:52.766+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:39:52.767+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:52.767+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:39:52.769+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:52.769+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:39:52.795+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:52.794+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:39:54.553+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:54.553+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:39:54.557+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:54.557+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:39:54.560+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:39:54.567+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:54.567+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:39:54.578+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:39:54.578+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:39:54.592+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.139 seconds
[2025-05-24T23:40:24.874+0000] {processor.py:161} INFO - Started process (PID=2789) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:40:24.877+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:40:24.879+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:24.879+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:40:25.233+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:25.233+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:40:25.238+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:25.238+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:40:25.240+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:25.240+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:40:25.241+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:25.241+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:40:25.243+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:25.243+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:40:25.244+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:25.244+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:40:25.244+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:25.244+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:40:25.247+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:25.246+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:40:25.247+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:25.247+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:40:25.248+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:25.248+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:40:25.249+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:25.248+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:40:25.249+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:25.249+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:40:25.250+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:25.250+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:40:25.251+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:25.251+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:40:25.251+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:25.251+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:40:25.260+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:25.260+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:40:26.670+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:26.670+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:40:26.671+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:26.671+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:40:26.673+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:40:26.678+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:26.678+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:40:26.688+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:26.688+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:40:26.700+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.831 seconds
[2025-05-24T23:40:56.755+0000] {processor.py:161} INFO - Started process (PID=2853) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:40:56.756+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:40:56.759+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:56.758+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:40:57.188+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:57.187+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:40:57.193+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:57.193+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:40:57.198+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:57.198+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:40:57.200+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:57.200+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:40:57.202+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:57.202+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:40:57.206+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:57.206+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:40:57.207+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:57.206+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:40:57.213+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:57.213+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:40:57.214+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:57.214+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:40:57.215+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:57.215+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:40:57.218+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:57.217+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:40:57.219+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:57.219+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:40:57.220+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:57.220+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:40:57.221+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:57.221+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:40:57.221+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:57.221+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:40:57.233+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:57.232+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:40:58.673+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:58.673+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:40:58.675+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:58.675+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:40:58.678+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:40:58.695+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:58.694+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:40:58.710+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:40:58.710+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:40:58.722+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.974 seconds
[2025-05-24T23:41:29.155+0000] {processor.py:161} INFO - Started process (PID=2919) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:41:29.157+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:41:29.160+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:29.160+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:41:29.668+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:29.668+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:41:29.676+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:29.676+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:41:29.679+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:29.679+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:41:29.681+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:29.681+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:41:29.683+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:29.683+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:41:29.687+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:29.687+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:41:29.687+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:29.687+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:41:29.692+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:29.692+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:41:29.693+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:29.693+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:41:29.695+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:29.695+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:41:29.697+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:29.697+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:41:29.698+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:29.698+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:41:29.699+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:29.699+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:41:29.700+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:29.700+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:41:29.701+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:29.701+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:41:29.724+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:29.724+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:41:31.554+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:31.554+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:41:31.558+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:31.558+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:41:31.561+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:41:31.574+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:31.574+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:41:31.584+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:41:31.584+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:41:31.592+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.466 seconds
[2025-05-24T23:42:01.708+0000] {processor.py:161} INFO - Started process (PID=2997) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:42:01.708+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:42:01.709+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:01.709+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:42:02.055+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:02.055+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:42:02.060+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:02.060+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:42:02.062+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:02.062+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:42:02.063+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:02.063+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:42:02.065+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:02.065+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:42:02.066+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:02.066+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:42:02.066+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:02.066+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:42:02.068+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:02.068+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:42:02.069+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:02.069+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:42:02.070+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:02.069+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:42:02.070+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:02.070+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:42:02.071+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:02.071+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:42:02.071+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:02.071+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:42:02.072+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:02.072+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:42:02.072+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:02.072+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:42:02.081+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:02.081+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:42:05.385+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:05.384+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:42:05.386+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:05.386+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:42:05.390+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:42:05.398+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:05.398+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:42:05.408+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:05.408+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:42:05.417+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.714 seconds
[2025-05-24T23:42:35.459+0000] {processor.py:161} INFO - Started process (PID=3065) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:42:35.460+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:42:35.461+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:35.461+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:42:35.803+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:35.803+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:42:35.808+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:35.808+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:42:35.810+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:35.810+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:42:35.812+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:35.812+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:42:35.814+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:35.814+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:42:35.818+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:35.818+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:42:35.819+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:35.819+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:42:35.823+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:35.823+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:42:35.824+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:35.824+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:42:35.826+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:35.826+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:42:35.828+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:35.828+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:42:35.831+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:35.831+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:42:35.833+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:35.833+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:42:35.834+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:35.834+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:42:35.835+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:35.835+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:42:35.865+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:35.865+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:42:39.394+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:39.394+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:42:39.397+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:39.397+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:42:39.399+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:42:39.405+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:39.405+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:42:39.416+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:42:39.415+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:42:39.426+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.971 seconds
[2025-05-24T23:43:09.854+0000] {processor.py:161} INFO - Started process (PID=3167) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:43:09.855+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:43:09.857+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:09.856+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:43:10.194+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:10.194+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:43:10.199+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:10.199+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:43:10.201+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:10.201+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:43:10.202+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:10.202+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:43:10.204+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:10.204+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:43:10.205+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:10.205+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:43:10.206+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:10.206+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:43:10.208+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:10.208+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:43:10.209+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:10.209+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:43:10.210+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:10.210+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:43:10.212+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:10.212+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:43:10.213+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:10.213+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:43:10.214+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:10.214+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:43:10.215+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:10.215+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:43:10.216+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:10.216+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:43:10.228+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:10.228+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:43:11.629+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:11.628+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:43:11.630+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:11.630+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:43:11.635+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:43:11.645+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:11.644+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:43:11.660+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:11.660+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:43:11.678+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.833 seconds
[2025-05-24T23:43:42.155+0000] {processor.py:161} INFO - Started process (PID=3229) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:43:42.155+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:43:42.157+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:42.156+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:43:42.553+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:42.553+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:43:42.559+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:42.558+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:43:42.562+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:42.562+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:43:42.565+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:42.564+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:43:42.567+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:42.567+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:43:42.568+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:42.568+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:43:42.570+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:42.570+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:43:42.576+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:42.575+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:43:42.577+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:42.577+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:43:42.579+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:42.578+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:43:42.581+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:42.580+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:43:42.582+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:42.582+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:43:42.583+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:42.583+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:43:42.584+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:42.584+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:43:42.585+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:42.585+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:43:42.600+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:42.600+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:43:45.632+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:45.631+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:43:45.636+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:45.635+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:43:45.641+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:43:45.651+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:45.650+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:43:45.664+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:43:45.664+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:43:45.677+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.525 seconds
[2025-05-24T23:44:16.064+0000] {processor.py:161} INFO - Started process (PID=3309) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:44:16.067+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:44:16.071+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:16.070+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:44:16.523+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:16.523+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:44:16.529+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:16.529+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:44:16.534+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:16.534+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:44:16.536+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:16.536+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:44:16.538+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:16.537+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:44:16.539+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:16.538+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:44:16.539+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:16.539+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:44:16.541+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:16.541+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:44:16.542+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:16.542+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:44:16.544+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:16.544+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:44:16.545+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:16.545+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:44:16.546+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:16.546+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:44:16.547+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:16.547+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:44:16.548+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:16.548+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:44:16.548+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:16.548+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:44:16.559+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:16.559+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:44:18.391+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:18.391+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:44:18.393+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:18.392+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:44:18.397+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:44:18.404+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:18.404+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:44:18.420+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:18.419+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:44:18.445+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.387 seconds
[2025-05-24T23:44:49.225+0000] {processor.py:161} INFO - Started process (PID=3375) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:44:49.227+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:44:49.230+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:49.230+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:44:49.542+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:49.542+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:44:49.546+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:49.546+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:44:49.547+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:49.547+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:44:49.549+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:49.548+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:44:49.550+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:49.550+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:44:49.551+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:49.551+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:44:49.551+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:49.551+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:44:49.553+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:49.553+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:44:49.554+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:49.554+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:44:49.555+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:49.555+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:44:49.555+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:49.555+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:44:49.556+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:49.556+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:44:49.557+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:49.557+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:44:49.557+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:49.557+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:44:49.558+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:49.558+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:44:49.572+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:49.572+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:44:51.467+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:51.467+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:44:51.469+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:51.468+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:44:51.471+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:44:51.476+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:51.476+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:44:51.488+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:44:51.488+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:44:51.497+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.281 seconds
[2025-05-24T23:45:21.898+0000] {processor.py:161} INFO - Started process (PID=3441) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:45:21.900+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:45:21.902+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:21.902+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:45:22.207+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:22.207+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:45:22.212+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:22.212+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:45:22.216+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:22.216+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:45:22.218+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:22.218+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:45:22.220+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:22.220+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:45:22.221+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:22.221+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:45:22.222+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:22.222+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:45:22.224+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:22.224+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:45:22.225+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:22.225+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:45:22.226+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:22.226+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:45:22.228+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:22.227+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:45:22.229+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:22.228+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:45:22.230+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:22.229+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:45:22.230+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:22.230+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:45:22.231+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:22.231+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:45:22.244+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:22.244+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:45:25.465+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:25.465+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:45:25.466+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:25.466+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:45:25.470+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:45:25.478+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:25.477+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:45:25.490+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:25.490+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:45:25.499+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.608 seconds
[2025-05-24T23:45:55.600+0000] {processor.py:161} INFO - Started process (PID=3509) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:45:55.602+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:45:55.606+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:55.605+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:45:55.938+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:55.938+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:45:55.942+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:55.942+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:45:55.944+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:55.944+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:45:55.945+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:55.945+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:45:55.947+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:55.947+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:45:55.948+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:55.948+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:45:55.949+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:55.949+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:45:55.951+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:55.951+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:45:55.951+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:55.951+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:45:55.952+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:55.952+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:45:55.952+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:55.952+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:45:55.953+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:55.953+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:45:55.954+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:55.954+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:45:55.954+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:55.954+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:45:55.955+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:55.955+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:45:55.964+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:55.964+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:45:57.410+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:57.410+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:45:57.412+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:57.412+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:45:57.415+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:45:57.421+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:57.421+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:45:57.431+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:45:57.431+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:45:57.440+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.848 seconds
[2025-05-24T23:46:27.483+0000] {processor.py:161} INFO - Started process (PID=3575) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:46:27.485+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:46:27.487+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:27.486+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:46:27.796+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:27.796+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:46:27.800+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:27.800+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:46:27.801+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:27.801+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:46:27.803+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:27.802+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:46:27.804+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:27.804+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:46:27.805+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:27.805+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:46:27.805+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:27.805+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:46:27.807+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:27.807+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:46:27.807+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:27.807+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:46:27.808+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:27.808+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:46:27.809+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:27.809+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:46:27.810+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:27.810+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:46:27.810+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:27.810+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:46:27.811+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:27.811+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:46:27.811+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:27.811+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:46:27.820+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:27.820+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:46:29.405+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:29.405+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:46:29.407+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:29.407+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:46:29.411+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:46:29.421+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:29.420+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:46:29.433+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:29.433+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:46:29.463+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.987 seconds
[2025-05-24T23:46:59.977+0000] {processor.py:161} INFO - Started process (PID=3641) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:46:59.977+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:46:59.979+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:46:59.978+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:47:00.253+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:00.253+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:47:00.257+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:00.257+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:47:00.259+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:00.259+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:47:00.260+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:00.260+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:47:00.262+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:00.262+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:47:00.263+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:00.263+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:47:00.263+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:00.263+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:47:00.265+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:00.265+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:47:00.266+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:00.266+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:47:00.267+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:00.267+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:47:00.268+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:00.268+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:47:00.268+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:00.268+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:47:00.270+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:00.269+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:47:00.270+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:00.270+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:47:00.271+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:00.271+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:47:00.280+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:00.280+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:47:01.890+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:01.890+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:47:01.893+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:01.893+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:47:01.900+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:47:01.919+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:01.918+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:47:01.943+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:01.943+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:47:01.956+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.982 seconds
[2025-05-24T23:47:32.282+0000] {processor.py:161} INFO - Started process (PID=3712) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:47:32.283+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:47:32.284+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:32.283+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:47:32.545+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:32.544+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:47:32.549+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:32.549+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:47:32.551+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:32.551+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:47:32.552+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:32.552+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:47:32.553+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:32.553+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:47:32.555+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:32.554+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:47:32.555+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:32.555+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:47:32.557+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:32.557+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:47:32.557+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:32.557+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:47:32.558+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:32.558+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:47:32.559+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:32.559+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:47:32.560+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:32.560+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:47:32.560+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:32.560+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:47:32.561+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:32.561+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:47:32.561+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:32.561+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:47:32.570+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:32.570+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:47:36.062+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:36.061+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:47:36.066+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:36.065+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:47:36.073+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:47:36.086+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:36.086+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:47:36.103+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:47:36.103+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:47:36.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.834 seconds
[2025-05-24T23:48:07.003+0000] {processor.py:161} INFO - Started process (PID=3782) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:48:07.005+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:48:07.007+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:07.006+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:48:07.300+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:07.299+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:48:07.303+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:07.303+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:48:07.305+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:07.305+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:48:07.306+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:07.306+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:48:07.308+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:07.308+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:48:07.309+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:07.309+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:48:07.309+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:07.309+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:48:07.312+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:07.312+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:48:07.312+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:07.312+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:48:07.313+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:07.313+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:48:07.314+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:07.314+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:48:07.314+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:07.314+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:48:07.315+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:07.315+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:48:07.316+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:07.316+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:48:07.316+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:07.316+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:48:07.327+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:07.327+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:48:08.746+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:08.745+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:48:08.748+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:08.747+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:48:08.754+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:48:08.766+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:08.766+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:48:08.786+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:08.786+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:48:08.797+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.798 seconds
[2025-05-24T23:48:39.208+0000] {processor.py:161} INFO - Started process (PID=3848) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:48:39.211+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:48:39.217+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:39.216+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:48:39.708+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:39.708+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:48:39.713+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:39.712+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:48:39.715+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:39.715+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:48:39.716+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:39.716+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:48:39.718+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:39.718+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:48:39.719+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:39.719+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:48:39.720+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:39.720+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:48:39.723+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:39.723+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:48:39.723+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:39.723+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:48:39.724+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:39.724+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:48:39.725+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:39.725+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:48:39.726+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:39.726+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:48:39.726+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:39.726+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:48:39.727+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:39.727+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:48:39.727+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:39.727+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:48:39.737+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:39.737+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:48:43.460+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:43.457+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:48:43.467+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:43.467+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:48:43.487+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:48:43.525+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:43.524+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:48:43.553+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:48:43.553+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:48:43.579+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 4.385 seconds
[2025-05-24T23:49:13.908+0000] {processor.py:161} INFO - Started process (PID=3922) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:49:13.910+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:49:13.913+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:13.912+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:49:14.226+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:14.226+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:49:14.236+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:14.236+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:49:14.239+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:14.239+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:49:14.242+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:14.241+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:49:14.244+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:14.244+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:49:14.247+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:14.247+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:49:14.248+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:14.248+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:49:14.251+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:14.251+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:49:14.252+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:14.252+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:49:14.253+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:14.253+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:49:14.254+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:14.254+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:49:14.255+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:14.255+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:49:14.256+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:14.256+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:49:14.257+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:14.256+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:49:14.257+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:14.257+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:49:14.284+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:14.284+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:49:15.815+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:15.815+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:49:15.819+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:15.819+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:49:15.821+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:49:15.828+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:15.828+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:49:15.837+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:15.837+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:49:15.849+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.950 seconds
[2025-05-24T23:49:45.905+0000] {processor.py:161} INFO - Started process (PID=4002) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:49:45.907+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:49:45.910+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:45.909+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:49:46.343+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:46.343+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:49:46.349+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:46.348+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:49:46.351+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:46.351+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:49:46.352+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:46.352+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:49:46.355+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:46.355+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:49:46.356+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:46.356+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:49:46.357+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:46.357+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:49:46.363+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:46.363+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:49:46.364+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:46.363+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:49:46.366+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:46.365+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:49:46.368+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:46.368+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:49:46.369+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:46.369+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:49:46.370+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:46.370+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:49:46.371+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:46.370+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:49:46.371+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:46.371+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:49:46.398+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:46.398+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:49:47.832+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:47.831+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:49:47.833+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:47.833+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:49:47.838+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:49:47.850+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:47.850+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:49:47.866+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:49:47.866+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:49:47.879+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 1.970 seconds
[2025-05-24T23:50:18.837+0000] {processor.py:161} INFO - Started process (PID=4082) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:50:18.838+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:50:18.841+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:18.840+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:50:19.154+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:19.153+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:50:19.158+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:19.158+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:50:19.160+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:19.160+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:50:19.162+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:19.161+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:50:19.163+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:19.163+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:50:19.164+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:19.164+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:50:19.165+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:19.165+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:50:19.167+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:19.167+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:50:19.167+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:19.167+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:50:19.168+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:19.168+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:50:19.169+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:19.169+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:50:19.169+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:19.169+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:50:19.170+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:19.170+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:50:19.171+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:19.171+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:50:19.171+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:19.171+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:50:19.181+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:19.180+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:50:20.959+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:20.959+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:50:20.962+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:20.962+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:50:20.970+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:50:20.984+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:20.984+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:50:21.000+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:21.000+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:50:21.010+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.180 seconds
[2025-05-24T23:50:51.660+0000] {processor.py:161} INFO - Started process (PID=4148) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:50:51.662+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:50:51.665+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:51.665+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:50:52.196+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:52.196+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:50:52.202+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:52.202+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:50:52.207+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:52.207+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:50:52.209+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:52.209+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:50:52.211+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:52.211+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:50:52.212+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:52.212+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:50:52.212+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:52.212+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:50:52.216+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:52.215+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:50:52.216+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:52.216+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:50:52.217+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:52.217+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:50:52.218+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:52.218+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:50:52.219+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:52.219+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:50:52.220+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:52.219+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:50:52.220+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:52.220+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:50:52.221+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:52.220+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:50:52.231+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:52.231+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:50:53.835+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:53.834+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:50:53.838+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:53.837+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:50:53.846+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:50:53.864+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:53.864+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:50:53.885+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:50:53.885+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:50:53.897+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 2.253 seconds
[2025-05-24T23:51:24.418+0000] {processor.py:161} INFO - Started process (PID=4224) to work on /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:51:24.422+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/supplier_dag.py for tasks to queue
[2025-05-24T23:51:24.426+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:24.425+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:51:24.751+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:24.751+0000] {etl.py:173} INFO - Starting ETL pipeline
[2025-05-24T23:51:24.756+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:24.756+0000] {etl.py:21} INFO - csv read
[2025-05-24T23:51:24.758+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:24.758+0000] {etl.py:23} INFO - csv data source pickled
[2025-05-24T23:51:24.760+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:24.759+0000] {etl.py:67} INFO - api data read
[2025-05-24T23:51:24.761+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:24.761+0000] {etl.py:69} INFO - api json data normalized
[2025-05-24T23:51:24.763+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:24.762+0000] {etl.py:71} INFO - api data source pickled
[2025-05-24T23:51:24.763+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:24.763+0000] {etl.py:78} INFO - connected to db source
[2025-05-24T23:51:24.766+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:24.766+0000] {etl.py:80} INFO - queried db source
[2025-05-24T23:51:24.766+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:24.766+0000] {etl.py:82} INFO - closed db connection
[2025-05-24T23:51:24.767+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:24.767+0000] {etl.py:84} INFO - db source pickled
[2025-05-24T23:51:24.768+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:24.768+0000] {etl.py:92} INFO - Pickled CSV data converted into DataFrame
[2025-05-24T23:51:24.769+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:24.769+0000] {etl.py:94} INFO - Pickled API data converted into DataFrame
[2025-05-24T23:51:24.770+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:24.770+0000] {etl.py:96} INFO - Pickled SQL data converted into DataFrame
[2025-05-24T23:51:24.771+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:24.771+0000] {etl.py:104} INFO - DataFrames merged side-by-side
[2025-05-24T23:51:24.771+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:24.771+0000] {etl.py:113} INFO - Column names standardized post-merge
[2025-05-24T23:51:24.782+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:24.782+0000] {etl.py:119} INFO - Merged data saved to /opt/airflow/dags/merged_supplier_data.csv
[2025-05-24T23:51:27.955+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:27.954+0000] {etl.py:160} INFO - Successfully uploaded to s3://fmcg-de-assessment/supplier_data/merged_supplier_data.csv
[2025-05-24T23:51:27.957+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:27.957+0000] {etl.py:182} INFO - ETL completed successfully
[2025-05-24T23:51:27.961+0000] {processor.py:840} INFO - DAG(s) 'supplier_data_etl_pipeline' retrieved from /opt/airflow/dags/supplier_dag.py
[2025-05-24T23:51:27.970+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:27.970+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-05-24T23:51:27.985+0000] {logging_mixin.py:188} INFO - [2025-05-24T23:51:27.984+0000] {dag.py:3954} INFO - Setting next_dagrun for supplier_data_etl_pipeline to 2025-05-24 00:00:00+00:00, run_after=2025-05-25 00:00:00+00:00
[2025-05-24T23:51:28.000+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/supplier_dag.py took 3.591 seconds
